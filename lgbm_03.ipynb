{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43152d3849664f889992597e225418f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47daabe9d9544367821180fc3b1ef1bc",
              "IPY_MODEL_0a1749457e034881a9826d642e99b39d",
              "IPY_MODEL_030a3d3228b747dd840fccec5df1b3e7"
            ],
            "layout": "IPY_MODEL_11a628104e3c4fe6adda07e63c82d326"
          }
        },
        "47daabe9d9544367821180fc3b1ef1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8f6ffb864b49d1ba436e07ffa64997",
            "placeholder": "​",
            "style": "IPY_MODEL_b036a5b8d58248f281b201a9baa64295",
            "value": "Best trial: 46. Best value: 14.66: 100%"
          }
        },
        "0a1749457e034881a9826d642e99b39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f7ed7062744e3ea97df76e850d2f44",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7071a35e0d04508b60234d6cf417d52",
            "value": 50
          }
        },
        "030a3d3228b747dd840fccec5df1b3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19da4bb30cdf48b19b321521f4772c93",
            "placeholder": "​",
            "style": "IPY_MODEL_2de81e663696400190029c9d773b2517",
            "value": " 50/50 [10:14&lt;00:00, 10.22s/it]"
          }
        },
        "11a628104e3c4fe6adda07e63c82d326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8f6ffb864b49d1ba436e07ffa64997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b036a5b8d58248f281b201a9baa64295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48f7ed7062744e3ea97df76e850d2f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7071a35e0d04508b60234d6cf417d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19da4bb30cdf48b19b321521f4772c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de81e663696400190029c9d773b2517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 가능 여부\n",
        "print(torch.cuda.is_available())  # True면 GPU 사용 가능\n",
        "print(torch.cuda.get_device_name(0))  # GPU 이름 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVriMxe9_Mvc",
        "outputId": "196d976d-2c6e-434b-b3a9-ea5b38a23b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab 환경을 위한 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltSlwie6Fvg",
        "outputId": "c5eb797d-957d-4d8f-f85f-0f0c3b289da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-Cell Clean Preprocessing (공휴일여부 + 휴무일여부) ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/lg_aimers_2/data/train/train.csv'\n",
        "OUT_PATH   = '/content/drive/MyDrive/lg_aimers_2/train_preprocessed_03.csv'\n",
        "\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    !pip install holidays -q\n",
        "    import holidays\n",
        "\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"train.csv 전처리 + (공휴일/휴무일) + 계절/사이클릭 + 출시일 파생\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "     # === 음수 매출수량 보정 ===\n",
        "    if \"매출수량\" in df.columns:\n",
        "        df[\"매출수량\"] = df[\"매출수량\"].clip(lower=0)\n",
        "\n",
        "    df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    df['년'] = df['영업일자'].dt.year\n",
        "    df['월'] = df['영업일자'].dt.month\n",
        "    df['일'] = df['영업일자'].dt.day\n",
        "    df['요일'] = df['영업일자'].dt.dayofweek\n",
        "    df['주말여부'] = df['요일'].isin([5, 6])\n",
        "\n",
        "    # 메뉴 분리\n",
        "    df[['영업장명', '메뉴명']] = df['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "\n",
        "    # === 캘린더 피처 ===\n",
        "    def add_calendar_features(frame: pd.DataFrame, date_col=\"영업일자\") -> pd.DataFrame:\n",
        "        out = frame.copy()\n",
        "        d = pd.to_datetime(out[date_col])\n",
        "        years = sorted(d.dt.year.unique())\n",
        "        try:\n",
        "            KR_HOL = holidays.KR(years=years, language=\"ko\")\n",
        "        except Exception:\n",
        "            KR_HOL = holidays.KR(years=years)\n",
        "\n",
        "        # 공휴일 여부만\n",
        "        out[\"공휴일여부\"] = d.dt.date.map(lambda x: x in KR_HOL)\n",
        "\n",
        "        # 계절\n",
        "        m = d.dt.month\n",
        "        out[\"계절(겨울0봄1여름2가을3)\"] = (\n",
        "            (m.isin([12,1,2]))*0 +\n",
        "            (m.isin([3,4,5]))*1 +\n",
        "            (m.isin([6,7,8]))*2 +\n",
        "            (m.isin([9,10,11]))*3\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "        # 사이클릭\n",
        "        out[\"요일_sin\"] = np.sin(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"요일_cos\"] = np.cos(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"월_sin\"]   = np.sin(2*np.pi*(out[\"월\"]-1)/12)\n",
        "        out[\"월_cos\"]   = np.cos(2*np.pi*(out[\"월\"]-1)/12)\n",
        "\n",
        "        return out\n",
        "\n",
        "    df = add_calendar_features(df, date_col='영업일자')\n",
        "\n",
        "    # === 추가: 휴무일여부 (주말 OR 공휴일) ===\n",
        "    df[\"휴무일여부\"] = df[\"주말여부\"] | df[\"공휴일여부\"]\n",
        "\n",
        "    # === 출시일 파생 ===\n",
        "    launch_dates = {\n",
        "        '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "        '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "        '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "        '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "        '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "        '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "        '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "        '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "        '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "        '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "        '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "        '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "        '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "        '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "        '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "        '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "        '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "        '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "        '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "        '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "        '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "        '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "        '라그로타_해산물 토마토 스튜 파스타': '2023-12-08', '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "        '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "        '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "        '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "        '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "        '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "        '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "        '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "        '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "        '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "        '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "        '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "        '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "        '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "        '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "        '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "        '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "        '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "        '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "        '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "        '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "        '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "        '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "        '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "        '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "        '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "        '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "        '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "        '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "    }\n",
        "    launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "    def calculate_days_since_launch(row):\n",
        "        menu = row['영업장명_메뉴명']\n",
        "        if menu in launch_dates:\n",
        "            launch_date = launch_dates[menu]\n",
        "            if row['영업일자'] >= launch_date:\n",
        "                return (row['영업일자'] - launch_date).days\n",
        "        return 0\n",
        "\n",
        "    df['출시일로부터경과일'] = df.apply(calculate_days_since_launch, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 실행\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "train_preprocessed = preprocess_data(train_df)\n",
        "train_preprocessed.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(\"전처리 완료 →\", OUT_PATH)\n",
        "\n",
        "# 점검\n",
        "print(\"공휴일여부 분포:\", train_preprocessed[\"공휴일여부\"].value_counts(dropna=False).to_dict())\n",
        "print(\"휴무일여부 분포:\", train_preprocessed[\"휴무일여부\"].value_counts(dropna=False).to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5qtDmEV0vXk",
        "outputId": "f6233b56-7681-4785-f53d-0fc252d9a3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 완료 → /content/drive/MyDrive/lg_aimers_2/train_preprocessed_03.csv\n",
            "공휴일여부 분포: {False: 97079, True: 5597}\n",
            "휴무일여부 분포: {False: 69287, True: 33389}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lightgbm -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRegressor, log_evaluation, early_stopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib, os\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/lg_aimers_2/train_preprocessed_03.csv\"\n",
        "\n",
        "# 1) Load\n",
        "df = pd.read_csv(TRAIN_PATH, parse_dates=[\"영업일자\"])\n",
        "\n",
        "# 2) 기본 타입 정리 (★ 휴무일여부 추가)\n",
        "bool_cols = [\"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\"]\n",
        "bool_cols = [c for c in bool_cols if c in df.columns]\n",
        "for c in bool_cols:\n",
        "    df[c] = df[c].astype(int)\n",
        "\n",
        "# 원-소스 카테고리\n",
        "key_col = \"영업장명_메뉴명\"\n",
        "if key_col not in df.columns:\n",
        "    raise ValueError(f\"'{key_col}' 컬럼이 필요합니다.\")\n",
        "df[key_col] = df[key_col].astype(\"category\")\n",
        "\n",
        "# 3) 누수 방지용 시계열 정렬 후 lag/rolling 생성\n",
        "df = df.sort_values([key_col, \"영업일자\"]).copy()\n",
        "\n",
        "lag_list = [1, 7, 14, 28]\n",
        "for lag in lag_list:\n",
        "    df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
        "\n",
        "# rolling\n",
        "def add_rolling(g, window, how=\"mean\"):\n",
        "    s = g[\"매출수량\"].shift(1)\n",
        "    if how == \"mean\":\n",
        "        return s.rolling(window, min_periods=1).mean()\n",
        "    elif how == \"sum\":\n",
        "        return s.rolling(window, min_periods=1).sum()\n",
        "\n",
        "df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"mean\")\n",
        "df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"sum\")\n",
        "df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
        "df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
        "\n",
        "# 같은 요일 패턴\n",
        "def same_dow_mean_28(g):\n",
        "    out = np.full(len(g), np.nan, dtype=float)\n",
        "    vals = g[\"매출수량\"].shift(1)\n",
        "    dows = g[\"요일\"]\n",
        "    for i in range(len(g)):\n",
        "        lo = max(0, i-28)\n",
        "        same_idx = [j for j in range(lo,i) if dows.iloc[j]==dows.iloc[i]]\n",
        "        if same_idx:\n",
        "            out[i] = vals.iloc[same_idx].mean()\n",
        "    return pd.Series(out, index=g.index)\n",
        "\n",
        "if \"요일\" in df.columns:\n",
        "    df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
        "\n",
        "# 신규 메뉴 추가 파생\n",
        "if \"출시일로부터경과일\" in df.columns:\n",
        "    df[\"출시후_주차\"] = (df[\"출시일로부터경과일\"] // 7).clip(lower=0).astype(int)\n",
        "    df[\"출시_0주\"]     = (df[\"출시후_주차\"] == 0).astype(int)\n",
        "    df[\"출시_1_2주\"]   = df[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    df[\"출시_3_4주\"]   = df[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    df[\"출시_5주이상\"] = (df[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "# 4) Train/Valid split\n",
        "train_cut   = pd.Timestamp(\"2024-05-31\")\n",
        "valid_start = pd.Timestamp(\"2024-06-01\")\n",
        "valid_end   = pd.Timestamp(\"2024-06-15\")\n",
        "\n",
        "train_df = df[df[\"영업일자\"] <= train_cut].copy()\n",
        "valid_df = df[(df[\"영업일자\"] >= valid_start) & (df[\"영업일자\"] <= valid_end)].copy()\n",
        "\n",
        "# 사용 피처 (★ 휴무일여부 추가)\n",
        "base_feats = [c for c in [\n",
        "    \"년\",\"월\",\"일\",\"요일\",\n",
        "    \"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\",\n",
        "    \"출시일로부터경과일\",\"출시후_주차\",\n",
        "    \"출시_0주\",\"출시_1_2주\",\"출시_3_4주\",\"출시_5주이상\",\n",
        "    \"요일_sin\",\"요일_cos\",\"월_sin\",\"월_cos\",\"계절(겨울0봄1여름2가을3)\"\n",
        "] if c in df.columns]\n",
        "\n",
        "lag_feats  = [f\"lag_{l}\" for l in lag_list]\n",
        "roll_feats = [c for c in [\"roll7_mean\",\"roll7_sum\",\"roll14_mean\",\"roll28_mean\",\"same_dow_mean_28\"] if c in df.columns]\n",
        "\n",
        "use_cols = base_feats + lag_feats + roll_feats + [key_col]\n",
        "\n",
        "# NA drop & fill\n",
        "train_df = train_df.dropna(subset=[f\"lag_{min(lag_list)}\"])\n",
        "valid_df = valid_df.dropna(subset=[f\"lag_{min(lag_list)}\"])\n",
        "fill_cols = list(set(use_cols) - {key_col})\n",
        "train_df[fill_cols] = train_df[fill_cols].fillna(0)\n",
        "valid_df[fill_cols] = valid_df[fill_cols].fillna(0)\n",
        "\n",
        "X_trn, y_trn = train_df[use_cols], train_df[\"매출수량\"].astype(float).values\n",
        "X_val, y_val = valid_df[use_cols], valid_df[\"매출수량\"].astype(float).values\n",
        "\n",
        "cat_features = [use_cols.index(key_col)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa1fh3qV0_3B",
        "outputId": "555d5ac3-27b8-4d55-ec1e-3f3a92cd8661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3699021691.py:31: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-3699021691.py:31: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-3699021691.py:31: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-3699021691.py:31: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-3699021691.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:42: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"sum\")\n",
            "/tmp/ipython-input-3699021691.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7, how=\"sum\")\n",
            "/tmp/ipython-input-3699021691.py:43: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:44: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-3699021691.py:59: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
            "/tmp/ipython-input-3699021691.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 0) Colab 셋업 =====\n",
        "!nvidia-smi -L || echo \"⚠️ GPU가 감지되지 않음: 런타임 > 런타임 유형 변경 > GPU 선택\"\n",
        "!pip -q install optuna lightgbm\n",
        "\n",
        "import numpy as np, optuna\n",
        "from lightgbm import LGBMRegressor, log_evaluation, early_stopping\n",
        "\n",
        "USE_GPU = True\n",
        "DEVICE_PARAMS = {\"device\": \"gpu\"} if USE_GPU else {\"device\": \"cpu\"}\n",
        "\n",
        "# ===== 1) Optuna Objective =====\n",
        "def objective(trial):\n",
        "    objective_choice = trial.suggest_categorical(\"objective\", [\"regression\", \"poisson\", \"tweedie\"])\n",
        "    params = {\n",
        "        \"objective\": objective_choice,\n",
        "        \"tweedie_variance_power\": trial.suggest_float(\"tweedie_variance_power\", 1.1, 1.6)\n",
        "                                  if objective_choice == \"tweedie\" else None,\n",
        "\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True),\n",
        "        \"n_estimators\":  trial.suggest_int(\"n_estimators\", 3000, 12000, step=1000),\n",
        "        \"num_leaves\":    trial.suggest_int(\"num_leaves\", 31, 127, step=8),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 60, 220, step=20),\n",
        "\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.95),\n",
        "        \"subsample_freq\": 1,\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.8),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 5.0),\n",
        "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.2),\n",
        "\n",
        "        **DEVICE_PARAMS,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    if params[\"objective\"] != \"tweedie\":\n",
        "        params.pop(\"tweedie_variance_power\", None)\n",
        "\n",
        "    model = LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_trn, y_trn,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"rmse\",\n",
        "        categorical_feature=cat_features,      # ← pandas category로 캐스팅되어 있어야 함\n",
        "        callbacks=[log_evaluation(200), early_stopping(200)],\n",
        "    )\n",
        "    pred = model.predict(X_val, num_iteration=getattr(model, \"best_iteration_\", None))\n",
        "    rmse = float(np.sqrt(((y_val - pred)**2).mean()))\n",
        "    return rmse\n",
        "\n",
        "# ===== 2) Study 실행 =====\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
        ")\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)   # 시간/자원에 맞게 n_trials 조절\n",
        "\n",
        "print(\"Best RMSE :\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)\n",
        "\n",
        "# ===== 3) 최적 파라미터로 재학습 + 저장 =====\n",
        "best_params = study.best_params.copy()\n",
        "if best_params.get(\"objective\") != \"tweedie\":\n",
        "    best_params.pop(\"tweedie_variance_power\", None)\n",
        "best_params.update(DEVICE_PARAMS)\n",
        "best_params.update({\"random_state\": 42, \"n_jobs\": -1})\n",
        "\n",
        "best_model = LGBMRegressor(**best_params)\n",
        "best_model.fit(\n",
        "    X_trn, y_trn,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    eval_metric=\"rmse\",\n",
        "    categorical_feature=cat_features,\n",
        "    callbacks=[log_evaluation(200), early_stopping(200)],\n",
        ")\n",
        "\n",
        "import joblib, os\n",
        "os.makedirs(\"/content/drive/MyDrive/lg_aimers_2/models\", exist_ok=True)\n",
        "joblib.dump({\"model\": best_model, \"features\": list(X_trn.columns), \"cat_idx\": cat_features},\n",
        "            \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_03_menu_demand_optuna_gpu.pkl\")\n",
        "print(\"✅ Saved -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_03_menu_demand_optuna_gpu.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "43152d3849664f889992597e225418f9",
            "47daabe9d9544367821180fc3b1ef1bc",
            "0a1749457e034881a9826d642e99b39d",
            "030a3d3228b747dd840fccec5df1b3e7",
            "11a628104e3c4fe6adda07e63c82d326",
            "0d8f6ffb864b49d1ba436e07ffa64997",
            "b036a5b8d58248f281b201a9baa64295",
            "48f7ed7062744e3ea97df76e850d2f44",
            "c7071a35e0d04508b60234d6cf417d52",
            "19da4bb30cdf48b19b321521f4772c93",
            "2de81e663696400190029c9d773b2517"
          ]
        },
        "id": "zqhrWrQ2BRLK",
        "outputId": "e445203b-4f78-40f8-ee7e-2392ab6ac72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-7c39fd62-5369-75a5-41a2-79b33b9db1e6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-18 07:25:44,491] A new study created in memory with name: no-name-561b8925-a216-4043-bce2-635e341ed389\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43152d3849664f889992597e225418f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005096 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6617\tvalid_0's poisson: -16.3716\n",
            "[400]\tvalid_0's rmse: 15.2341\tvalid_0's poisson: -16.903\n",
            "[600]\tvalid_0's rmse: 15.0108\tvalid_0's poisson: -17.0421\n",
            "[800]\tvalid_0's rmse: 14.946\tvalid_0's poisson: -17.0828\n",
            "[1000]\tvalid_0's rmse: 14.9175\tvalid_0's poisson: -17.0975\n",
            "[1200]\tvalid_0's rmse: 14.9006\tvalid_0's poisson: -17.1\n",
            "Early stopping, best iteration is:\n",
            "[1023]\tvalid_0's rmse: 14.9007\tvalid_0's poisson: -17.1052\n",
            "[I 2025-08-18 07:26:05,968] Trial 0 finished with value: 14.900692741444301 and parameters: {'objective': 'poisson', 'learning_rate': 0.026208630215377525, 'n_estimators': 4000, 'num_leaves': 47, 'min_child_samples': 60, 'subsample': 0.9165440364437337, 'colsample_bytree': 0.7803345035229626, 'reg_alpha': 0.5664580622368364, 'reg_lambda': 0.592630224331111, 'min_split_gain': 0.19398197043239887}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005081 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9061\tvalid_0's l2: 253.003\n",
            "[400]\tvalid_0's rmse: 15.7279\tvalid_0's l2: 247.367\n",
            "[600]\tvalid_0's rmse: 15.6723\tvalid_0's l2: 245.622\n",
            "[800]\tvalid_0's rmse: 15.619\tvalid_0's l2: 243.953\n",
            "[1000]\tvalid_0's rmse: 15.6123\tvalid_0's l2: 243.745\n",
            "[1200]\tvalid_0's rmse: 15.6351\tvalid_0's l2: 244.457\n",
            "Early stopping, best iteration is:\n",
            "[1024]\tvalid_0's rmse: 15.6041\tvalid_0's l2: 243.488\n",
            "[I 2025-08-18 07:26:37,053] Trial 1 finished with value: 15.604092210160374 and parameters: {'objective': 'regression', 'learning_rate': 0.013433656868034296, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 120, 'subsample': 0.7728072850495105, 'colsample_bytree': 0.7835558684167139, 'reg_alpha': 0.11159508852163347, 'reg_lambda': 1.8146509184084816, 'min_split_gain': 0.07327236865873835}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004907 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.919\tvalid_0's poisson: -16.102\n",
            "[400]\tvalid_0's rmse: 15.5066\tvalid_0's poisson: -16.7057\n",
            "[600]\tvalid_0's rmse: 15.2521\tvalid_0's poisson: -16.8955\n",
            "[800]\tvalid_0's rmse: 15.144\tvalid_0's poisson: -16.9545\n",
            "[1000]\tvalid_0's rmse: 15.0741\tvalid_0's poisson: -16.9936\n",
            "[1200]\tvalid_0's rmse: 15.0246\tvalid_0's poisson: -17.0216\n",
            "[1400]\tvalid_0's rmse: 15.0013\tvalid_0's poisson: -17.0343\n",
            "[1600]\tvalid_0's rmse: 14.9747\tvalid_0's poisson: -17.0376\n",
            "Early stopping, best iteration is:\n",
            "[1554]\tvalid_0's rmse: 14.962\tvalid_0's poisson: -17.0452\n",
            "[I 2025-08-18 07:26:56,058] Trial 2 finished with value: 14.96199551102866 and parameters: {'objective': 'poisson', 'learning_rate': 0.022878863522445895, 'n_estimators': 8000, 'num_leaves': 31, 'min_child_samples': 160, 'subsample': 0.7426310309218228, 'colsample_bytree': 0.6195154778955838, 'reg_alpha': 0.7591084298026667, 'reg_lambda': 4.845344148835517, 'min_split_gain': 0.16167946962329224}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005153 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.2929\tvalid_0's tweedie: 13.8741\n",
            "[400]\tvalid_0's rmse: 15.621\tvalid_0's tweedie: 13.5249\n",
            "[600]\tvalid_0's rmse: 15.1861\tvalid_0's tweedie: 13.4214\n",
            "[800]\tvalid_0's rmse: 15.0126\tvalid_0's tweedie: 13.3871\n",
            "[1000]\tvalid_0's rmse: 14.9645\tvalid_0's tweedie: 13.3794\n",
            "[1200]\tvalid_0's rmse: 14.9622\tvalid_0's tweedie: 13.3766\n",
            "Early stopping, best iteration is:\n",
            "[1046]\tvalid_0's rmse: 14.9448\tvalid_0's tweedie: 13.375\n",
            "[I 2025-08-18 07:27:11,653] Trial 3 finished with value: 14.944764408016086 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.3200762468698009, 'learning_rate': 0.01217029388373879, 'n_estimators': 7000, 'num_leaves': 31, 'min_child_samples': 220, 'subsample': 0.7646949954000042, 'colsample_bytree': 0.7987566853061946, 'reg_alpha': 0.24936886087152876, 'reg_lambda': 2.8403060953001487, 'min_split_gain': 0.10934205586865593}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004870 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5524\tvalid_0's poisson: -16.7352\n",
            "[400]\tvalid_0's rmse: 15.122\tvalid_0's poisson: -16.9835\n",
            "[600]\tvalid_0's rmse: 14.9472\tvalid_0's poisson: -17.0196\n",
            "Early stopping, best iteration is:\n",
            "[475]\tvalid_0's rmse: 14.9932\tvalid_0's poisson: -17.0374\n",
            "[I 2025-08-18 07:27:28,287] Trial 4 finished with value: 14.993195320007358 and parameters: {'objective': 'poisson', 'learning_rate': 0.04536089127921624, 'n_estimators': 11000, 'num_leaves': 87, 'min_child_samples': 220, 'subsample': 0.7221231255129799, 'colsample_bytree': 0.6587948587257435, 'reg_alpha': 0.03618183112843045, 'reg_lambda': 1.9639864884346896, 'min_split_gain': 0.07773545793789641}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004886 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.4698\tvalid_0's poisson: -15.4189\n",
            "[400]\tvalid_0's rmse: 15.8728\tvalid_0's poisson: -16.4078\n",
            "[600]\tvalid_0's rmse: 15.5648\tvalid_0's poisson: -16.7129\n",
            "[800]\tvalid_0's rmse: 15.3622\tvalid_0's poisson: -16.8482\n",
            "[1000]\tvalid_0's rmse: 15.2534\tvalid_0's poisson: -16.9151\n",
            "[1200]\tvalid_0's rmse: 15.1439\tvalid_0's poisson: -16.9657\n",
            "[1400]\tvalid_0's rmse: 15.0971\tvalid_0's poisson: -16.9841\n",
            "[1600]\tvalid_0's rmse: 15.0694\tvalid_0's poisson: -16.9962\n",
            "[1800]\tvalid_0's rmse: 15.0591\tvalid_0's poisson: -16.998\n",
            "[2000]\tvalid_0's rmse: 15.0347\tvalid_0's poisson: -17.0015\n",
            "Early stopping, best iteration is:\n",
            "[1892]\tvalid_0's rmse: 15.0433\tvalid_0's poisson: -17.0058\n",
            "[I 2025-08-18 07:27:56,077] Trial 5 finished with value: 15.043281583582173 and parameters: {'objective': 'poisson', 'learning_rate': 0.015716824201651516, 'n_estimators': 8000, 'num_leaves': 39, 'min_child_samples': 200, 'subsample': 0.7186376609199426, 'colsample_bytree': 0.8960660809801553, 'reg_alpha': 0.617795815437326, 'reg_lambda': 1.3942205669037757, 'min_split_gain': 0.00110442342472048}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004994 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8133\tvalid_0's l2: 250.06\n",
            "[400]\tvalid_0's rmse: 15.6143\tvalid_0's l2: 243.807\n",
            "[600]\tvalid_0's rmse: 15.6568\tvalid_0's l2: 245.134\n",
            "Early stopping, best iteration is:\n",
            "[469]\tvalid_0's rmse: 15.5835\tvalid_0's l2: 242.846\n",
            "[I 2025-08-18 07:28:07,275] Trial 6 finished with value: 15.583532923946507 and parameters: {'objective': 'regression', 'learning_rate': 0.03460149293996123, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 80, 'subsample': 0.9157758564688984, 'colsample_bytree': 0.7869894380482674, 'reg_alpha': 0.26471841988211936, 'reg_lambda': 0.7860125762871064, 'min_split_gain': 0.062196464343132446}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004973 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6823\tvalid_0's poisson: -16.6425\n",
            "[400]\tvalid_0's rmse: 15.319\tvalid_0's poisson: -16.9078\n",
            "[600]\tvalid_0's rmse: 15.1742\tvalid_0's poisson: -16.9763\n",
            "[800]\tvalid_0's rmse: 15.189\tvalid_0's poisson: -16.9569\n",
            "Early stopping, best iteration is:\n",
            "[635]\tvalid_0's rmse: 15.1514\tvalid_0's poisson: -16.9891\n",
            "[I 2025-08-18 07:28:16,304] Trial 7 finished with value: 15.151437579554813 and parameters: {'objective': 'poisson', 'learning_rate': 0.04169990777997927, 'n_estimators': 7000, 'num_leaves': 39, 'min_child_samples': 180, 'subsample': 0.8901962621542243, 'colsample_bytree': 0.7683831592708489, 'reg_alpha': 0.6167737439636488, 'reg_lambda': 2.7220801836397586, 'min_split_gain': 0.10454656587639882}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004831 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.0592\tvalid_0's l2: 257.897\n",
            "[400]\tvalid_0's rmse: 15.7075\tvalid_0's l2: 246.725\n",
            "[600]\tvalid_0's rmse: 15.5855\tvalid_0's l2: 242.907\n",
            "[800]\tvalid_0's rmse: 15.5331\tvalid_0's l2: 241.276\n",
            "[1000]\tvalid_0's rmse: 15.513\tvalid_0's l2: 240.652\n",
            "[1200]\tvalid_0's rmse: 15.4839\tvalid_0's l2: 239.752\n",
            "[1400]\tvalid_0's rmse: 15.4724\tvalid_0's l2: 239.394\n",
            "[1600]\tvalid_0's rmse: 15.4652\tvalid_0's l2: 239.172\n",
            "Early stopping, best iteration is:\n",
            "[1563]\tvalid_0's rmse: 15.4605\tvalid_0's l2: 239.026\n",
            "[I 2025-08-18 07:28:46,453] Trial 8 finished with value: 15.460459853767553 and parameters: {'objective': 'regression', 'learning_rate': 0.01051884505877539, 'n_estimators': 9000, 'num_leaves': 63, 'min_child_samples': 140, 'subsample': 0.9268916184815232, 'colsample_bytree': 0.6747876687446624, 'reg_alpha': 0.3283063384285038, 'reg_lambda': 3.899980123443719, 'min_split_gain': 0.045759633098324495}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004928 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4393\tvalid_0's poisson: -16.8068\n",
            "[400]\tvalid_0's rmse: 15.302\tvalid_0's poisson: -16.8959\n",
            "Early stopping, best iteration is:\n",
            "[283]\tvalid_0's rmse: 15.2862\tvalid_0's poisson: -16.885\n",
            "[I 2025-08-18 07:28:55,511] Trial 9 finished with value: 15.286195385541445 and parameters: {'objective': 'poisson', 'learning_rate': 0.044650957058567906, 'n_estimators': 11000, 'num_leaves': 95, 'min_child_samples': 200, 'subsample': 0.9009180192247785, 'colsample_bytree': 0.6559710176658108, 'reg_alpha': 0.7140471987919823, 'reg_lambda': 2.927040088620428, 'min_split_gain': 0.1614880310328125}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004961 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3317\tvalid_0's tweedie: 7.0518\n",
            "Early stopping, best iteration is:\n",
            "[180]\tvalid_0's rmse: 15.3889\tvalid_0's tweedie: 7.04694\n",
            "[I 2025-08-18 07:29:06,408] Trial 10 finished with value: 15.388926710692765 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.5403315984044441, 'learning_rate': 0.025175090048784612, 'n_estimators': 3000, 'num_leaves': 119, 'min_child_samples': 60, 'subsample': 0.8380488628337971, 'colsample_bytree': 0.8611890069836565, 'reg_alpha': 0.47956112594526357, 'reg_lambda': 0.5554002630422179, 'min_split_gain': 0.1984808282297822}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004839 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6201\tvalid_0's tweedie: 29.8704\n",
            "[400]\tvalid_0's rmse: 15.2041\tvalid_0's tweedie: 29.6482\n",
            "[600]\tvalid_0's rmse: 15.1282\tvalid_0's tweedie: 29.6329\n",
            "Early stopping, best iteration is:\n",
            "[599]\tvalid_0's rmse: 15.1265\tvalid_0's tweedie: 29.6319\n",
            "[I 2025-08-18 07:29:19,999] Trial 11 finished with value: 15.126526380151004 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.1780746432804705, 'learning_rate': 0.01753544859975535, 'n_estimators': 5000, 'num_leaves': 55, 'min_child_samples': 100, 'subsample': 0.8098048270080273, 'colsample_bytree': 0.8366712419489499, 'reg_alpha': 0.4416209920604153, 'reg_lambda': 3.072372562063103, 'min_split_gain': 0.1379073202241284}. Best is trial 0 with value: 14.900692741444301.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004894 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1923\tvalid_0's tweedie: 12.3323\n",
            "[400]\tvalid_0's rmse: 14.8103\tvalid_0's tweedie: 12.2842\n",
            "Early stopping, best iteration is:\n",
            "[347]\tvalid_0's rmse: 14.7719\tvalid_0's tweedie: 12.2709\n",
            "[I 2025-08-18 07:29:29,191] Trial 12 finished with value: 14.771942627032583 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.3406824696246964, 'learning_rate': 0.028733660262823625, 'n_estimators': 5000, 'num_leaves': 47, 'min_child_samples': 60, 'subsample': 0.8473344025845203, 'colsample_bytree': 0.7321360225551538, 'reg_alpha': 0.21545738155048322, 'reg_lambda': 3.757827839818135, 'min_split_gain': 0.11816155970431218}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005036 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.0693\tvalid_0's tweedie: 8.69714\n",
            "[400]\tvalid_0's rmse: 14.9807\tvalid_0's tweedie: 8.71008\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 14.838\tvalid_0's tweedie: 8.67137\n",
            "[I 2025-08-18 07:29:35,479] Trial 13 finished with value: 14.8380292236554 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4441147559750998, 'learning_rate': 0.030140271419959973, 'n_estimators': 5000, 'num_leaves': 55, 'min_child_samples': 60, 'subsample': 0.8607631004894194, 'colsample_bytree': 0.7205354307837304, 'reg_alpha': 0.13973193452639981, 'reg_lambda': 3.98348580330859, 'min_split_gain': 0.19644038465105063}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.009478 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1704\tvalid_0's tweedie: 8.98521\n",
            "[400]\tvalid_0's rmse: 15.1175\tvalid_0's tweedie: 8.97127\n",
            "Early stopping, best iteration is:\n",
            "[328]\tvalid_0's rmse: 15.0823\tvalid_0's tweedie: 8.95775\n",
            "[I 2025-08-18 07:29:45,578] Trial 14 finished with value: 15.08225637184528 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4323089855073925, 'learning_rate': 0.03229463371434206, 'n_estimators': 5000, 'num_leaves': 63, 'min_child_samples': 100, 'subsample': 0.8611095904474834, 'colsample_bytree': 0.7219020382351226, 'reg_alpha': 0.14674139628764266, 'reg_lambda': 4.096411634066236, 'min_split_gain': 0.13591433592146776}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004955 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1225\tvalid_0's tweedie: 12.2441\n",
            "[400]\tvalid_0's rmse: 14.9952\tvalid_0's tweedie: 12.2443\n",
            "Early stopping, best iteration is:\n",
            "[278]\tvalid_0's rmse: 14.9237\tvalid_0's tweedie: 12.21\n",
            "[I 2025-08-18 07:29:56,043] Trial 15 finished with value: 14.923652301737606 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.3423578866620045, 'learning_rate': 0.029868159534627674, 'n_estimators': 5000, 'num_leaves': 79, 'min_child_samples': 80, 'subsample': 0.8495858563754616, 'colsample_bytree': 0.7226096141222577, 'reg_alpha': 0.17062666850489772, 'reg_lambda': 4.043507614039044, 'min_split_gain': 0.1657137761974405}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004877 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4429\tvalid_0's tweedie: 8.11491\n",
            "[400]\tvalid_0's rmse: 15.1609\tvalid_0's tweedie: 8.10581\n",
            "Early stopping, best iteration is:\n",
            "[312]\tvalid_0's rmse: 15.157\tvalid_0's tweedie: 8.08062\n",
            "[I 2025-08-18 07:30:10,550] Trial 16 finished with value: 15.157018886024092 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4743163425029917, 'learning_rate': 0.0184389557422008, 'n_estimators': 6000, 'num_leaves': 127, 'min_child_samples': 60, 'subsample': 0.8065866665235234, 'colsample_bytree': 0.7141971828688197, 'reg_alpha': 0.056027728217411116, 'reg_lambda': 4.75015972136493, 'min_split_gain': 0.023620813970197163}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004957 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3598\tvalid_0's tweedie: 18.6049\n",
            "[400]\tvalid_0's rmse: 15.3759\tvalid_0's tweedie: 18.6646\n",
            "Early stopping, best iteration is:\n",
            "[216]\tvalid_0's rmse: 15.2923\tvalid_0's tweedie: 18.5863\n",
            "[I 2025-08-18 07:30:21,518] Trial 17 finished with value: 15.292293458899765 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.25270442229342, 'learning_rate': 0.03716050545902338, 'n_estimators': 4000, 'num_leaves': 103, 'min_child_samples': 100, 'subsample': 0.8800956642347831, 'colsample_bytree': 0.6903091479975801, 'reg_alpha': 0.34359033855726584, 'reg_lambda': 3.4288645527933586, 'min_split_gain': 0.1276461582762414}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004897 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4986\tvalid_0's tweedie: 9.29246\n",
            "[400]\tvalid_0's rmse: 15.1351\tvalid_0's tweedie: 9.19666\n",
            "[600]\tvalid_0's rmse: 15.1002\tvalid_0's tweedie: 9.19401\n",
            "Early stopping, best iteration is:\n",
            "[468]\tvalid_0's rmse: 15.0633\tvalid_0's tweedie: 9.18656\n",
            "[I 2025-08-18 07:30:33,065] Trial 18 finished with value: 15.063319814700135 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4239766922291226, 'learning_rate': 0.020438101482341018, 'n_estimators': 9000, 'num_leaves': 55, 'min_child_samples': 140, 'subsample': 0.9488003374441406, 'colsample_bytree': 0.7481244429743636, 'reg_alpha': 0.2079793841348618, 'reg_lambda': 3.5817875576098124, 'min_split_gain': 0.1737921968153107}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.014971 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3281\tvalid_0's tweedie: 6.64362\n",
            "[400]\tvalid_0's rmse: 15.2271\tvalid_0's tweedie: 6.64506\n",
            "Early stopping, best iteration is:\n",
            "[268]\tvalid_0's rmse: 15.1712\tvalid_0's tweedie: 6.62415\n",
            "[I 2025-08-18 07:30:40,622] Trial 19 finished with value: 15.17117575604452 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.5761582950267283, 'learning_rate': 0.02614064619462517, 'n_estimators': 6000, 'num_leaves': 71, 'min_child_samples': 80, 'subsample': 0.8136064016947593, 'colsample_bytree': 0.6052075210517306, 'reg_alpha': 0.010006176366624764, 'reg_lambda': 4.4496670960659195, 'min_split_gain': 0.09412618448364685}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005230 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.2061\tvalid_0's tweedie: 14.9935\n",
            "[400]\tvalid_0's rmse: 14.9382\tvalid_0's tweedie: 14.951\n",
            "Early stopping, best iteration is:\n",
            "[367]\tvalid_0's rmse: 14.8928\tvalid_0's tweedie: 14.9382\n",
            "[I 2025-08-18 07:30:50,872] Trial 20 finished with value: 14.892846283785941 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.295222940587092, 'learning_rate': 0.029321745620757518, 'n_estimators': 4000, 'num_leaves': 55, 'min_child_samples': 60, 'subsample': 0.8692311249906453, 'colsample_bytree': 0.8183212661013632, 'reg_alpha': 0.3360765773593602, 'reg_lambda': 2.278713232586554, 'min_split_gain': 0.12517225778759677}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004923 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.2305\tvalid_0's tweedie: 16.9067\n",
            "[400]\tvalid_0's rmse: 15.0652\tvalid_0's tweedie: 16.8684\n",
            "[600]\tvalid_0's rmse: 15.089\tvalid_0's tweedie: 16.8982\n",
            "Early stopping, best iteration is:\n",
            "[476]\tvalid_0's rmse: 15.0288\tvalid_0's tweedie: 16.8613\n",
            "[I 2025-08-18 07:31:01,582] Trial 21 finished with value: 15.028769013833045 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.2706760789872549, 'learning_rate': 0.030878968094364717, 'n_estimators': 4000, 'num_leaves': 47, 'min_child_samples': 60, 'subsample': 0.8619099479243723, 'colsample_bytree': 0.822599944822544, 'reg_alpha': 0.35018080302884336, 'reg_lambda': 2.4431226596345836, 'min_split_gain': 0.1211176732354673}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004980 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.0989\tvalid_0's tweedie: 10.1422\n",
            "[400]\tvalid_0's rmse: 15.2112\tvalid_0's tweedie: 10.1709\n",
            "Early stopping, best iteration is:\n",
            "[203]\tvalid_0's rmse: 15.0967\tvalid_0's tweedie: 10.141\n",
            "[I 2025-08-18 07:31:06,834] Trial 22 finished with value: 15.09665838094537 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.393634709051956, 'learning_rate': 0.038012693546335206, 'n_estimators': 3000, 'num_leaves': 47, 'min_child_samples': 80, 'subsample': 0.832453534172562, 'colsample_bytree': 0.7576771449730372, 'reg_alpha': 0.25482579987999576, 'reg_lambda': 3.448080256041726, 'min_split_gain': 0.09017704435681473}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.006770 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4417\tvalid_0's tweedie: 54.1507\n",
            "[400]\tvalid_0's rmse: 15.1542\tvalid_0's tweedie: 54.0605\n",
            "Early stopping, best iteration is:\n",
            "[395]\tvalid_0's rmse: 15.1381\tvalid_0's tweedie: 54.0527\n",
            "[I 2025-08-18 07:31:18,544] Trial 23 finished with value: 15.138055134548502 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.1105086194874143, 'learning_rate': 0.028114123062348853, 'n_estimators': 5000, 'num_leaves': 71, 'min_child_samples': 120, 'subsample': 0.8769367431678003, 'colsample_bytree': 0.7390114781114205, 'reg_alpha': 0.11356377690422204, 'reg_lambda': 2.200768462060135, 'min_split_gain': 0.14301073391311286}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004932 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.2502\tvalid_0's tweedie: 16.7252\n",
            "[400]\tvalid_0's rmse: 15.0438\tvalid_0's tweedie: 16.6598\n",
            "[600]\tvalid_0's rmse: 14.9433\tvalid_0's tweedie: 16.6569\n",
            "Early stopping, best iteration is:\n",
            "[492]\tvalid_0's rmse: 14.9651\tvalid_0's tweedie: 16.6506\n",
            "[I 2025-08-18 07:31:30,146] Trial 24 finished with value: 14.965140369024036 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.2732674722004431, 'learning_rate': 0.02236751815045728, 'n_estimators': 4000, 'num_leaves': 55, 'min_child_samples': 60, 'subsample': 0.8649999357124747, 'colsample_bytree': 0.6956502458694308, 'reg_alpha': 0.3937337837087067, 'reg_lambda': 4.242043155905131, 'min_split_gain': 0.15001393515805833}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004869 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7947\tvalid_0's l2: 249.472\n",
            "[400]\tvalid_0's rmse: 15.6066\tvalid_0's l2: 243.565\n",
            "[600]\tvalid_0's rmse: 15.5466\tvalid_0's l2: 241.697\n",
            "[800]\tvalid_0's rmse: 15.5491\tvalid_0's l2: 241.775\n",
            "Early stopping, best iteration is:\n",
            "[684]\tvalid_0's rmse: 15.5012\tvalid_0's l2: 240.289\n",
            "[I 2025-08-18 07:31:41,552] Trial 25 finished with value: 15.50124647986214 and parameters: {'objective': 'regression', 'learning_rate': 0.034222477420411276, 'n_estimators': 6000, 'num_leaves': 39, 'min_child_samples': 80, 'subsample': 0.7900331000928404, 'colsample_bytree': 0.827044970672731, 'reg_alpha': 0.2831927405678099, 'reg_lambda': 3.761679401218504, 'min_split_gain': 0.18626805235891664}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005004 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.414\tvalid_0's tweedie: 7.75392\n",
            "[400]\tvalid_0's rmse: 15.2299\tvalid_0's tweedie: 7.72527\n",
            "Early stopping, best iteration is:\n",
            "[301]\tvalid_0's rmse: 15.1912\tvalid_0's tweedie: 7.7176\n",
            "[I 2025-08-18 07:31:52,385] Trial 26 finished with value: 15.191208968326945 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4917856016895499, 'learning_rate': 0.024421804723138596, 'n_estimators': 4000, 'num_leaves': 71, 'min_child_samples': 120, 'subsample': 0.8405708234845692, 'colsample_bytree': 0.8744948753086479, 'reg_alpha': 0.20482245785254422, 'reg_lambda': 1.3636607182878215, 'min_split_gain': 0.11799467339735001}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005037 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4943\tvalid_0's tweedie: 10.9905\n",
            "[400]\tvalid_0's rmse: 15.1055\tvalid_0's tweedie: 10.8864\n",
            "[600]\tvalid_0's rmse: 15.1154\tvalid_0's tweedie: 10.8926\n",
            "Early stopping, best iteration is:\n",
            "[495]\tvalid_0's rmse: 15.0684\tvalid_0's tweedie: 10.8771\n",
            "[I 2025-08-18 07:32:04,522] Trial 27 finished with value: 15.068353259918437 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.373029783320441, 'learning_rate': 0.019892529903475967, 'n_estimators': 7000, 'num_leaves': 55, 'min_child_samples': 100, 'subsample': 0.8241939060585655, 'colsample_bytree': 0.8076619504707518, 'reg_alpha': 0.49827424371704504, 'reg_lambda': 4.413792775130947, 'min_split_gain': 0.03936774155620995}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004987 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1289\tvalid_0's tweedie: 24.8182\n",
            "[400]\tvalid_0's rmse: 15.0456\tvalid_0's tweedie: 24.7913\n",
            "Early stopping, best iteration is:\n",
            "[256]\tvalid_0's rmse: 14.9148\tvalid_0's tweedie: 24.7517\n",
            "[I 2025-08-18 07:32:10,499] Trial 28 finished with value: 14.914811674860655 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.2036664574319835, 'learning_rate': 0.028982752155152527, 'n_estimators': 12000, 'num_leaves': 47, 'min_child_samples': 60, 'subsample': 0.8968021285954371, 'colsample_bytree': 0.8506360342629979, 'reg_alpha': 0.07567807081293532, 'reg_lambda': 3.2040429064258182, 'min_split_gain': 0.17758801954794445}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004922 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5923\tvalid_0's l2: 243.118\n",
            "[400]\tvalid_0's rmse: 15.439\tvalid_0's l2: 238.364\n",
            "[600]\tvalid_0's rmse: 15.3579\tvalid_0's l2: 235.865\n",
            "[800]\tvalid_0's rmse: 15.2976\tvalid_0's l2: 234.016\n",
            "Early stopping, best iteration is:\n",
            "[796]\tvalid_0's rmse: 15.2827\tvalid_0's l2: 233.561\n",
            "[I 2025-08-18 07:32:20,949] Trial 29 finished with value: 15.282708092095548 and parameters: {'objective': 'regression', 'learning_rate': 0.048535905199311676, 'n_estimators': 3000, 'num_leaves': 31, 'min_child_samples': 60, 'subsample': 0.9166076480262071, 'colsample_bytree': 0.7377404107873928, 'reg_alpha': 0.3908560815469628, 'reg_lambda': 2.454143451919067, 'min_split_gain': 0.15155745867140843}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004948 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1259\tvalid_0's tweedie: 13.629\n",
            "[400]\tvalid_0's rmse: 15.0283\tvalid_0's tweedie: 13.6095\n",
            "Early stopping, best iteration is:\n",
            "[388]\tvalid_0's rmse: 14.9926\tvalid_0's tweedie: 13.6042\n",
            "[I 2025-08-18 07:32:29,845] Trial 30 finished with value: 14.992589214507525 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.3165882826342594, 'learning_rate': 0.0392705232083144, 'n_estimators': 5000, 'num_leaves': 39, 'min_child_samples': 80, 'subsample': 0.791334116141804, 'colsample_bytree': 0.7670134104824081, 'reg_alpha': 0.1819602376637783, 'reg_lambda': 3.199736081589186, 'min_split_gain': 0.18853546382930347}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.004879 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6441\tvalid_0's poisson: -16.3604\n",
            "[400]\tvalid_0's rmse: 15.2171\tvalid_0's poisson: -16.8703\n",
            "[600]\tvalid_0's rmse: 15.0595\tvalid_0's poisson: -16.9839\n",
            "Early stopping, best iteration is:\n",
            "[559]\tvalid_0's rmse: 15.053\tvalid_0's poisson: -16.984\n",
            "[I 2025-08-18 07:32:41,404] Trial 31 finished with value: 15.053033305192274 and parameters: {'objective': 'poisson', 'learning_rate': 0.026687836206310357, 'n_estimators': 4000, 'num_leaves': 47, 'min_child_samples': 60, 'subsample': 0.9425986183116681, 'colsample_bytree': 0.7797181304200771, 'reg_alpha': 0.5872783136387114, 'reg_lambda': 1.4444935555535483, 'min_split_gain': 0.19870670769404342}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005901 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3536\tvalid_0's poisson: -16.6369\n",
            "[400]\tvalid_0's rmse: 14.9205\tvalid_0's poisson: -17.0213\n",
            "[600]\tvalid_0's rmse: 14.9074\tvalid_0's poisson: -17.06\n",
            "[800]\tvalid_0's rmse: 14.9062\tvalid_0's poisson: -17.0478\n",
            "Early stopping, best iteration is:\n",
            "[695]\tvalid_0's rmse: 14.8561\tvalid_0's poisson: -17.0785\n",
            "[I 2025-08-18 07:32:57,053] Trial 32 finished with value: 14.856097637381431 and parameters: {'objective': 'poisson', 'learning_rate': 0.032831595934534115, 'n_estimators': 6000, 'num_leaves': 63, 'min_child_samples': 80, 'subsample': 0.8750364757758538, 'colsample_bytree': 0.7037616918626007, 'reg_alpha': 0.10141803674703415, 'reg_lambda': 1.0779549126024255, 'min_split_gain': 0.07846728156274549}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005052 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4456\tvalid_0's poisson: -16.6312\n",
            "[400]\tvalid_0's rmse: 15.111\tvalid_0's poisson: -16.9844\n",
            "[600]\tvalid_0's rmse: 15.0599\tvalid_0's poisson: -17.0204\n",
            "Early stopping, best iteration is:\n",
            "[528]\tvalid_0's rmse: 15.0397\tvalid_0's poisson: -17.0269\n",
            "[I 2025-08-18 07:33:10,163] Trial 33 finished with value: 15.039725917649916 and parameters: {'objective': 'poisson', 'learning_rate': 0.033549919731664025, 'n_estimators': 6000, 'num_leaves': 63, 'min_child_samples': 80, 'subsample': 0.8522132083411104, 'colsample_bytree': 0.70522898366953, 'reg_alpha': 0.09790394279453601, 'reg_lambda': 1.7356029985562036, 'min_split_gain': 0.06055696268438837}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005106 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3373\tvalid_0's poisson: -16.5932\n",
            "[400]\tvalid_0's rmse: 15.1135\tvalid_0's poisson: -16.95\n",
            "[600]\tvalid_0's rmse: 15.1007\tvalid_0's poisson: -16.9669\n",
            "Early stopping, best iteration is:\n",
            "[484]\tvalid_0's rmse: 15.0517\tvalid_0's poisson: -16.9875\n",
            "[I 2025-08-18 07:33:23,868] Trial 34 finished with value: 15.051747071339031 and parameters: {'objective': 'poisson', 'learning_rate': 0.028548572579940718, 'n_estimators': 5000, 'num_leaves': 79, 'min_child_samples': 60, 'subsample': 0.8729934413915637, 'colsample_bytree': 0.6432833088019791, 'reg_alpha': 0.13677369277809237, 'reg_lambda': 0.9693620999639375, 'min_split_gain': 0.10483390433646106}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005151 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8139\tvalid_0's poisson: -16.1602\n",
            "[400]\tvalid_0's rmse: 15.521\tvalid_0's poisson: -16.752\n",
            "[600]\tvalid_0's rmse: 15.2526\tvalid_0's poisson: -16.9169\n",
            "[800]\tvalid_0's rmse: 15.1338\tvalid_0's poisson: -16.9827\n",
            "Early stopping, best iteration is:\n",
            "[783]\tvalid_0's rmse: 15.117\tvalid_0's poisson: -16.9891\n",
            "[I 2025-08-18 07:33:39,495] Trial 35 finished with value: 15.117011312058382 and parameters: {'objective': 'poisson', 'learning_rate': 0.02212393545584599, 'n_estimators': 7000, 'num_leaves': 55, 'min_child_samples': 100, 'subsample': 0.8515476913462632, 'colsample_bytree': 0.6833614299118054, 'reg_alpha': 0.29345038587907624, 'reg_lambda': 1.1601605184364834, 'min_split_gain': 0.07207587166945956}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005187 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.645\tvalid_0's poisson: -16.6379\n",
            "[400]\tvalid_0's rmse: 15.3781\tvalid_0's poisson: -16.8638\n",
            "[600]\tvalid_0's rmse: 15.2293\tvalid_0's poisson: -16.9182\n",
            "[800]\tvalid_0's rmse: 15.2392\tvalid_0's poisson: -16.8876\n",
            "Early stopping, best iteration is:\n",
            "[603]\tvalid_0's rmse: 15.2208\tvalid_0's poisson: -16.9242\n",
            "[I 2025-08-18 07:33:56,862] Trial 36 finished with value: 15.220830034578663 and parameters: {'objective': 'poisson', 'learning_rate': 0.03622024007899615, 'n_estimators': 6000, 'num_leaves': 87, 'min_child_samples': 160, 'subsample': 0.8841951575871799, 'colsample_bytree': 0.7397866960486102, 'reg_alpha': 0.22119451649984392, 'reg_lambda': 4.872403744202272, 'min_split_gain': 0.08538316603932003}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.040781 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.725228\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5844\tvalid_0's l2: 242.874\n",
            "[400]\tvalid_0's rmse: 15.4732\tvalid_0's l2: 239.419\n",
            "[600]\tvalid_0's rmse: 15.3637\tvalid_0's l2: 236.042\n",
            "Early stopping, best iteration is:\n",
            "[590]\tvalid_0's rmse: 15.3493\tvalid_0's l2: 235.601\n",
            "[I 2025-08-18 07:34:13,232] Trial 37 finished with value: 15.349306407926791 and parameters: {'objective': 'regression', 'learning_rate': 0.02365322335637925, 'n_estimators': 8000, 'num_leaves': 71, 'min_child_samples': 80, 'subsample': 0.9059042632578823, 'colsample_bytree': 0.724021720570861, 'reg_alpha': 0.08287577359097809, 'reg_lambda': 2.0001090182351184, 'min_split_gain': 0.11523674304788002}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005494 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3955\tvalid_0's tweedie: 8.95315\n",
            "[400]\tvalid_0's rmse: 14.8819\tvalid_0's tweedie: 8.8942\n",
            "[600]\tvalid_0's rmse: 14.9539\tvalid_0's tweedie: 8.92229\n",
            "Early stopping, best iteration is:\n",
            "[413]\tvalid_0's rmse: 14.8473\tvalid_0's tweedie: 8.893\n",
            "[I 2025-08-18 07:34:19,990] Trial 38 finished with value: 14.847323551774169 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4360136479774601, 'learning_rate': 0.031277445072073425, 'n_estimators': 4000, 'num_leaves': 31, 'min_child_samples': 60, 'subsample': 0.8246508250937397, 'colsample_bytree': 0.6722737302701732, 'reg_alpha': 0.01506013078263907, 'reg_lambda': 2.619023599825019, 'min_split_gain': 0.07941532780823475}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005070 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.459\tvalid_0's poisson: -16.6858\n",
            "[400]\tvalid_0's rmse: 14.9639\tvalid_0's poisson: -17.0274\n",
            "[600]\tvalid_0's rmse: 14.8963\tvalid_0's poisson: -17.0785\n",
            "Early stopping, best iteration is:\n",
            "[536]\tvalid_0's rmse: 14.8688\tvalid_0's poisson: -17.0801\n",
            "[I 2025-08-18 07:34:30,142] Trial 39 finished with value: 14.868823750390803 and parameters: {'objective': 'poisson', 'learning_rate': 0.04273271262564645, 'n_estimators': 5000, 'num_leaves': 31, 'min_child_samples': 120, 'subsample': 0.7555472006399728, 'colsample_bytree': 0.6321460588036206, 'reg_alpha': 0.024689063789862492, 'reg_lambda': 3.6861493183728147, 'min_split_gain': 0.059876418603890105}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.008076 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1331\tvalid_0's tweedie: 7.97956\n",
            "[400]\tvalid_0's rmse: 14.9496\tvalid_0's tweedie: 7.94677\n",
            "Early stopping, best iteration is:\n",
            "[287]\tvalid_0's rmse: 14.8142\tvalid_0's tweedie: 7.94217\n",
            "[I 2025-08-18 07:34:37,574] Trial 40 finished with value: 14.814178256028512 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4787326281635245, 'learning_rate': 0.03192658072838155, 'n_estimators': 7000, 'num_leaves': 39, 'min_child_samples': 80, 'subsample': 0.7025698438329145, 'colsample_bytree': 0.6671614584208367, 'reg_alpha': 0.045931082815937815, 'reg_lambda': 2.649256728118651, 'min_split_gain': 0.07908505497112059}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.013037 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.1175\tvalid_0's tweedie: 7.92239\n",
            "[400]\tvalid_0's rmse: 15.0472\tvalid_0's tweedie: 7.90532\n",
            "Early stopping, best iteration is:\n",
            "[285]\tvalid_0's rmse: 14.8673\tvalid_0's tweedie: 7.89833\n",
            "[I 2025-08-18 07:34:44,839] Trial 41 finished with value: 14.867267680339069 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.480761642818381, 'learning_rate': 0.03179185253716044, 'n_estimators': 7000, 'num_leaves': 39, 'min_child_samples': 80, 'subsample': 0.7345308002269131, 'colsample_bytree': 0.6629930923354919, 'reg_alpha': 0.044355756040552745, 'reg_lambda': 2.8462828690658957, 'min_split_gain': 0.0779338013431542}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005167 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4096\tvalid_0's tweedie: 8.83658\n",
            "[400]\tvalid_0's rmse: 14.8559\tvalid_0's tweedie: 8.74527\n",
            "[600]\tvalid_0's rmse: 14.8468\tvalid_0's tweedie: 8.74637\n",
            "Early stopping, best iteration is:\n",
            "[491]\tvalid_0's rmse: 14.8476\tvalid_0's tweedie: 8.73666\n",
            "[I 2025-08-18 07:34:54,685] Trial 42 finished with value: 14.847602464544062 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4410704669020746, 'learning_rate': 0.026646489455175927, 'n_estimators': 8000, 'num_leaves': 31, 'min_child_samples': 60, 'subsample': 0.7141058463331899, 'colsample_bytree': 0.6745784597057959, 'reg_alpha': 0.005925689904834218, 'reg_lambda': 2.6560827764242143, 'min_split_gain': 0.05067904116555048}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005163 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4442\tvalid_0's tweedie: 9.01044\n",
            "[400]\tvalid_0's rmse: 15.0039\tvalid_0's tweedie: 8.9263\n",
            "[600]\tvalid_0's rmse: 14.9306\tvalid_0's tweedie: 8.91749\n",
            "[800]\tvalid_0's rmse: 15.0032\tvalid_0's tweedie: 8.93958\n",
            "Early stopping, best iteration is:\n",
            "[613]\tvalid_0's rmse: 14.9046\tvalid_0's tweedie: 8.91565\n",
            "[I 2025-08-18 07:35:05,855] Trial 43 finished with value: 14.904600035071592 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4343965431444767, 'learning_rate': 0.02663878344760373, 'n_estimators': 9000, 'num_leaves': 31, 'min_child_samples': 60, 'subsample': 0.7034349259330575, 'colsample_bytree': 0.6709102847349973, 'reg_alpha': 0.0014492201962151285, 'reg_lambda': 2.6328810614711857, 'min_split_gain': 0.04910443119572966}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005227 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.2885\tvalid_0's tweedie: 7.35529\n",
            "[400]\tvalid_0's rmse: 14.9283\tvalid_0's tweedie: 7.30076\n",
            "[600]\tvalid_0's rmse: 14.9701\tvalid_0's tweedie: 7.31355\n",
            "Early stopping, best iteration is:\n",
            "[432]\tvalid_0's rmse: 14.8702\tvalid_0's tweedie: 7.29351\n",
            "[I 2025-08-18 07:35:13,596] Trial 44 finished with value: 14.870157936592149 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.5164878199441745, 'learning_rate': 0.026730816850986862, 'n_estimators': 10000, 'num_leaves': 39, 'min_child_samples': 60, 'subsample': 0.7037299526552837, 'colsample_bytree': 0.619900179862047, 'reg_alpha': 0.05595937320874281, 'reg_lambda': 2.603942390797145, 'min_split_gain': 0.02131355822347293}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005976 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.0675\tvalid_0's tweedie: 10.0935\n",
            "[400]\tvalid_0's rmse: 15.2099\tvalid_0's tweedie: 10.0828\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 14.9528\tvalid_0's tweedie: 10.0737\n",
            "[I 2025-08-18 07:35:20,897] Trial 45 finished with value: 14.952822037145697 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.3955442735821764, 'learning_rate': 0.04094322973295242, 'n_estimators': 8000, 'num_leaves': 31, 'min_child_samples': 60, 'subsample': 0.7196257945605032, 'colsample_bytree': 0.6520830005229894, 'reg_alpha': 0.1659599993057104, 'reg_lambda': 3.0259062759615563, 'min_split_gain': 0.03213898652793619}. Best is trial 12 with value: 14.771942627032583.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005397 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.9266\tvalid_0's tweedie: 8.59271\n",
            "[400]\tvalid_0's rmse: 14.7936\tvalid_0's tweedie: 8.56708\n",
            "Early stopping, best iteration is:\n",
            "[307]\tvalid_0's rmse: 14.66\tvalid_0's tweedie: 8.55647\n",
            "[I 2025-08-18 07:35:27,400] Trial 46 finished with value: 14.660027504047395 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.448230075774238, 'learning_rate': 0.0350417627436791, 'n_estimators': 9000, 'num_leaves': 39, 'min_child_samples': 100, 'subsample': 0.7371906871461208, 'colsample_bytree': 0.6790907633046391, 'reg_alpha': 0.12724870490703605, 'reg_lambda': 3.3148626392416722, 'min_split_gain': 0.053529051122392854}. Best is trial 46 with value: 14.660027504047395.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.011383 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.9302\tvalid_0's tweedie: 8.37005\n",
            "[400]\tvalid_0's rmse: 15.0285\tvalid_0's tweedie: 8.37981\n",
            "Early stopping, best iteration is:\n",
            "[250]\tvalid_0's rmse: 14.8105\tvalid_0's tweedie: 8.3597\n",
            "[I 2025-08-18 07:35:36,280] Trial 47 finished with value: 14.81051699839347 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4579060928018432, 'learning_rate': 0.03684214274466055, 'n_estimators': 10000, 'num_leaves': 47, 'min_child_samples': 100, 'subsample': 0.7417847598239635, 'colsample_bytree': 0.6848724211240825, 'reg_alpha': 0.1303343825490915, 'reg_lambda': 3.9563399255452443, 'min_split_gain': 0.06864646874147247}. Best is trial 46 with value: 14.660027504047395.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005281 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.0891\tvalid_0's tweedie: 6.93706\n",
            "[400]\tvalid_0's rmse: 15.2025\tvalid_0's tweedie: 6.93881\n",
            "Early stopping, best iteration is:\n",
            "[255]\tvalid_0's rmse: 14.9824\tvalid_0's tweedie: 6.92202\n",
            "[I 2025-08-18 07:35:44,269] Trial 48 finished with value: 14.982425916234323 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.547036012561942, 'learning_rate': 0.03560877553856506, 'n_estimators': 10000, 'num_leaves': 47, 'min_child_samples': 100, 'subsample': 0.7330977927070564, 'colsample_bytree': 0.6884725791165843, 'reg_alpha': 0.13382385153112059, 'reg_lambda': 3.8495160958191583, 'min_split_gain': 0.09646893496323072}. Best is trial 46 with value: 14.660027504047395.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.008656 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.0257\tvalid_0's tweedie: 8.24686\n",
            "[400]\tvalid_0's rmse: 15.2678\tvalid_0's tweedie: 8.0442\n",
            "[600]\tvalid_0's rmse: 14.9278\tvalid_0's tweedie: 8.00662\n",
            "[800]\tvalid_0's rmse: 14.9065\tvalid_0's tweedie: 8.0007\n",
            "Early stopping, best iteration is:\n",
            "[723]\tvalid_0's rmse: 14.8828\tvalid_0's tweedie: 7.99833\n",
            "[I 2025-08-18 07:35:59,130] Trial 49 finished with value: 14.882821111058208 and parameters: {'objective': 'tweedie', 'tweedie_variance_power': 1.4748865052224394, 'learning_rate': 0.014963995538610319, 'n_estimators': 10000, 'num_leaves': 39, 'min_child_samples': 120, 'subsample': 0.7523361123851979, 'colsample_bytree': 0.711934723102777, 'reg_alpha': 0.23250823269154888, 'reg_lambda': 3.9996137765940647, 'min_split_gain': 0.063750587163906}. Best is trial 46 with value: 14.660027504047395.\n",
            "Best RMSE : 14.660027504047395\n",
            "Best Params: {'objective': 'tweedie', 'tweedie_variance_power': 1.448230075774238, 'learning_rate': 0.0350417627436791, 'n_estimators': 9000, 'num_leaves': 39, 'min_child_samples': 100, 'subsample': 0.7371906871461208, 'colsample_bytree': 0.6790907633046391, 'reg_alpha': 0.12724870490703605, 'reg_lambda': 3.3148626392416722, 'min_split_gain': 0.053529051122392854}\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2925\n",
            "[LightGBM] [Info] Number of data points in the train set: 99588, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.28 MB) transferred to GPU in 0.005029 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.372599\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.2968\tvalid_0's tweedie: 8.61286\n",
            "[400]\tvalid_0's rmse: 14.8279\tvalid_0's tweedie: 8.52207\n",
            "[600]\tvalid_0's rmse: 14.7946\tvalid_0's tweedie: 8.52254\n",
            "[800]\tvalid_0's rmse: 14.863\tvalid_0's tweedie: 8.55499\n",
            "Early stopping, best iteration is:\n",
            "[618]\tvalid_0's rmse: 14.7468\tvalid_0's tweedie: 8.51893\n",
            "✅ Saved -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_03_menu_demand_optuna_gpu.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lightgbm -q\n",
        "import os, glob, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# ===== 경로 설정 =====\n",
        "MODEL_PATH   = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_03_menu_demand_optuna_gpu.pkl\"\n",
        "TEST_DIR     = \"/content/drive/MyDrive/lg_aimers_2/data/test\"   # TEST_00.csv ~ TEST_09.csv\n",
        "SAMPLE_PATH  = \"/content/drive/MyDrive/lg_aimers_2/data/sample_submission.csv\"  # 필요 시 수정\n",
        "OUT_PATH     = \"/content/drive/MyDrive/lg_aimers_2/submission_lightgbm_03.csv\"\n",
        "\n",
        "# ===== 모델 로드 =====\n",
        "bundle = joblib.load(MODEL_PATH)\n",
        "model: LGBMRegressor = bundle[\"model\"]\n",
        "use_cols = bundle[\"features\"]\n",
        "cat_idx  = bundle[\"cat_idx\"]\n",
        "cat_col  = use_cols[cat_idx[0]]\n",
        "\n",
        "# ===== 공휴일 계산: holidays 라이브러리 사용 =====\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    # Colab 등에서 미설치 시\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"holidays\", \"-q\"], check=True)\n",
        "    import holidays\n",
        "\n",
        "# 연도별 캐시(테스트 파일마다 연도가 다를 수 있으므로 지연 생성)\n",
        "_HOL_CACHE = {}\n",
        "def is_holiday(ts: pd.Timestamp) -> bool:\n",
        "    y = int(ts.year)\n",
        "    if y not in _HOL_CACHE:\n",
        "        try:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y], language=\"ko\")\n",
        "        except Exception:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y])\n",
        "    return ts.date() in _HOL_CACHE[y]\n",
        "\n",
        "# ===== 출시 정보 (전처리와 동일 규칙 사용: dict 기반) =====\n",
        "launch_dates = {\n",
        "    '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "    '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "    '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "    '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "    '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "    '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "    '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "    '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "    '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "    '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "    '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "    '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "    '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "    '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "    '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "    '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "    '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "    '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "    '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "    '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "    '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "    '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "    '라그로타_해산물 토마토 스튜 파스타': '2023-12-08',\n",
        "    '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "    '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "    '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "    '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "    '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "    '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "    '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "    '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "    '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "    '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "    '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "    '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "    '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "    '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "    '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "    '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "    '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "    '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "    '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "    '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "    '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "    '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "    '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "    '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "    '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "    '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "    '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "    '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "    '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "}\n",
        "launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "def add_future_meta_row(date, key):\n",
        "    \"\"\"모델 입력 피처(학습 시 사용한 use_cols 기준)와 일치하도록 미래 1행 메타 피처 생성\"\"\"\n",
        "    row = pd.DataFrame({\"영업일자\":[pd.Timestamp(date)], \"영업장명_메뉴명\":[key]})\n",
        "    d = row.loc[0, \"영업일자\"]\n",
        "\n",
        "    # 기본 달력\n",
        "    row[\"년\"] = d.year\n",
        "    row[\"월\"] = d.month\n",
        "    row[\"일\"] = d.day\n",
        "    row[\"요일\"] = d.dayofweek\n",
        "    row[\"주말여부\"] = int(row.loc[0,\"요일\"] in [5,6])\n",
        "\n",
        "    # 공휴일 & 휴무일\n",
        "    hol = is_holiday(d)\n",
        "    row[\"공휴일여부\"] = int(hol)\n",
        "    row[\"휴무일여부\"] = int(hol or bool(row.loc[0,\"주말여부\"]))\n",
        "\n",
        "    # 사이클릭\n",
        "    row[\"요일_sin\"] = np.sin(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"요일_cos\"] = np.cos(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"월_sin\"]   = np.sin(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "    row[\"월_cos\"]   = np.cos(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "\n",
        "    # 계절\n",
        "    m = row.loc[0,\"월\"]\n",
        "    if m in [12,1,2]:\n",
        "        season = 0\n",
        "    elif m in [3,4,5]:\n",
        "        season = 1\n",
        "    elif m in [6,7,8]:\n",
        "        season = 2\n",
        "    else:\n",
        "        season = 3\n",
        "    row[\"계절(겨울0봄1여름2가을3)\"] = np.int8(season)\n",
        "\n",
        "    # 출시 파생\n",
        "    row[\"신규메뉴여부\"] = int(key in launch_dates)\n",
        "    if key in launch_dates and d >= launch_dates[key]:\n",
        "        row[\"출시일로부터경과일\"] = int((d - launch_dates[key]).days)\n",
        "    else:\n",
        "        row[\"출시일로부터경과일\"] = 0\n",
        "\n",
        "    # 출시 후 주차 & 더미 (학습 시 사용했다면 동일하게)\n",
        "    row[\"출시후_주차\"] = (row[\"출시일로부터경과일\"] // 7)\n",
        "    row[\"출시_0주\"]   = (row[\"출시후_주차\"] == 0).astype(int)\n",
        "    row[\"출시_1_2주\"] = row[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    row[\"출시_3_4주\"] = row[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    row[\"출시_5주이상\"] = (row[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "    return row\n",
        "\n",
        "def predict_group_autoreg(g: pd.DataFrame):\n",
        "    key = g[cat_col].iloc[0]\n",
        "    g = g.sort_values(\"영업일자\").copy()\n",
        "    g = g.dropna(subset=[\"매출수량\"])\n",
        "    assert len(g) >= 28, f\"{key}: 28일 히스토리 부족\"\n",
        "\n",
        "    hist_vals = g[\"매출수량\"].values.tolist()[-28:]\n",
        "    hist_dates = g[\"영업일자\"].tolist()[-28:]\n",
        "    last_date = g[\"영업일자\"].max()\n",
        "    preds = []\n",
        "\n",
        "    for h in range(1, 8):\n",
        "        cur_date = last_date + pd.Timedelta(days=h)\n",
        "        row = add_future_meta_row(cur_date, key)\n",
        "\n",
        "        # lags\n",
        "        def lag(n): return hist_vals[-n] if len(hist_vals) >= n else np.nan\n",
        "        row[\"lag_1\"], row[\"lag_7\"], row[\"lag_14\"], row[\"lag_28\"] = lag(1), lag(7), lag(14), lag(28)\n",
        "\n",
        "        # rolling (과거값만)\n",
        "        def rmean(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.mean(arr)) if arr else 0.0\n",
        "        def rsum(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.sum(arr)) if arr else 0.0\n",
        "        row[\"roll7_mean\"], row[\"roll7_sum\"], row[\"roll14_mean\"], row[\"roll28_mean\"] = \\\n",
        "            rmean(7), rsum(7), rmean(14), rmean(28)\n",
        "\n",
        "        # 같은 요일 평균(최근 28일 범위)\n",
        "        cur_dow = cur_date.dayofweek\n",
        "        hist_dows = [pd.Timestamp(d).dayofweek for d in hist_dates]\n",
        "        same_idx = [i for i in range(len(hist_vals)) if hist_dows[i] == cur_dow]\n",
        "        row[\"same_dow_mean_28\"] = float(np.mean([hist_vals[i] for i in same_idx])) if same_idx else 0.0\n",
        "\n",
        "        # 모델 입력 정렬\n",
        "        X = row.reindex(columns=use_cols).copy()\n",
        "        X[cat_col] = X[cat_col].astype(\"category\")\n",
        "        for c in X.columns:\n",
        "            if c != cat_col:\n",
        "                X[c] = X[c].fillna(0)\n",
        "\n",
        "        yhat = float(model.predict(X, num_iteration=getattr(model, \"best_iteration_\", None))[0])\n",
        "        yhat = max(0.0, yhat)  # 음수 방지\n",
        "        preds.append(yhat)\n",
        "\n",
        "        # autoreg 업데이트\n",
        "        hist_vals.append(yhat); hist_dates.append(cur_date)\n",
        "        if len(hist_vals) > 28:\n",
        "            hist_vals, hist_dates = hist_vals[-28:], hist_dates[-28:]\n",
        "\n",
        "    return preds  # 길이 7\n",
        "\n",
        "# ===== 샘플 제출 파일 불러오기 =====\n",
        "sub = pd.read_csv(SAMPLE_PATH)\n",
        "menu_cols = [c for c in sub.columns if c != \"영업일자\"]\n",
        "sub[menu_cols] = sub[menu_cols].astype(float)\n",
        "\n",
        "# ===== TEST 파일 순회 =====\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "print(\"Found:\", test_files)\n",
        "\n",
        "for f in test_files:\n",
        "    test_name = os.path.splitext(os.path.basename(f))[0]  # TEST_00\n",
        "    test_id = test_name.split(\"_\")[1]                     # 00\n",
        "    df = pd.read_csv(f, parse_dates=[\"영업일자\"])\n",
        "\n",
        "    # 타입/정렬\n",
        "    if cat_col in df.columns:\n",
        "        df[cat_col] = df[cat_col].astype(\"category\")\n",
        "    df = df.sort_values([cat_col, \"영업일자\"]).copy()\n",
        "\n",
        "    # 메뉴 단위 예측\n",
        "    for key, g in df.groupby(cat_col, observed=True):\n",
        "        preds7 = predict_group_autoreg(g)\n",
        "\n",
        "        # 제출 파일에 해당 메뉴 열이 없으면 skip\n",
        "        if key not in menu_cols:\n",
        "            continue\n",
        "\n",
        "        # +1~+7일 채우기\n",
        "        for k in range(1, 8):\n",
        "            ridx = sub.index[sub[\"영업일자\"] == f\"{test_name}+{k}일\"]\n",
        "            if len(ridx) == 1:\n",
        "                sub.loc[ridx[0], key] = preds7[k-1]\n",
        "\n",
        "# (선택) 후처리\n",
        "# sub[menu_cols] = sub[menu_cols].clip(lower=0).round(4)\n",
        "\n",
        "# ===== 저장 =====\n",
        "sub.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved submission ->\", OUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21w6WXEF3i86",
        "outputId": "2e08c122-45c2-4fca-f2bf-1107ba3eeba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: ['/content/drive/MyDrive/lg_aimers_2/data/test/TEST_00.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_01.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_02.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_03.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_04.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_05.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_06.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_07.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_08.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_09.csv']\n",
            "Saved submission -> /content/drive/MyDrive/lg_aimers_2/submission_lightgbm_03.csv\n"
          ]
        }
      ]
    }
  ]
}