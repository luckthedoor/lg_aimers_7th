{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c39e25dacd7a48c7a9406d5bf89d941c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a0e5e9c3a3c498380e08c802bd6a68e",
              "IPY_MODEL_86417df3ca294cf6ae60c98d2a56f985",
              "IPY_MODEL_dc8eee4c983f40afac2dade9ac3d0005"
            ],
            "layout": "IPY_MODEL_4d30487027b844e8a2568f63fd150071"
          }
        },
        "4a0e5e9c3a3c498380e08c802bd6a68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d5f732dbb554fe3af3c76a8d9a5cf43",
            "placeholder": "​",
            "style": "IPY_MODEL_70aa88be8c774761a34fd5b275b905cd",
            "value": "Best trial: 46. Best value: 15.5414: 100%"
          }
        },
        "86417df3ca294cf6ae60c98d2a56f985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba5dfb94456457590f067c53a5340e5",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23a6e1ff88614f55a1c8c702c383303a",
            "value": 50
          }
        },
        "dc8eee4c983f40afac2dade9ac3d0005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd90989af1e840da847ddc589f1cecb5",
            "placeholder": "​",
            "style": "IPY_MODEL_514853e041a04b0684807be4daa5d86a",
            "value": " 50/50 [46:57&lt;00:00, 56.76s/it]"
          }
        },
        "4d30487027b844e8a2568f63fd150071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5f732dbb554fe3af3c76a8d9a5cf43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70aa88be8c774761a34fd5b275b905cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fba5dfb94456457590f067c53a5340e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a6e1ff88614f55a1c8c702c383303a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd90989af1e840da847ddc589f1cecb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514853e041a04b0684807be4daa5d86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 가능 여부\n",
        "print(torch.cuda.is_available())  # True면 GPU 사용 가능\n",
        "print(torch.cuda.get_device_name(0))  # GPU 이름 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVriMxe9_Mvc",
        "outputId": "dcb17ef9-3b4b-46b8-f486-51e8f2d9151d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab 환경을 위한 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltSlwie6Fvg",
        "outputId": "057d9625-22e4-405a-a5ef-04961bfdf2f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-Cell Clean Preprocessing (공휴일여부 + 휴무일여부) ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/lg_aimers_2/data/train/train.csv'\n",
        "OUT_PATH   = '/content/drive/MyDrive/lg_aimers_2/train_preprocessed_05.csv'\n",
        "\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    !pip install holidays -q\n",
        "    import holidays\n",
        "\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"train.csv 전처리 + (공휴일/휴무일) + 계절/사이클릭 + 출시일 파생\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "     # === 음수 매출수량 보정 ===\n",
        "    if \"매출수량\" in df.columns:\n",
        "        df[\"매출수량\"] = df[\"매출수량\"].clip(lower=0)\n",
        "\n",
        "    df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    df['년'] = df['영업일자'].dt.year\n",
        "    df['월'] = df['영업일자'].dt.month\n",
        "    df['일'] = df['영업일자'].dt.day\n",
        "    df['요일'] = df['영업일자'].dt.dayofweek\n",
        "    df['주말여부'] = df['요일'].isin([5, 6])\n",
        "\n",
        "    # 메뉴 분리\n",
        "    df[['영업장명', '메뉴명']] = df['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "\n",
        "    # === 캘린더 피처 ===\n",
        "    def add_calendar_features(frame: pd.DataFrame, date_col=\"영업일자\") -> pd.DataFrame:\n",
        "        out = frame.copy()\n",
        "        d = pd.to_datetime(out[date_col])\n",
        "        years = sorted(d.dt.year.unique())\n",
        "        try:\n",
        "            KR_HOL = holidays.KR(years=years, language=\"ko\")\n",
        "        except Exception:\n",
        "            KR_HOL = holidays.KR(years=years)\n",
        "\n",
        "        # 공휴일 여부만\n",
        "        out[\"공휴일여부\"] = d.dt.date.map(lambda x: x in KR_HOL)\n",
        "\n",
        "        # 계절\n",
        "        m = d.dt.month\n",
        "        out[\"계절(겨울0봄1여름2가을3)\"] = (\n",
        "            (m.isin([12,1,2]))*0 +\n",
        "            (m.isin([3,4,5]))*1 +\n",
        "            (m.isin([6,7,8]))*2 +\n",
        "            (m.isin([9,10,11]))*3\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "        # 사이클릭\n",
        "        out[\"요일_sin\"] = np.sin(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"요일_cos\"] = np.cos(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"월_sin\"]   = np.sin(2*np.pi*(out[\"월\"]-1)/12)\n",
        "        out[\"월_cos\"]   = np.cos(2*np.pi*(out[\"월\"]-1)/12)\n",
        "\n",
        "        return out\n",
        "\n",
        "    df = add_calendar_features(df, date_col='영업일자')\n",
        "\n",
        "    # === 추가: 휴무일여부 (주말 OR 공휴일) ===\n",
        "    df[\"휴무일여부\"] = df[\"주말여부\"] | df[\"공휴일여부\"]\n",
        "\n",
        "    # === 출시일 파생 ===\n",
        "    launch_dates = {\n",
        "        '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "        '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "        '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "        '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "        '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "        '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "        '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "        '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "        '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "        '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "        '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "        '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "        '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "        '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "        '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "        '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "        '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "        '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "        '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "        '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "        '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "        '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "        '라그로타_해산물 토마토 스튜 파스타': '2023-12-08', '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "        '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "        '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "        '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "        '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "        '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "        '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "        '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "        '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "        '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "        '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "        '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "        '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "        '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "        '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "        '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "        '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "        '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "        '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "        '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "        '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "        '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "        '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "        '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "        '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "        '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "        '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "        '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "        '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "    }\n",
        "    launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "    def calculate_days_since_launch(row):\n",
        "        menu = row['영업장명_메뉴명']\n",
        "        if menu in launch_dates:\n",
        "            launch_date = launch_dates[menu]\n",
        "            if row['영업일자'] >= launch_date:\n",
        "                return (row['영업일자'] - launch_date).days\n",
        "        return 0\n",
        "\n",
        "    df['출시일로부터경과일'] = df.apply(calculate_days_since_launch, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 실행\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "train_preprocessed = preprocess_data(train_df)\n",
        "train_preprocessed.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(\"전처리 완료 →\", OUT_PATH)\n",
        "\n",
        "# 점검\n",
        "print(\"공휴일여부 분포:\", train_preprocessed[\"공휴일여부\"].value_counts(dropna=False).to_dict())\n",
        "print(\"휴무일여부 분포:\", train_preprocessed[\"휴무일여부\"].value_counts(dropna=False).to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5qtDmEV0vXk",
        "outputId": "7fb0ff0c-ae9b-43cf-f7a5-420dc907d4d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 완료 → /content/drive/MyDrive/lg_aimers_2/train_preprocessed_05.csv\n",
            "공휴일여부 분포: {False: 97079, True: 5597}\n",
            "휴무일여부 분포: {False: 69287, True: 33389}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Colab 셋업 =====\n",
        "!pip -q install optuna lightgbm\n",
        "\n",
        "import os, joblib, optuna, numpy as np, pandas as pd\n",
        "from lightgbm import LGBMRegressor, log_evaluation, early_stopping\n",
        "\n",
        "# -----------------------------\n",
        "# 0) 경로/상수\n",
        "# -----------------------------\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/lg_aimers_2/train_preprocessed_05.csv\"\n",
        "MODEL_OUT  = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05_optuna.pkl\"\n",
        "USE_GPU    = True\n",
        "DEVICE     = {\"device\": \"gpu\"} if USE_GPU else {\"device\": \"cpu\"}\n",
        "MAX_LAG    = 28  # 너의 lag 최댓값과 동일해야 함\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 데이터 로드 & 기본 세팅\n",
        "# -----------------------------\n",
        "df = pd.read_csv(TRAIN_PATH, parse_dates=[\"영업일자\"])\n",
        "\n",
        "# 음수 매출 안전 처리(대회 데이터 이슈 대응)\n",
        "df[\"매출수량\"] = pd.to_numeric(df[\"매출수량\"], errors=\"coerce\").fillna(0)\n",
        "df[\"매출수량\"] = df[\"매출수량\"].clip(lower=0)\n",
        "\n",
        "# bool -> int\n",
        "for c in [c for c in [\"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\"] if c in df.columns]:\n",
        "    df[c] = df[c].astype(int)\n",
        "\n",
        "# key 컬럼\n",
        "key_col = \"영업장명_메뉴명\"\n",
        "df[key_col] = df[key_col].astype(\"category\")\n",
        "\n",
        "# 시간 정렬\n",
        "df = df.sort_values([key_col, \"영업일자\"]).copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) lag/rolling/same_dow 생성 (네 기존 규칙 유지)\n",
        "# -----------------------------\n",
        "lag_list = [1, 7, 14, 28]\n",
        "for lag in lag_list:\n",
        "    df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
        "\n",
        "def add_rolling(g, window, how=\"mean\"):\n",
        "    s = g[\"매출수량\"].shift(1)  # 과거만\n",
        "    if how == \"mean\":\n",
        "        return s.rolling(window, min_periods=1).mean()\n",
        "    elif how == \"sum\":\n",
        "        return s.rolling(window, min_periods=1).sum()\n",
        "\n",
        "df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
        "df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
        "df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
        "df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
        "\n",
        "def same_dow_mean_28(g):\n",
        "    out = np.full(len(g), np.nan, dtype=float)\n",
        "    vals = g[\"매출수량\"].shift(1)\n",
        "    dows = g[\"요일\"]\n",
        "    for i in range(len(g)):\n",
        "        lo = max(0, i-28)\n",
        "        same_idx = [j for j in range(lo, i) if dows.iloc[j] == dows.iloc[i]]\n",
        "        if same_idx:\n",
        "            out[i] = vals.iloc[same_idx].mean()\n",
        "    return pd.Series(out, index=g.index)\n",
        "\n",
        "if \"요일\" in df.columns:\n",
        "    df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
        "\n",
        "# 신규메뉴 파생(있을 때만)\n",
        "if \"출시일로부터경과일\" in df.columns:\n",
        "    df[\"출시후_주차\"]   = (df[\"출시일로부터경과일\"] // 7).clip(lower=0).astype(int)\n",
        "    df[\"출시_0주\"]     = (df[\"출시후_주차\"] == 0).astype(int)\n",
        "    df[\"출시_1_2주\"]   = df[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    df[\"출시_3_4주\"]   = df[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    df[\"출시_5주이상\"] = (df[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 학습에 쓸 피처 목록 (네 기준 유지 + 휴무일여부 포함)\n",
        "# -----------------------------\n",
        "base_feats = [c for c in [\n",
        "    \"년\",\"월\",\"일\",\"요일\",\"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\",\n",
        "    \"출시일로부터경과일\",\"출시후_주차\",\"출시_0주\",\"출시_1_2주\",\"출시_3_4주\",\"출시_5주이상\",\n",
        "    \"요일_sin\",\"요일_cos\",\"월_sin\",\"월_cos\",\"계절(겨울0봄1여름2가을3)\"\n",
        "] if c in df.columns]\n",
        "\n",
        "lag_feats  = [f\"lag_{l}\" for l in lag_list]\n",
        "roll_feats = [c for c in [\"roll7_mean\",\"roll7_sum\",\"roll14_mean\",\"roll28_mean\",\"same_dow_mean_28\"] if c in df.columns]\n",
        "\n",
        "use_cols = base_feats + lag_feats + roll_feats + [key_col]\n",
        "cat_features = [use_cols.index(key_col)]  # LightGBM용 범주형 인덱스\n",
        "\n",
        "# -----------------------------\n",
        "# 4) 폴드 정의(롤링) + gap purge\n",
        "#    - 아래는 예시: 14일 검증창을 여러 번 슬라이딩\n",
        "#    - 네 데이터의 마지막: 2024-06-15 기준으로 몇 개 컷 생성\n",
        "# -----------------------------\n",
        "def build_folds(df, val_window_days=14, n_folds=4, last_val_end=\"2024-06-15\"):\n",
        "    last_val_end = pd.Timestamp(last_val_end)\n",
        "    folds = []\n",
        "    for k in range(n_folds):\n",
        "        val_end   = last_val_end - pd.Timedelta(days=(n_folds-1-k)*val_window_days)\n",
        "        val_start = val_end - pd.Timedelta(days=val_window_days-1)\n",
        "        # train은 val_start - MAX_LAG 까지만 사용 (누수 방지)\n",
        "        train_end = val_start - pd.Timedelta(days=MAX_LAG)\n",
        "        folds.append((train_end, val_start, val_end))\n",
        "    return folds\n",
        "\n",
        "folds = build_folds(df, val_window_days=14, n_folds=4, last_val_end=\"2024-06-15\")\n",
        "print(\"Folds:\")\n",
        "for i,(te,vs,ve) in enumerate(folds,1):\n",
        "    print(f\"Fold{i}: train_end={te.date()} | val={vs.date()}~{ve.date()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) 폴드 데이터 구성 함수\n",
        "# -----------------------------\n",
        "def make_fold_data(df, use_cols, key_col, tr_end, va_st, va_en):\n",
        "    tr = df[df[\"영업일자\"] <= tr_end].copy()\n",
        "    va = df[(df[\"영업일자\"] >= va_st) & (df[\"영업일자\"] <= va_en)].copy()\n",
        "\n",
        "    # 최소 lag 확보(가장 작은 lag만 체크)\n",
        "    tr = tr.dropna(subset=[\"lag_1\"])\n",
        "    va = va.dropna(subset=[\"lag_1\"])\n",
        "\n",
        "    # NA 대체\n",
        "    fill_cols = list(set(use_cols) - {key_col})\n",
        "    tr[fill_cols] = tr[fill_cols].fillna(0)\n",
        "    va[fill_cols] = va[fill_cols].fillna(0)\n",
        "\n",
        "    X_tr, y_tr = tr[use_cols], tr[\"매출수량\"].astype(float).values\n",
        "    X_va, y_va = va[use_cols], va[\"매출수량\"].astype(float).values\n",
        "    return X_tr, y_tr, X_va, y_va\n",
        "\n",
        "# -----------------------------\n",
        "# 6) 폴드 평균 RMSE 계산\n",
        "# -----------------------------\n",
        "def cv_rmse(params, df, use_cols, key_col, cat_features, nonneg_target=False):\n",
        "    rmses = []\n",
        "    for (tr_end, va_st, va_en) in folds:\n",
        "        # gap purge: train_end를 val_start - MAX_LAG 로 강제\n",
        "        te = va_st - pd.Timedelta(days=MAX_LAG)\n",
        "        X_tr, y_tr, X_va, y_va = make_fold_data(df, use_cols, key_col, te, va_st, va_en)\n",
        "\n",
        "        # poisson/tweedie 평가를 원하면 비음수 타깃 사용\n",
        "        if nonneg_target:\n",
        "            y_tr = np.clip(y_tr, 0, None)\n",
        "            y_va = np.clip(y_va, 0, None)\n",
        "\n",
        "        model = LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"rmse\",\n",
        "            categorical_feature=cat_features,\n",
        "            callbacks=[log_evaluation(200), early_stopping(200)],\n",
        "        )\n",
        "        pred = model.predict(X_va, num_iteration=getattr(model, \"best_iteration_\", None))\n",
        "        rmse = float(np.sqrt(((y_va - pred)**2).mean()))\n",
        "        rmses.append(rmse)\n",
        "    return float(np.mean(rmses))\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Optuna 목적함수 (시계열 CV + gap)\n",
        "# -----------------------------\n",
        "def objective(trial):\n",
        "    obj = trial.suggest_categorical(\"objective\", [\"regression\",\"poisson\",\"tweedie\"])\n",
        "    params = dict(\n",
        "        objective=obj,\n",
        "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True),\n",
        "        n_estimators=trial.suggest_int(\"n_estimators\", 3000, 9000, step=1000),\n",
        "        num_leaves=trial.suggest_int(\"num_leaves\", 63, 191, step=16),\n",
        "        min_child_samples=trial.suggest_int(\"min_child_samples\", 80, 220, step=20),\n",
        "        subsample=trial.suggest_float(\"subsample\", 0.75, 0.95),\n",
        "        subsample_freq=1,\n",
        "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
        "        reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 0.6),\n",
        "        reg_lambda=trial.suggest_float(\"reg_lambda\", 0.5, 4.0),\n",
        "        min_split_gain=trial.suggest_float(\"min_split_gain\", 0.0, 0.2),\n",
        "        random_state=42, n_jobs=-1, **DEVICE\n",
        "    )\n",
        "    if obj == \"tweedie\":\n",
        "        params[\"tweedie_variance_power\"] = trial.suggest_float(\"tweedie_variance_power\", 1.1, 1.6)\n",
        "\n",
        "    # poisson/tweedie는 비음수 타깃으로 CV\n",
        "    nonneg = (obj in {\"poisson\",\"tweedie\"})\n",
        "    score = cv_rmse(params, df, use_cols, key_col, cat_features, nonneg_target=nonneg)\n",
        "    return score\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best RMSE :\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c39e25dacd7a48c7a9406d5bf89d941c",
            "4a0e5e9c3a3c498380e08c802bd6a68e",
            "86417df3ca294cf6ae60c98d2a56f985",
            "dc8eee4c983f40afac2dade9ac3d0005",
            "4d30487027b844e8a2568f63fd150071",
            "8d5f732dbb554fe3af3c76a8d9a5cf43",
            "70aa88be8c774761a34fd5b275b905cd",
            "fba5dfb94456457590f067c53a5340e5",
            "23a6e1ff88614f55a1c8c702c383303a",
            "bd90989af1e840da847ddc589f1cecb5",
            "514853e041a04b0684807be4daa5d86a"
          ]
        },
        "id": "cerX1PYIjcGQ",
        "outputId": "6fab67cb-8a2e-468f-dff5-66ae8cc347b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/247.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2228773888.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-2228773888.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-2228773888.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-2228773888.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-2228773888.py:50: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:51: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
            "/tmp/ipython-input-2228773888.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
            "/tmp/ipython-input-2228773888.py:52: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:53: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-2228773888.py:67: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
            "/tmp/ipython-input-2228773888.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
            "[I 2025-08-18 15:45:12,945] A new study created in memory with name: no-name-c885c128-1c5b-41d5-a18a-c5e90f74a416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds:\n",
            "Fold1: train_end=2024-03-24 | val=2024-04-21~2024-05-04\n",
            "Fold2: train_end=2024-04-07 | val=2024-05-05~2024-05-18\n",
            "Fold3: train_end=2024-04-21 | val=2024-05-19~2024-06-01\n",
            "Fold4: train_end=2024-05-05 | val=2024-06-02~2024-06-15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c39e25dacd7a48c7a9406d5bf89d941c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005085 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5561\tvalid_0's poisson: -27.5713\n",
            "[400]\tvalid_0's rmse: 23.5216\tvalid_0's poisson: -27.5219\n",
            "Early stopping, best iteration is:\n",
            "[242]\tvalid_0's rmse: 22.415\tvalid_0's poisson: -27.748\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004869 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6844\tvalid_0's poisson: -13.4485\n",
            "[400]\tvalid_0's rmse: 12.4252\tvalid_0's poisson: -13.8335\n",
            "[600]\tvalid_0's rmse: 12.1155\tvalid_0's poisson: -13.9806\n",
            "[800]\tvalid_0's rmse: 12.0762\tvalid_0's poisson: -13.9934\n",
            "[1000]\tvalid_0's rmse: 12.067\tvalid_0's poisson: -14.0027\n",
            "[1200]\tvalid_0's rmse: 11.9938\tvalid_0's poisson: -14.0259\n",
            "Early stopping, best iteration is:\n",
            "[1197]\tvalid_0's rmse: 11.9912\tvalid_0's poisson: -14.0262\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004997 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5936\tvalid_0's poisson: -17.0471\n",
            "[400]\tvalid_0's rmse: 13.2285\tvalid_0's poisson: -17.5007\n",
            "[600]\tvalid_0's rmse: 13.4615\tvalid_0's poisson: -17.5589\n",
            "Early stopping, best iteration is:\n",
            "[471]\tvalid_0's rmse: 13.1848\tvalid_0's poisson: -17.5582\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.013160 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8782\tvalid_0's poisson: -15.6417\n",
            "[400]\tvalid_0's rmse: 15.6037\tvalid_0's poisson: -16.062\n",
            "[600]\tvalid_0's rmse: 15.5302\tvalid_0's poisson: -16.0755\n",
            "Early stopping, best iteration is:\n",
            "[456]\tvalid_0's rmse: 15.5413\tvalid_0's poisson: -16.0901\n",
            "[I 2025-08-18 15:46:28,842] Trial 0 finished with value: 15.783072128025898 and parameters: {'objective': 'poisson', 'learning_rate': 0.026208630215377525, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 80, 'subsample': 0.923235229154987, 'colsample_bytree': 0.7803345035229626, 'reg_alpha': 0.4248435466776273, 'reg_lambda': 0.5720457300353086, 'min_split_gain': 0.19398197043239887}. Best is trial 0 with value: 15.783072128025898.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005031 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.2863\tvalid_0's l2: 496.681\n",
            "[400]\tvalid_0's rmse: 21.9818\tvalid_0's l2: 483.198\n",
            "[600]\tvalid_0's rmse: 21.9946\tvalid_0's l2: 483.764\n",
            "Early stopping, best iteration is:\n",
            "[464]\tvalid_0's rmse: 21.963\tvalid_0's l2: 482.371\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.027328 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0461\tvalid_0's l2: 170.201\n",
            "[400]\tvalid_0's rmse: 12.8637\tvalid_0's l2: 165.476\n",
            "[600]\tvalid_0's rmse: 12.9357\tvalid_0's l2: 167.332\n",
            "Early stopping, best iteration is:\n",
            "[430]\tvalid_0's rmse: 12.8579\tvalid_0's l2: 165.325\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005055 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.9077\tvalid_0's l2: 193.425\n",
            "[400]\tvalid_0's rmse: 13.3451\tvalid_0's l2: 178.093\n",
            "[600]\tvalid_0's rmse: 13.2247\tvalid_0's l2: 174.894\n",
            "[800]\tvalid_0's rmse: 13.2064\tvalid_0's l2: 174.409\n",
            "Early stopping, best iteration is:\n",
            "[660]\tvalid_0's rmse: 13.1856\tvalid_0's l2: 173.859\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.019742 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.0408\tvalid_0's l2: 257.307\n",
            "[400]\tvalid_0's rmse: 15.9459\tvalid_0's l2: 254.272\n",
            "[600]\tvalid_0's rmse: 15.9385\tvalid_0's l2: 254.035\n",
            "Early stopping, best iteration is:\n",
            "[581]\tvalid_0's rmse: 15.927\tvalid_0's l2: 253.671\n",
            "[I 2025-08-18 15:48:06,861] Trial 1 finished with value: 15.983354918428141 and parameters: {'objective': 'regression', 'learning_rate': 0.013433656868034296, 'n_estimators': 5000, 'num_leaves': 127, 'min_child_samples': 140, 'subsample': 0.8082458280396083, 'colsample_bytree': 0.7835558684167139, 'reg_alpha': 0.0836963163912251, 'reg_lambda': 1.5225062698732637, 'min_split_gain': 0.07327236865873835}. Best is trial 0 with value: 15.783072128025898.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.029956 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3431\tvalid_0's poisson: -27.3531\n",
            "[400]\tvalid_0's rmse: 22.3143\tvalid_0's poisson: -27.8476\n",
            "Early stopping, best iteration is:\n",
            "[317]\tvalid_0's rmse: 21.8362\tvalid_0's poisson: -27.8758\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005187 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9212\tvalid_0's poisson: -13.3145\n",
            "[400]\tvalid_0's rmse: 12.6235\tvalid_0's poisson: -13.8151\n",
            "[600]\tvalid_0's rmse: 12.4277\tvalid_0's poisson: -13.9396\n",
            "[800]\tvalid_0's rmse: 12.3029\tvalid_0's poisson: -13.9883\n",
            "[1000]\tvalid_0's rmse: 12.265\tvalid_0's poisson: -14.0036\n",
            "[1200]\tvalid_0's rmse: 12.2345\tvalid_0's poisson: -14.0317\n",
            "[1400]\tvalid_0's rmse: 12.2177\tvalid_0's poisson: -14.0573\n",
            "Early stopping, best iteration is:\n",
            "[1360]\tvalid_0's rmse: 12.1966\tvalid_0's poisson: -14.0585\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004864 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.9679\tvalid_0's poisson: -16.8071\n",
            "[400]\tvalid_0's rmse: 13.1586\tvalid_0's poisson: -17.4158\n",
            "[600]\tvalid_0's rmse: 13.0177\tvalid_0's poisson: -17.5677\n",
            "[800]\tvalid_0's rmse: 13.2814\tvalid_0's poisson: -17.5794\n",
            "Early stopping, best iteration is:\n",
            "[609]\tvalid_0's rmse: 13.0015\tvalid_0's poisson: -17.576\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004916 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9785\tvalid_0's poisson: -15.4541\n",
            "[400]\tvalid_0's rmse: 15.6153\tvalid_0's poisson: -16.0724\n",
            "[600]\tvalid_0's rmse: 15.4797\tvalid_0's poisson: -16.1704\n",
            "Early stopping, best iteration is:\n",
            "[570]\tvalid_0's rmse: 15.4484\tvalid_0's poisson: -16.1798\n",
            "[I 2025-08-18 15:49:13,507] Trial 2 finished with value: 15.62066943284027 and parameters: {'objective': 'poisson', 'learning_rate': 0.022878863522445895, 'n_estimators': 7000, 'num_leaves': 63, 'min_child_samples': 160, 'subsample': 0.7841048247374584, 'colsample_bytree': 0.6195154778955838, 'reg_alpha': 0.5693313223519999, 'reg_lambda': 3.8797121157609578, 'min_split_gain': 0.16167946962329224}. Best is trial 2 with value: 15.62066943284027.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.008184 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.8489\tvalid_0's tweedie: 12.7744\n",
            "[400]\tvalid_0's rmse: 24.9942\tvalid_0's tweedie: 13.1864\n",
            "Early stopping, best iteration is:\n",
            "[205]\tvalid_0's rmse: 21.8469\tvalid_0's tweedie: 12.7735\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004658 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2666\tvalid_0's tweedie: 10.3402\n",
            "[400]\tvalid_0's rmse: 11.961\tvalid_0's tweedie: 10.3335\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's rmse: 12.0496\tvalid_0's tweedie: 10.3098\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.010030 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0817\tvalid_0's tweedie: 10.8443\n",
            "[400]\tvalid_0's rmse: 13.1278\tvalid_0's tweedie: 10.7734\n",
            "Early stopping, best iteration is:\n",
            "[290]\tvalid_0's rmse: 12.8718\tvalid_0's tweedie: 10.7721\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004735 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4821\tvalid_0's tweedie: 10.6796\n",
            "[400]\tvalid_0's rmse: 15.8332\tvalid_0's tweedie: 10.7846\n",
            "Early stopping, best iteration is:\n",
            "[228]\tvalid_0's rmse: 15.4379\tvalid_0's tweedie: 10.6648\n",
            "[I 2025-08-18 15:50:05,991] Trial 3 finished with value: 15.55155755194344 and parameters: {'objective': 'tweedie', 'learning_rate': 0.020307356380344244, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 80, 'subsample': 0.9318640804157564, 'colsample_bytree': 0.677633994480005, 'reg_alpha': 0.39751337061238917, 'reg_lambda': 1.5909887663129383, 'min_split_gain': 0.10401360423556216, 'tweedie_variance_power': 1.37335513967164}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005241 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5358\tvalid_0's poisson: -27.8168\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's rmse: 21.9094\tvalid_0's poisson: -27.7871\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005357 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.247\tvalid_0's poisson: -13.8682\n",
            "[400]\tvalid_0's rmse: 12.0772\tvalid_0's poisson: -14.0463\n",
            "Early stopping, best iteration is:\n",
            "[389]\tvalid_0's rmse: 12.0687\tvalid_0's poisson: -14.0396\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.009407 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.323\tvalid_0's poisson: -17.4349\n",
            "[400]\tvalid_0's rmse: 13.3429\tvalid_0's poisson: -17.5842\n",
            "Early stopping, best iteration is:\n",
            "[274]\tvalid_0's rmse: 13.1245\tvalid_0's poisson: -17.5674\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005204 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8227\tvalid_0's poisson: -15.9968\n",
            "[400]\tvalid_0's rmse: 15.6657\tvalid_0's poisson: -16.0863\n",
            "Early stopping, best iteration is:\n",
            "[312]\tvalid_0's rmse: 15.5651\tvalid_0's poisson: -16.1321\n",
            "[I 2025-08-18 15:51:07,784] Trial 4 finished with value: 15.666930634050392 and parameters: {'objective': 'poisson', 'learning_rate': 0.04536089127921624, 'n_estimators': 9000, 'num_leaves': 143, 'min_child_samples': 220, 'subsample': 0.7676985004103839, 'colsample_bytree': 0.6587948587257435, 'reg_alpha': 0.02713637334632284, 'reg_lambda': 1.6386561576714251, 'min_split_gain': 0.07773545793789641}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.011435 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.3583\tvalid_0's poisson: -26.5183\n",
            "[400]\tvalid_0's rmse: 22.5096\tvalid_0's poisson: -27.5783\n",
            "Early stopping, best iteration is:\n",
            "[370]\tvalid_0's rmse: 22.4698\tvalid_0's poisson: -27.5332\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005032 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4672\tvalid_0's poisson: -12.6716\n",
            "[400]\tvalid_0's rmse: 13.0298\tvalid_0's poisson: -13.5331\n",
            "[600]\tvalid_0's rmse: 12.7579\tvalid_0's poisson: -13.7635\n",
            "[800]\tvalid_0's rmse: 12.5658\tvalid_0's poisson: -13.8725\n",
            "[1000]\tvalid_0's rmse: 12.4167\tvalid_0's poisson: -13.9446\n",
            "[1200]\tvalid_0's rmse: 12.334\tvalid_0's poisson: -13.9881\n",
            "[1400]\tvalid_0's rmse: 12.2793\tvalid_0's poisson: -14.0198\n",
            "[1600]\tvalid_0's rmse: 12.2665\tvalid_0's poisson: -14.0408\n",
            "[1800]\tvalid_0's rmse: 12.2321\tvalid_0's poisson: -14.0499\n",
            "[2000]\tvalid_0's rmse: 12.2553\tvalid_0's poisson: -14.0487\n",
            "Early stopping, best iteration is:\n",
            "[1907]\tvalid_0's rmse: 12.2188\tvalid_0's poisson: -14.0588\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005817 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.7053\tvalid_0's poisson: -16.1448\n",
            "[400]\tvalid_0's rmse: 13.3136\tvalid_0's poisson: -17.2199\n",
            "[600]\tvalid_0's rmse: 13.0056\tvalid_0's poisson: -17.4701\n",
            "[800]\tvalid_0's rmse: 12.9431\tvalid_0's poisson: -17.5803\n",
            "Early stopping, best iteration is:\n",
            "[736]\tvalid_0's rmse: 12.9017\tvalid_0's poisson: -17.5624\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.009584 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.5243\tvalid_0's poisson: -14.7158\n",
            "[400]\tvalid_0's rmse: 16.0279\tvalid_0's poisson: -15.7406\n",
            "[600]\tvalid_0's rmse: 15.7539\tvalid_0's poisson: -16.0246\n",
            "[800]\tvalid_0's rmse: 15.5461\tvalid_0's poisson: -16.1405\n",
            "[1000]\tvalid_0's rmse: 15.526\tvalid_0's poisson: -16.1581\n",
            "Early stopping, best iteration is:\n",
            "[879]\tvalid_0's rmse: 15.5045\tvalid_0's poisson: -16.1684\n",
            "[I 2025-08-18 15:52:53,116] Trial 5 finished with value: 15.773682468508973 and parameters: {'objective': 'poisson', 'learning_rate': 0.015716824201651516, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 200, 'subsample': 0.7649101287359542, 'colsample_bytree': 0.8960660809801553, 'reg_alpha': 0.46334686157799443, 'reg_lambda': 1.1955048853696035, 'min_split_gain': 0.00110442342472048}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004977 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.9\tvalid_0's l2: 479.609\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 21.7753\tvalid_0's l2: 474.166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005027 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7047\tvalid_0's l2: 161.409\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 12.6113\tvalid_0's l2: 159.044\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004908 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.166\tvalid_0's l2: 173.345\n",
            "[400]\tvalid_0's rmse: 13.3173\tvalid_0's l2: 177.35\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 13.15\tvalid_0's l2: 172.922\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005207 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9498\tvalid_0's l2: 254.396\n",
            "[400]\tvalid_0's rmse: 15.892\tvalid_0's l2: 252.556\n",
            "Early stopping, best iteration is:\n",
            "[345]\tvalid_0's rmse: 15.8133\tvalid_0's l2: 250.061\n",
            "[I 2025-08-18 15:53:34,938] Trial 6 finished with value: 15.837481535763835 and parameters: {'objective': 'regression', 'learning_rate': 0.03460149293996123, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 80, 'subsample': 0.9226206851751186, 'colsample_bytree': 0.7869894380482674, 'reg_alpha': 0.1985388149115895, 'reg_lambda': 0.7224542260010827, 'min_split_gain': 0.062196464343132446}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004774 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.6005\tvalid_0's poisson: -27.7378\n",
            "Early stopping, best iteration is:\n",
            "[146]\tvalid_0's rmse: 22.3152\tvalid_0's poisson: -27.6688\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004999 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3324\tvalid_0's poisson: -13.7997\n",
            "[400]\tvalid_0's rmse: 12.1295\tvalid_0's poisson: -14.0276\n",
            "[600]\tvalid_0's rmse: 11.9846\tvalid_0's poisson: -14.1118\n",
            "[800]\tvalid_0's rmse: 11.9585\tvalid_0's poisson: -14.1265\n",
            "[1000]\tvalid_0's rmse: 11.9247\tvalid_0's poisson: -14.1341\n",
            "Early stopping, best iteration is:\n",
            "[892]\tvalid_0's rmse: 11.9016\tvalid_0's poisson: -14.1438\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005259 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2524\tvalid_0's poisson: -17.3784\n",
            "[400]\tvalid_0's rmse: 13.3299\tvalid_0's poisson: -17.5634\n",
            "Early stopping, best iteration is:\n",
            "[274]\tvalid_0's rmse: 13.0759\tvalid_0's poisson: -17.5342\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004706 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9093\tvalid_0's poisson: -15.9145\n",
            "[400]\tvalid_0's rmse: 15.7486\tvalid_0's poisson: -16.0067\n",
            "Early stopping, best iteration is:\n",
            "[392]\tvalid_0's rmse: 15.7277\tvalid_0's poisson: -16.0198\n",
            "[I 2025-08-18 15:54:27,796] Trial 7 finished with value: 15.755084493561663 and parameters: {'objective': 'poisson', 'learning_rate': 0.04169990777997927, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.9021570097233794, 'colsample_bytree': 0.7683831592708489, 'reg_alpha': 0.4625803079727366, 'reg_lambda': 2.2282845872753674, 'min_split_gain': 0.10454656587639882}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004587 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7287\tvalid_0's l2: 516.592\n",
            "[400]\tvalid_0's rmse: 22.1304\tvalid_0's l2: 489.756\n",
            "[600]\tvalid_0's rmse: 21.974\tvalid_0's l2: 482.855\n",
            "[800]\tvalid_0's rmse: 21.9385\tvalid_0's l2: 481.299\n",
            "[1000]\tvalid_0's rmse: 21.9282\tvalid_0's l2: 480.847\n",
            "Early stopping, best iteration is:\n",
            "[838]\tvalid_0's rmse: 21.9124\tvalid_0's l2: 480.153\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.042384 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1376\tvalid_0's l2: 172.596\n",
            "[400]\tvalid_0's rmse: 12.9254\tvalid_0's l2: 167.067\n",
            "[600]\tvalid_0's rmse: 12.8949\tvalid_0's l2: 166.278\n",
            "[800]\tvalid_0's rmse: 12.9129\tvalid_0's l2: 166.744\n",
            "Early stopping, best iteration is:\n",
            "[679]\tvalid_0's rmse: 12.8773\tvalid_0's l2: 165.825\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005152 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.4773\tvalid_0's l2: 209.591\n",
            "[400]\tvalid_0's rmse: 13.5761\tvalid_0's l2: 184.311\n",
            "[600]\tvalid_0's rmse: 13.389\tvalid_0's l2: 179.265\n",
            "[800]\tvalid_0's rmse: 13.3228\tvalid_0's l2: 177.497\n",
            "[1000]\tvalid_0's rmse: 13.3053\tvalid_0's l2: 177.031\n",
            "Early stopping, best iteration is:\n",
            "[907]\tvalid_0's rmse: 13.2857\tvalid_0's l2: 176.509\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004951 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.1057\tvalid_0's l2: 259.394\n",
            "[400]\tvalid_0's rmse: 15.8907\tvalid_0's l2: 252.514\n",
            "[600]\tvalid_0's rmse: 15.8142\tvalid_0's l2: 250.089\n",
            "[800]\tvalid_0's rmse: 15.811\tvalid_0's l2: 249.986\n",
            "Early stopping, best iteration is:\n",
            "[694]\tvalid_0's rmse: 15.7952\tvalid_0's l2: 249.488\n",
            "[I 2025-08-18 15:55:54,628] Trial 8 finished with value: 15.967629407315348 and parameters: {'objective': 'regression', 'learning_rate': 0.01051884505877539, 'n_estimators': 7000, 'num_leaves': 95, 'min_child_samples': 160, 'subsample': 0.9315132947852186, 'colsample_bytree': 0.6747876687446624, 'reg_alpha': 0.24622975382137782, 'reg_lambda': 3.1444289849006704, 'min_split_gain': 0.045759633098324495}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005462 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5893\tvalid_0's poisson: -27.7596\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 22.1136\tvalid_0's poisson: -27.7031\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004896 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5661\tvalid_0's poisson: -13.8332\n",
            "[400]\tvalid_0's rmse: 12.3759\tvalid_0's poisson: -13.9768\n",
            "[600]\tvalid_0's rmse: 12.4552\tvalid_0's poisson: -14.0078\n",
            "Early stopping, best iteration is:\n",
            "[453]\tvalid_0's rmse: 12.3165\tvalid_0's poisson: -14.0086\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005020 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1692\tvalid_0's poisson: -17.4657\n",
            "[400]\tvalid_0's rmse: 13.4135\tvalid_0's poisson: -17.5684\n",
            "Early stopping, best iteration is:\n",
            "[242]\tvalid_0's rmse: 13.0105\tvalid_0's poisson: -17.5594\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004905 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5762\tvalid_0's poisson: -16.1052\n",
            "[400]\tvalid_0's rmse: 15.602\tvalid_0's poisson: -16.0225\n",
            "Early stopping, best iteration is:\n",
            "[263]\tvalid_0's rmse: 15.47\tvalid_0's poisson: -16.1457\n",
            "[I 2025-08-18 15:56:54,524] Trial 9 finished with value: 15.72765891330701 and parameters: {'objective': 'poisson', 'learning_rate': 0.044650957058567906, 'n_estimators': 8000, 'num_leaves': 143, 'min_child_samples': 200, 'subsample': 0.9107344153798229, 'colsample_bytree': 0.6559710176658108, 'reg_alpha': 0.5355353990939866, 'reg_lambda': 2.3876978467047776, 'min_split_gain': 0.1614880310328125}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004652 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.611\tvalid_0's tweedie: 11.8171\n",
            "[400]\tvalid_0's rmse: 24.1676\tvalid_0's tweedie: 11.9571\n",
            "Early stopping, best iteration is:\n",
            "[235]\tvalid_0's rmse: 22.2778\tvalid_0's tweedie: 11.7803\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005237 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6462\tvalid_0's tweedie: 9.67915\n",
            "[400]\tvalid_0's rmse: 12.1067\tvalid_0's tweedie: 9.62441\n",
            "Early stopping, best iteration is:\n",
            "[335]\tvalid_0's rmse: 12.3456\tvalid_0's tweedie: 9.61997\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.009922 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3368\tvalid_0's tweedie: 10.1081\n",
            "[400]\tvalid_0's rmse: 13.3053\tvalid_0's tweedie: 10.0103\n",
            "Early stopping, best iteration is:\n",
            "[311]\tvalid_0's rmse: 12.854\tvalid_0's tweedie: 10.0065\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005204 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7866\tvalid_0's tweedie: 9.93786\n",
            "[400]\tvalid_0's rmse: 15.6682\tvalid_0's tweedie: 9.90891\n",
            "Early stopping, best iteration is:\n",
            "[269]\tvalid_0's rmse: 15.6137\tvalid_0's tweedie: 9.87425\n",
            "[I 2025-08-18 15:58:09,158] Trial 10 finished with value: 15.772780337339398 and parameters: {'objective': 'tweedie', 'learning_rate': 0.01718814765928189, 'n_estimators': 3000, 'num_leaves': 191, 'min_child_samples': 120, 'subsample': 0.8650416133630545, 'colsample_bytree': 0.7083935989325427, 'reg_alpha': 0.34664294031762954, 'reg_lambda': 2.416453843746414, 'min_split_gain': 0.11481705215099816, 'tweedie_variance_power': 1.395237344796865}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.035275 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.2372\tvalid_0's tweedie: 38.5403\n",
            "Early stopping, best iteration is:\n",
            "[170]\tvalid_0's rmse: 21.9706\tvalid_0's tweedie: 38.5238\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.036325 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2076\tvalid_0's tweedie: 27.7214\n",
            "[400]\tvalid_0's rmse: 12.1479\tvalid_0's tweedie: 27.6903\n",
            "[600]\tvalid_0's rmse: 12.3499\tvalid_0's tweedie: 27.7475\n",
            "Early stopping, best iteration is:\n",
            "[400]\tvalid_0's rmse: 12.1479\tvalid_0's tweedie: 27.6903\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005045 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0678\tvalid_0's tweedie: 30.1695\n",
            "[400]\tvalid_0's rmse: 13.5054\tvalid_0's tweedie: 30.1153\n",
            "Early stopping, best iteration is:\n",
            "[252]\tvalid_0's rmse: 12.9527\tvalid_0's tweedie: 30.1158\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.006904 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5792\tvalid_0's tweedie: 29.4548\n",
            "[400]\tvalid_0's rmse: 15.7609\tvalid_0's tweedie: 29.5874\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid_0's rmse: 15.4105\tvalid_0's tweedie: 29.4161\n",
            "[I 2025-08-18 15:59:20,101] Trial 11 finished with value: 15.620421029453663 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02247060760847469, 'n_estimators': 7000, 'num_leaves': 191, 'min_child_samples': 120, 'subsample': 0.8354224611266663, 'colsample_bytree': 0.603483925739636, 'reg_alpha': 0.5643890617661809, 'reg_lambda': 3.821435471654726, 'min_split_gain': 0.14752469291369416, 'tweedie_variance_power': 1.1748892059080516}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004900 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.6356\tvalid_0's tweedie: 69.1177\n",
            "Early stopping, best iteration is:\n",
            "[114]\tvalid_0's rmse: 21.7332\tvalid_0's tweedie: 68.7606\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.006557 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.1405\tvalid_0's tweedie: 47.6277\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 12.11\tvalid_0's tweedie: 47.6358\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005099 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1573\tvalid_0's tweedie: 52.4801\n",
            "Early stopping, best iteration is:\n",
            "[175]\tvalid_0's rmse: 13.1262\tvalid_0's tweedie: 52.5083\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005058 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3649\tvalid_0's tweedie: 51.0743\n",
            "Early stopping, best iteration is:\n",
            "[179]\tvalid_0's rmse: 15.3447\tvalid_0's tweedie: 51.0617\n",
            "[I 2025-08-18 16:00:15,044] Trial 12 finished with value: 15.578527024780966 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028589369741394886, 'n_estimators': 5000, 'num_leaves': 191, 'min_child_samples': 100, 'subsample': 0.8428231858051373, 'colsample_bytree': 0.6127817927615141, 'reg_alpha': 0.3668606917093568, 'reg_lambda': 3.9977685352454295, 'min_split_gain': 0.13682520143255805, 'tweedie_variance_power': 1.1127747474052716}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004693 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.1027\tvalid_0's tweedie: 16.7432\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 21.8986\tvalid_0's tweedie: 16.5946\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004938 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2897\tvalid_0's tweedie: 12.9682\n",
            "[400]\tvalid_0's rmse: 12.1931\tvalid_0's tweedie: 13.0169\n",
            "Early stopping, best iteration is:\n",
            "[229]\tvalid_0's rmse: 12.2409\tvalid_0's tweedie: 12.9627\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005062 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8374\tvalid_0's tweedie: 13.6816\n",
            "Early stopping, best iteration is:\n",
            "[182]\tvalid_0's rmse: 12.7647\tvalid_0's tweedie: 13.6868\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005135 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5956\tvalid_0's tweedie: 13.5143\n",
            "Early stopping, best iteration is:\n",
            "[179]\tvalid_0's rmse: 15.5568\tvalid_0's tweedie: 13.5001\n",
            "[I 2025-08-18 16:01:07,180] Trial 13 finished with value: 15.615240791975985 and parameters: {'objective': 'tweedie', 'learning_rate': 0.030161637008924914, 'n_estimators': 4000, 'num_leaves': 159, 'min_child_samples': 100, 'subsample': 0.8727733202516348, 'colsample_bytree': 0.7119775029677049, 'reg_alpha': 0.34065949767837567, 'reg_lambda': 3.106852184061829, 'min_split_gain': 0.12935838012227258, 'tweedie_variance_power': 1.3133251824288117}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005062 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.6123\tvalid_0's tweedie: 7.50235\n",
            "[400]\tvalid_0's rmse: 24.7311\tvalid_0's tweedie: 7.73613\n",
            "Early stopping, best iteration is:\n",
            "[207]\tvalid_0's rmse: 22.5458\tvalid_0's tweedie: 7.50091\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004813 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6144\tvalid_0's tweedie: 6.67735\n",
            "[400]\tvalid_0's rmse: 12.1837\tvalid_0's tweedie: 6.72211\n",
            "Early stopping, best iteration is:\n",
            "[218]\tvalid_0's rmse: 12.5373\tvalid_0's tweedie: 6.67029\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004898 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.6513\tvalid_0's tweedie: 6.80108\n",
            "[400]\tvalid_0's rmse: 13.3355\tvalid_0's tweedie: 6.73528\n",
            "Early stopping, best iteration is:\n",
            "[324]\tvalid_0's rmse: 13.1178\tvalid_0's tweedie: 6.73967\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005166 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7332\tvalid_0's tweedie: 6.71584\n",
            "[400]\tvalid_0's rmse: 15.9179\tvalid_0's tweedie: 6.76102\n",
            "Early stopping, best iteration is:\n",
            "[281]\tvalid_0's rmse: 15.5707\tvalid_0's tweedie: 6.68192\n",
            "[I 2025-08-18 16:02:14,026] Trial 14 finished with value: 15.942908386375425 and parameters: {'objective': 'tweedie', 'learning_rate': 0.018580948318753128, 'n_estimators': 4000, 'num_leaves': 175, 'min_child_samples': 100, 'subsample': 0.8354869235039075, 'colsample_bytree': 0.7113046387396094, 'reg_alpha': 0.3881167535483059, 'reg_lambda': 3.163713216492078, 'min_split_gain': 0.1954532931190156, 'tweedie_variance_power': 1.5604932822610826}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004905 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.0616\tvalid_0's tweedie: 70.0958\n",
            "Early stopping, best iteration is:\n",
            "[103]\tvalid_0's rmse: 21.7919\tvalid_0's tweedie: 69.4847\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005235 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0805\tvalid_0's tweedie: 48.1012\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 11.9841\tvalid_0's tweedie: 48.0861\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005357 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1448\tvalid_0's tweedie: 53.0057\n",
            "Early stopping, best iteration is:\n",
            "[171]\tvalid_0's rmse: 12.9049\tvalid_0's tweedie: 52.9981\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005175 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.686\tvalid_0's tweedie: 51.7257\n",
            "Early stopping, best iteration is:\n",
            "[149]\tvalid_0's rmse: 15.5295\tvalid_0's tweedie: 51.6555\n",
            "[I 2025-08-18 16:02:54,923] Trial 15 finished with value: 15.552572792470285 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03466739723341567, 'n_estimators': 5000, 'num_leaves': 127, 'min_child_samples': 100, 'subsample': 0.8866239850062897, 'colsample_bytree': 0.6373439880174905, 'reg_alpha': 0.27742407485479503, 'reg_lambda': 1.9043340794356798, 'min_split_gain': 0.13150443264775824, 'tweedie_variance_power': 1.1118477057529161}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005228 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.866\tvalid_0's tweedie: 14.4384\n",
            "Early stopping, best iteration is:\n",
            "[90]\tvalid_0's rmse: 22.115\tvalid_0's tweedie: 14.081\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004834 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6674\tvalid_0's tweedie: 11.2621\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 12.4635\tvalid_0's tweedie: 11.2656\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004997 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4729\tvalid_0's tweedie: 11.7296\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's rmse: 13.1508\tvalid_0's tweedie: 11.7293\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005049 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7711\tvalid_0's tweedie: 11.7253\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 15.5306\tvalid_0's tweedie: 11.6115\n",
            "[I 2025-08-18 16:03:32,257] Trial 16 finished with value: 15.814967256300458 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03708657756427748, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 80, 'subsample': 0.9496253896745149, 'colsample_bytree': 0.8516221247181768, 'reg_alpha': 0.1779202566400414, 'reg_lambda': 1.9329126254928861, 'min_split_gain': 0.09307533990004449, 'tweedie_variance_power': 1.3508458891678647}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005008 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.8902\tvalid_0's tweedie: 25.4051\n",
            "[400]\tvalid_0's rmse: 24.8548\tvalid_0's tweedie: 25.8974\n",
            "Early stopping, best iteration is:\n",
            "[202]\tvalid_0's rmse: 21.8697\tvalid_0's tweedie: 25.4017\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005193 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3476\tvalid_0's tweedie: 18.9937\n",
            "[400]\tvalid_0's rmse: 12.1912\tvalid_0's tweedie: 18.9049\n",
            "Early stopping, best iteration is:\n",
            "[379]\tvalid_0's rmse: 12.1766\tvalid_0's tweedie: 18.8993\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004848 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2646\tvalid_0's tweedie: 20.4249\n",
            "[400]\tvalid_0's rmse: 13.6927\tvalid_0's tweedie: 20.3591\n",
            "Early stopping, best iteration is:\n",
            "[283]\tvalid_0's rmse: 13.0571\tvalid_0's tweedie: 20.3294\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005370 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5525\tvalid_0's tweedie: 19.9864\n",
            "[400]\tvalid_0's rmse: 15.572\tvalid_0's tweedie: 20.0086\n",
            "Early stopping, best iteration is:\n",
            "[276]\tvalid_0's rmse: 15.4288\tvalid_0's tweedie: 19.932\n",
            "[I 2025-08-18 16:04:29,285] Trial 17 finished with value: 15.633060846253523 and parameters: {'objective': 'tweedie', 'learning_rate': 0.019884347306329243, 'n_estimators': 5000, 'num_leaves': 127, 'min_child_samples': 120, 'subsample': 0.8886885139276302, 'colsample_bytree': 0.6400793435520217, 'reg_alpha': 0.27621438062229775, 'reg_lambda': 1.2949397402460316, 'min_split_gain': 0.03990107435966163, 'tweedie_variance_power': 1.2347443814931296}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004790 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.0551\tvalid_0's tweedie: 9.37144\n",
            "[400]\tvalid_0's rmse: 22.9774\tvalid_0's tweedie: 9.32183\n",
            "Early stopping, best iteration is:\n",
            "[305]\tvalid_0's rmse: 22.0524\tvalid_0's tweedie: 9.26339\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004849 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8809\tvalid_0's tweedie: 7.96804\n",
            "[400]\tvalid_0's rmse: 12.421\tvalid_0's tweedie: 7.85367\n",
            "Early stopping, best iteration is:\n",
            "[381]\tvalid_0's rmse: 12.4879\tvalid_0's tweedie: 7.85325\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.2927\tvalid_0's tweedie: 8.28023\n",
            "[400]\tvalid_0's rmse: 13.1809\tvalid_0's tweedie: 8.10682\n",
            "[600]\tvalid_0's rmse: 13.7172\tvalid_0's tweedie: 8.11055\n",
            "Early stopping, best iteration is:\n",
            "[407]\tvalid_0's rmse: 13.1437\tvalid_0's tweedie: 8.10366\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.008852 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8802\tvalid_0's tweedie: 8.12457\n",
            "[400]\tvalid_0's rmse: 15.4904\tvalid_0's tweedie: 7.99317\n",
            "Early stopping, best iteration is:\n",
            "[377]\tvalid_0's rmse: 15.4881\tvalid_0's tweedie: 7.99051\n",
            "[I 2025-08-18 16:05:42,575] Trial 18 finished with value: 15.793015582896006 and parameters: {'objective': 'tweedie', 'learning_rate': 0.013946899292001554, 'n_estimators': 4000, 'num_leaves': 143, 'min_child_samples': 140, 'subsample': 0.8875606695557635, 'colsample_bytree': 0.6850177452919625, 'reg_alpha': 0.1440470957242423, 'reg_lambda': 0.9856947880070956, 'min_split_gain': 0.11953965914950453, 'tweedie_variance_power': 1.4687165605562627}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004946 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.6135\tvalid_0's tweedie: 21.8488\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 22.0517\tvalid_0's tweedie: 21.5998\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005004 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2637\tvalid_0's tweedie: 16.3901\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 12.3134\tvalid_0's tweedie: 16.3863\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005129 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8821\tvalid_0's tweedie: 17.4672\n",
            "Early stopping, best iteration is:\n",
            "[175]\tvalid_0's rmse: 12.7886\tvalid_0's tweedie: 17.4734\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005197 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5637\tvalid_0's tweedie: 17.2259\n",
            "Early stopping, best iteration is:\n",
            "[160]\tvalid_0's rmse: 15.5966\tvalid_0's tweedie: 17.1801\n",
            "[I 2025-08-18 16:06:22,336] Trial 19 finished with value: 15.68756459529561 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03247790361904009, 'n_estimators': 5000, 'num_leaves': 111, 'min_child_samples': 100, 'subsample': 0.9434917893214666, 'colsample_bytree': 0.7283617836586176, 'reg_alpha': 0.2870096154567432, 'reg_lambda': 1.8013132125762101, 'min_split_gain': 0.17249617983611412, 'tweedie_variance_power': 1.2623454049935752}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005221 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.6207\tvalid_0's tweedie: 10.8361\n",
            "Early stopping, best iteration is:\n",
            "[156]\tvalid_0's rmse: 22.3485\tvalid_0's tweedie: 10.8098\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.017848 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4924\tvalid_0's tweedie: 8.93529\n",
            "[400]\tvalid_0's rmse: 12.2947\tvalid_0's tweedie: 8.97945\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 12.3701\tvalid_0's tweedie: 8.92187\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005433 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2486\tvalid_0's tweedie: 9.28936\n",
            "[400]\tvalid_0's rmse: 14.1016\tvalid_0's tweedie: 9.31148\n",
            "Early stopping, best iteration is:\n",
            "[219]\tvalid_0's rmse: 13.1945\tvalid_0's tweedie: 9.27863\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005417 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3824\tvalid_0's tweedie: 9.1671\n",
            "[400]\tvalid_0's rmse: 16.1403\tvalid_0's tweedie: 9.37683\n",
            "Early stopping, best iteration is:\n",
            "[215]\tvalid_0's rmse: 15.3611\tvalid_0's tweedie: 9.16548\n",
            "[I 2025-08-18 16:07:18,686] Trial 20 finished with value: 15.818549193462243 and parameters: {'objective': 'tweedie', 'learning_rate': 0.025114236253011117, 'n_estimators': 3000, 'num_leaves': 159, 'min_child_samples': 80, 'subsample': 0.863940940013763, 'colsample_bytree': 0.6362954711057588, 'reg_alpha': 0.49759361086071774, 'reg_lambda': 2.741855466779706, 'min_split_gain': 0.09814360081167603, 'tweedie_variance_power': 1.419693020978923}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005134 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.6815\tvalid_0's tweedie: 70.3653\n",
            "Early stopping, best iteration is:\n",
            "[130]\tvalid_0's rmse: 21.9018\tvalid_0's tweedie: 69.9835\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004879 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0404\tvalid_0's tweedie: 48.3961\n",
            "[400]\tvalid_0's rmse: 12.0952\tvalid_0's tweedie: 48.3625\n",
            "Early stopping, best iteration is:\n",
            "[229]\tvalid_0's rmse: 12.0278\tvalid_0's tweedie: 48.3842\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005023 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2852\tvalid_0's tweedie: 53.4023\n",
            "Early stopping, best iteration is:\n",
            "[146]\tvalid_0's rmse: 13.0793\tvalid_0's tweedie: 53.4525\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005167 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4291\tvalid_0's tweedie: 51.9475\n",
            "Early stopping, best iteration is:\n",
            "[186]\tvalid_0's rmse: 15.4435\tvalid_0's tweedie: 51.9416\n",
            "[I 2025-08-18 16:08:10,026] Trial 21 finished with value: 15.613109041654122 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028852067420575046, 'n_estimators': 5000, 'num_leaves': 159, 'min_child_samples': 100, 'subsample': 0.8424485585222518, 'colsample_bytree': 0.6010747490532914, 'reg_alpha': 0.38250303531500607, 'reg_lambda': 2.1640679196323287, 'min_split_gain': 0.13536562523314966, 'tweedie_variance_power': 1.1112082805479917}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004683 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 25.3667\tvalid_0's tweedie: 71.4913\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 22.1685\tvalid_0's tweedie: 70.7123\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004882 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0724\tvalid_0's tweedie: 48.7893\n",
            "[400]\tvalid_0's rmse: 12.0069\tvalid_0's tweedie: 48.7868\n",
            "Early stopping, best iteration is:\n",
            "[320]\tvalid_0's rmse: 11.9707\tvalid_0's tweedie: 48.7621\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.037804 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3693\tvalid_0's tweedie: 53.8003\n",
            "Early stopping, best iteration is:\n",
            "[124]\tvalid_0's rmse: 13.0886\tvalid_0's tweedie: 53.8494\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.007730 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5654\tvalid_0's tweedie: 52.3977\n",
            "Early stopping, best iteration is:\n",
            "[170]\tvalid_0's rmse: 15.4718\tvalid_0's tweedie: 52.3627\n",
            "[I 2025-08-18 16:08:55,190] Trial 22 finished with value: 15.674924003190208 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03844855251030044, 'n_estimators': 6000, 'num_leaves': 127, 'min_child_samples': 120, 'subsample': 0.8101566809465273, 'colsample_bytree': 0.6299561842718182, 'reg_alpha': 0.32777504033210425, 'reg_lambda': 3.578842087564154, 'min_split_gain': 0.1365113911013482, 'tweedie_variance_power': 1.11049640833885}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.007548 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7221\tvalid_0's tweedie: 34.1467\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 22.0185\tvalid_0's tweedie: 34.0467\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.009034 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3497\tvalid_0's tweedie: 24.7452\n",
            "[400]\tvalid_0's rmse: 12.2751\tvalid_0's tweedie: 24.7454\n",
            "Early stopping, best iteration is:\n",
            "[285]\tvalid_0's rmse: 12.3181\tvalid_0's tweedie: 24.7346\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.011671 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0915\tvalid_0's tweedie: 26.7861\n",
            "[400]\tvalid_0's rmse: 14.0734\tvalid_0's tweedie: 26.8521\n",
            "Early stopping, best iteration is:\n",
            "[217]\tvalid_0's rmse: 13.0556\tvalid_0's tweedie: 26.7713\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005022 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4683\tvalid_0's tweedie: 26.1922\n",
            "Early stopping, best iteration is:\n",
            "[199]\tvalid_0's rmse: 15.4636\tvalid_0's tweedie: 26.1906\n",
            "[I 2025-08-18 16:09:57,515] Trial 23 finished with value: 15.713966241438104 and parameters: {'objective': 'tweedie', 'learning_rate': 0.0264261496498198, 'n_estimators': 5000, 'num_leaves': 175, 'min_child_samples': 100, 'subsample': 0.8913717266513894, 'colsample_bytree': 0.6807900806143269, 'reg_alpha': 0.2473452702185462, 'reg_lambda': 1.9614096628894502, 'min_split_gain': 0.14937788559453383, 'tweedie_variance_power': 1.1912969347643134}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005068 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3304\tvalid_0's tweedie: 41.2964\n",
            "Early stopping, best iteration is:\n",
            "[173]\tvalid_0's rmse: 22.0443\tvalid_0's tweedie: 41.2823\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005817 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2958\tvalid_0's tweedie: 29.5803\n",
            "[400]\tvalid_0's rmse: 12.0784\tvalid_0's tweedie: 29.4793\n",
            "[600]\tvalid_0's rmse: 12.0461\tvalid_0's tweedie: 29.4963\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's rmse: 12.0732\tvalid_0's tweedie: 29.4772\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005022 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2069\tvalid_0's tweedie: 32.235\n",
            "[400]\tvalid_0's rmse: 13.6432\tvalid_0's tweedie: 32.1682\n",
            "Early stopping, best iteration is:\n",
            "[267]\tvalid_0's rmse: 13.0712\tvalid_0's tweedie: 32.1486\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005639 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.548\tvalid_0's tweedie: 31.4619\n",
            "[400]\tvalid_0's rmse: 15.5926\tvalid_0's tweedie: 31.5059\n",
            "Early stopping, best iteration is:\n",
            "[253]\tvalid_0's rmse: 15.446\tvalid_0's tweedie: 31.4167\n",
            "[I 2025-08-18 16:10:52,471] Trial 24 finished with value: 15.658689757103383 and parameters: {'objective': 'tweedie', 'learning_rate': 0.020593805943850938, 'n_estimators': 4000, 'num_leaves': 127, 'min_child_samples': 80, 'subsample': 0.8199526681334187, 'colsample_bytree': 0.6512742506621042, 'reg_alpha': 0.39181354371922106, 'reg_lambda': 2.8292657589276384, 'min_split_gain': 0.1108711825693062, 'tweedie_variance_power': 1.1664049781841495}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005094 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.7796\tvalid_0's l2: 474.35\n",
            "[400]\tvalid_0's rmse: 21.7851\tvalid_0's l2: 474.592\n",
            "Early stopping, best iteration is:\n",
            "[218]\tvalid_0's rmse: 21.7148\tvalid_0's l2: 471.532\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005095 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8534\tvalid_0's l2: 165.211\n",
            "Early stopping, best iteration is:\n",
            "[132]\tvalid_0's rmse: 12.7875\tvalid_0's l2: 163.521\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004660 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5324\tvalid_0's l2: 183.126\n",
            "[400]\tvalid_0's rmse: 13.3511\tvalid_0's l2: 178.252\n",
            "Early stopping, best iteration is:\n",
            "[374]\tvalid_0's rmse: 13.3222\tvalid_0's l2: 177.482\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004959 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9425\tvalid_0's l2: 254.162\n",
            "Early stopping, best iteration is:\n",
            "[129]\tvalid_0's rmse: 15.8667\tvalid_0's l2: 251.753\n",
            "[I 2025-08-18 16:11:31,385] Trial 25 finished with value: 15.922823393521695 and parameters: {'objective': 'regression', 'learning_rate': 0.02991017175013676, 'n_estimators': 6000, 'num_leaves': 95, 'min_child_samples': 140, 'subsample': 0.8613170171542941, 'colsample_bytree': 0.6188198827497317, 'reg_alpha': 0.4265544715541839, 'reg_lambda': 1.5845542273541036, 'min_split_gain': 0.17789940329676346}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004766 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.6404\tvalid_0's tweedie: 18.2753\n",
            "Early stopping, best iteration is:\n",
            "[152]\tvalid_0's rmse: 22.2475\tvalid_0's tweedie: 18.2402\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004711 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2062\tvalid_0's tweedie: 14.0978\n",
            "[400]\tvalid_0's rmse: 12.1107\tvalid_0's tweedie: 14.0836\n",
            "Early stopping, best iteration is:\n",
            "[260]\tvalid_0's rmse: 12.2052\tvalid_0's tweedie: 14.0818\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004714 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0054\tvalid_0's tweedie: 14.9078\n",
            "[400]\tvalid_0's rmse: 13.6355\tvalid_0's tweedie: 14.9064\n",
            "Early stopping, best iteration is:\n",
            "[241]\tvalid_0's rmse: 12.8884\tvalid_0's tweedie: 14.87\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6476\tvalid_0's tweedie: 14.6909\n",
            "Early stopping, best iteration is:\n",
            "[199]\tvalid_0's rmse: 15.6461\tvalid_0's tweedie: 14.6902\n",
            "[I 2025-08-18 16:12:24,965] Trial 26 finished with value: 15.746807668873284 and parameters: {'objective': 'tweedie', 'learning_rate': 0.024203185783730298, 'n_estimators': 8000, 'num_leaves': 143, 'min_child_samples': 100, 'subsample': 0.906470010242546, 'colsample_bytree': 0.751317269586346, 'reg_alpha': 0.3140674719869775, 'reg_lambda': 1.2393114588181464, 'min_split_gain': 0.0869791659666497, 'tweedie_variance_power': 1.2946313865577248}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004867 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.2551\tvalid_0's tweedie: 80.2338\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 22.1795\tvalid_0's tweedie: 79.8961\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.016232 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2184\tvalid_0's tweedie: 54.7423\n",
            "[400]\tvalid_0's rmse: 12.328\tvalid_0's tweedie: 54.7706\n",
            "Early stopping, best iteration is:\n",
            "[261]\tvalid_0's rmse: 12.1256\tvalid_0's tweedie: 54.7121\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005198 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.507\tvalid_0's tweedie: 60.5562\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 13.1288\tvalid_0's tweedie: 60.6165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004997 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.758\tvalid_0's tweedie: 58.9602\n",
            "Early stopping, best iteration is:\n",
            "[168]\tvalid_0's rmse: 15.6476\tvalid_0's tweedie: 58.9149\n",
            "[I 2025-08-18 16:13:17,387] Trial 27 finished with value: 15.770386348410092 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03351950265559344, 'n_estimators': 4000, 'num_leaves': 175, 'min_child_samples': 120, 'subsample': 0.8777528057677301, 'colsample_bytree': 0.6743187886746159, 'reg_alpha': 0.21323232854175989, 'reg_lambda': 2.6172726319959576, 'min_split_gain': 0.12503587637704353, 'tweedie_variance_power': 1.1002705006465436}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004974 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 26.3309\tvalid_0's tweedie: 9.01411\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 21.6541\tvalid_0's tweedie: 8.52415\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004856 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0939\tvalid_0's tweedie: 7.46611\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 12.2209\tvalid_0's tweedie: 7.35528\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005239 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.8003\tvalid_0's tweedie: 7.51395\n",
            "Early stopping, best iteration is:\n",
            "[111]\tvalid_0's rmse: 12.9214\tvalid_0's tweedie: 7.50938\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005112 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9482\tvalid_0's tweedie: 7.56331\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 15.812\tvalid_0's tweedie: 7.46085\n",
            "[I 2025-08-18 16:13:49,823] Trial 28 finished with value: 15.652123312870163 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04826391782300679, 'n_estimators': 5000, 'num_leaves': 111, 'min_child_samples': 80, 'subsample': 0.8570704857217336, 'colsample_bytree': 0.8274555108011766, 'reg_alpha': 0.12381161573765373, 'reg_lambda': 3.597033199050255, 'min_split_gain': 0.14407284419567415, 'tweedie_variance_power': 1.5011977113796682}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004886 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.8238\tvalid_0's l2: 476.28\n",
            "[400]\tvalid_0's rmse: 21.9392\tvalid_0's l2: 481.328\n",
            "Early stopping, best iteration is:\n",
            "[216]\tvalid_0's rmse: 21.7506\tvalid_0's l2: 473.089\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005123 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4954\tvalid_0's l2: 156.136\n",
            "[400]\tvalid_0's rmse: 12.5035\tvalid_0's l2: 156.338\n",
            "[600]\tvalid_0's rmse: 12.631\tvalid_0's l2: 159.542\n",
            "Early stopping, best iteration is:\n",
            "[408]\tvalid_0's rmse: 12.4778\tvalid_0's l2: 155.695\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005399 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1859\tvalid_0's l2: 173.869\n",
            "[400]\tvalid_0's rmse: 13.123\tvalid_0's l2: 172.213\n",
            "Early stopping, best iteration is:\n",
            "[293]\tvalid_0's rmse: 13.101\tvalid_0's l2: 171.636\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005185 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7674\tvalid_0's l2: 248.61\n",
            "[400]\tvalid_0's rmse: 15.8045\tvalid_0's l2: 249.781\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid_0's rmse: 15.749\tvalid_0's l2: 248.032\n",
            "[I 2025-08-18 16:14:35,014] Trial 29 finished with value: 15.76960211639184 and parameters: {'objective': 'regression', 'learning_rate': 0.026883927840766448, 'n_estimators': 4000, 'num_leaves': 95, 'min_child_samples': 80, 'subsample': 0.9232861184209656, 'colsample_bytree': 0.6955630310104953, 'reg_alpha': 0.4331538024738121, 'reg_lambda': 1.0125643392759205, 'min_split_gain': 0.1799328655502752}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005009 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 25.2019\tvalid_0's tweedie: 12.7348\n",
            "Early stopping, best iteration is:\n",
            "[107]\tvalid_0's rmse: 21.8465\tvalid_0's tweedie: 12.2907\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005265 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2078\tvalid_0's tweedie: 10.0348\n",
            "Early stopping, best iteration is:\n",
            "[129]\tvalid_0's rmse: 12.2763\tvalid_0's tweedie: 10.0152\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005086 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4255\tvalid_0's tweedie: 10.4331\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 12.8176\tvalid_0's tweedie: 10.4362\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005329 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.022\tvalid_0's tweedie: 10.428\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 15.7264\tvalid_0's tweedie: 10.3164\n",
            "[I 2025-08-18 16:15:18,990] Trial 30 finished with value: 15.666674095813303 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03903478870327048, 'n_estimators': 6000, 'num_leaves': 159, 'min_child_samples': 100, 'subsample': 0.8473908795381443, 'colsample_bytree': 0.7375937813879478, 'reg_alpha': 0.5056480855111826, 'reg_lambda': 0.7387666839967129, 'min_split_gain': 0.15469966507762992, 'tweedie_variance_power': 1.3833003135765793}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004985 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.6906\tvalid_0's tweedie: 52.5665\n",
            "Early stopping, best iteration is:\n",
            "[130]\tvalid_0's rmse: 22.2078\tvalid_0's tweedie: 52.2746\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005484 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0248\tvalid_0's tweedie: 36.7676\n",
            "[400]\tvalid_0's rmse: 12.1782\tvalid_0's tweedie: 36.8123\n",
            "Early stopping, best iteration is:\n",
            "[206]\tvalid_0's rmse: 12.0179\tvalid_0's tweedie: 36.7643\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005343 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1909\tvalid_0's tweedie: 40.2838\n",
            "Early stopping, best iteration is:\n",
            "[165]\tvalid_0's rmse: 13.1459\tvalid_0's tweedie: 40.3201\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005127 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4033\tvalid_0's tweedie: 39.286\n",
            "[400]\tvalid_0's rmse: 15.889\tvalid_0's tweedie: 39.585\n",
            "Early stopping, best iteration is:\n",
            "[215]\tvalid_0's rmse: 15.357\tvalid_0's tweedie: 39.2762\n",
            "[I 2025-08-18 16:16:15,879] Trial 31 finished with value: 15.682129606837012 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028586417651508045, 'n_estimators': 5000, 'num_leaves': 191, 'min_child_samples': 100, 'subsample': 0.8377530003804945, 'colsample_bytree': 0.6017901355084659, 'reg_alpha': 0.37724204379932025, 'reg_lambda': 1.98088426135634, 'min_split_gain': 0.12446766509544566, 'tweedie_variance_power': 1.1395094373164354}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005081 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3237\tvalid_0's tweedie: 27.3772\n",
            "Early stopping, best iteration is:\n",
            "[173]\tvalid_0's rmse: 22.0835\tvalid_0's tweedie: 27.3682\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004848 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3198\tvalid_0's tweedie: 20.3011\n",
            "[400]\tvalid_0's rmse: 12.372\tvalid_0's tweedie: 20.2513\n",
            "Early stopping, best iteration is:\n",
            "[315]\tvalid_0's rmse: 12.1734\tvalid_0's tweedie: 20.2343\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005064 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3073\tvalid_0's tweedie: 21.8725\n",
            "[400]\tvalid_0's rmse: 13.472\tvalid_0's tweedie: 21.7934\n",
            "Early stopping, best iteration is:\n",
            "[252]\tvalid_0's rmse: 13.0464\tvalid_0's tweedie: 21.8062\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005302 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4972\tvalid_0's tweedie: 21.3991\n",
            "[400]\tvalid_0's rmse: 15.7849\tvalid_0's tweedie: 21.5362\n",
            "Early stopping, best iteration is:\n",
            "[223]\tvalid_0's rmse: 15.4748\tvalid_0's tweedie: 21.3917\n",
            "[I 2025-08-18 16:17:16,335] Trial 32 finished with value: 15.694534268181439 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02108967116258634, 'n_estimators': 5000, 'num_leaves': 159, 'min_child_samples': 80, 'subsample': 0.8178610776036992, 'colsample_bytree': 0.6015546914355853, 'reg_alpha': 0.3694370541636808, 'reg_lambda': 2.197398367473994, 'min_split_gain': 0.13330688856596276, 'tweedie_variance_power': 1.2230341406761043}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005324 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.2696\tvalid_0's tweedie: 55.6226\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 21.9972\tvalid_0's tweedie: 55.2047\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005094 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 11.989\tvalid_0's tweedie: 38.7293\n",
            "Early stopping, best iteration is:\n",
            "[187]\tvalid_0's rmse: 11.9062\tvalid_0's tweedie: 38.7264\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005205 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1925\tvalid_0's tweedie: 42.484\n",
            "Early stopping, best iteration is:\n",
            "[137]\tvalid_0's rmse: 13.0631\tvalid_0's tweedie: 42.5571\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004862 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4743\tvalid_0's tweedie: 41.3945\n",
            "Early stopping, best iteration is:\n",
            "[180]\tvalid_0's rmse: 15.4017\tvalid_0's tweedie: 41.363\n",
            "[I 2025-08-18 16:18:09,697] Trial 33 finished with value: 15.592061316857395 and parameters: {'objective': 'tweedie', 'learning_rate': 0.030902839680204148, 'n_estimators': 5000, 'num_leaves': 175, 'min_child_samples': 120, 'subsample': 0.7917842284731208, 'colsample_bytree': 0.6233742774807303, 'reg_alpha': 0.41975493308559414, 'reg_lambda': 1.7636933345501786, 'min_split_gain': 0.10685390614956913, 'tweedie_variance_power': 1.133766476859552}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004823 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.3978\tvalid_0's tweedie: 45.0672\n",
            "Early stopping, best iteration is:\n",
            "[111]\tvalid_0's rmse: 22.0378\tvalid_0's tweedie: 44.5688\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005109 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0889\tvalid_0's tweedie: 31.6897\n",
            "[400]\tvalid_0's rmse: 12.0144\tvalid_0's tweedie: 31.6654\n",
            "Early stopping, best iteration is:\n",
            "[329]\tvalid_0's rmse: 11.9922\tvalid_0's tweedie: 31.6498\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004900 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2332\tvalid_0's tweedie: 34.6066\n",
            "Early stopping, best iteration is:\n",
            "[143]\tvalid_0's rmse: 13.1294\tvalid_0's tweedie: 34.6501\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005145 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4362\tvalid_0's tweedie: 33.7947\n",
            "Early stopping, best iteration is:\n",
            "[178]\tvalid_0's rmse: 15.3434\tvalid_0's tweedie: 33.7681\n",
            "[I 2025-08-18 16:19:05,977] Trial 34 finished with value: 15.625701134143656 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03242346985361163, 'n_estimators': 4000, 'num_leaves': 175, 'min_child_samples': 140, 'subsample': 0.7768424163047266, 'colsample_bytree': 0.6239589396978755, 'reg_alpha': 0.45811397764766537, 'reg_lambda': 1.486417114853508, 'min_split_gain': 0.07887517495996568, 'tweedie_variance_power': 1.157227122372764}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004656 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.1243\tvalid_0's poisson: -27.5995\n",
            "[400]\tvalid_0's rmse: 22.9873\tvalid_0's poisson: -27.7292\n",
            "Early stopping, best iteration is:\n",
            "[250]\tvalid_0's rmse: 21.9377\tvalid_0's poisson: -27.8492\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.009494 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.484\tvalid_0's poisson: -13.4923\n",
            "[400]\tvalid_0's rmse: 12.1905\tvalid_0's poisson: -13.9246\n",
            "[600]\tvalid_0's rmse: 12.1846\tvalid_0's poisson: -13.9808\n",
            "Early stopping, best iteration is:\n",
            "[470]\tvalid_0's rmse: 12.1062\tvalid_0's poisson: -13.9672\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005710 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.6654\tvalid_0's poisson: -17.0123\n",
            "[400]\tvalid_0's rmse: 13.1507\tvalid_0's poisson: -17.5187\n",
            "[600]\tvalid_0's rmse: 13.3319\tvalid_0's poisson: -17.5747\n",
            "Early stopping, best iteration is:\n",
            "[447]\tvalid_0's rmse: 13.0699\tvalid_0's poisson: -17.568\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005304 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7788\tvalid_0's poisson: -15.6554\n",
            "[400]\tvalid_0's rmse: 15.45\tvalid_0's poisson: -16.1531\n",
            "[600]\tvalid_0's rmse: 15.5901\tvalid_0's poisson: -16.0853\n",
            "Early stopping, best iteration is:\n",
            "[465]\tvalid_0's rmse: 15.3506\tvalid_0's poisson: -16.1928\n",
            "[I 2025-08-18 16:20:36,290] Trial 35 finished with value: 15.616084757137557 and parameters: {'objective': 'poisson', 'learning_rate': 0.024153567276649482, 'n_estimators': 6000, 'num_leaves': 191, 'min_child_samples': 120, 'subsample': 0.7911272612768993, 'colsample_bytree': 0.6576369099900042, 'reg_alpha': 0.2938700757980006, 'reg_lambda': 1.6842271799467976, 'min_split_gain': 0.06423613720159106}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005058 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.0247\tvalid_0's l2: 485.088\n",
            "[400]\tvalid_0's rmse: 21.746\tvalid_0's l2: 472.89\n",
            "[600]\tvalid_0's rmse: 21.797\tvalid_0's l2: 475.108\n",
            "Early stopping, best iteration is:\n",
            "[424]\tvalid_0's rmse: 21.7223\tvalid_0's l2: 471.857\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.008165 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9\tvalid_0's l2: 166.41\n",
            "[400]\tvalid_0's rmse: 12.7516\tvalid_0's l2: 162.604\n",
            "[600]\tvalid_0's rmse: 12.787\tvalid_0's l2: 163.508\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's rmse: 12.7348\tvalid_0's l2: 162.176\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004782 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.7\tvalid_0's l2: 187.689\n",
            "[400]\tvalid_0's rmse: 13.2456\tvalid_0's l2: 175.447\n",
            "[600]\tvalid_0's rmse: 13.2335\tvalid_0's l2: 175.125\n",
            "[800]\tvalid_0's rmse: 13.2574\tvalid_0's l2: 175.758\n",
            "Early stopping, best iteration is:\n",
            "[660]\tvalid_0's rmse: 13.2073\tvalid_0's l2: 174.432\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005048 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7752\tvalid_0's l2: 248.856\n",
            "[400]\tvalid_0's rmse: 15.7181\tvalid_0's l2: 247.059\n",
            "[600]\tvalid_0's rmse: 15.7013\tvalid_0's l2: 246.532\n",
            "[800]\tvalid_0's rmse: 15.7555\tvalid_0's l2: 248.237\n",
            "Early stopping, best iteration is:\n",
            "[617]\tvalid_0's rmse: 15.6922\tvalid_0's l2: 246.246\n",
            "[I 2025-08-18 16:21:27,090] Trial 36 finished with value: 15.839151036273869 and parameters: {'objective': 'regression', 'learning_rate': 0.016783029491717706, 'n_estimators': 7000, 'num_leaves': 63, 'min_child_samples': 120, 'subsample': 0.795633171022208, 'colsample_bytree': 0.6415710778951217, 'reg_alpha': 0.42016789006081023, 'reg_lambda': 1.4441005061581815, 'min_split_gain': 0.10566004139746453}. Best is trial 3 with value: 15.55155755194344.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004734 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.5195\tvalid_0's tweedie: 31.0566\n",
            "Early stopping, best iteration is:\n",
            "[108]\tvalid_0's rmse: 21.5316\tvalid_0's tweedie: 30.4465\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.007144 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.1789\tvalid_0's tweedie: 22.3293\n",
            "Early stopping, best iteration is:\n",
            "[192]\tvalid_0's rmse: 12.1546\tvalid_0's tweedie: 22.3335\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005612 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2027\tvalid_0's tweedie: 24.1238\n",
            "Early stopping, best iteration is:\n",
            "[130]\tvalid_0's rmse: 13.0019\tvalid_0's tweedie: 24.1763\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004921 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5967\tvalid_0's tweedie: 23.694\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 15.4919\tvalid_0's tweedie: 23.6274\n",
            "[I 2025-08-18 16:22:08,977] Trial 37 finished with value: 15.545001315866337 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03596777509460307, 'n_estimators': 3000, 'num_leaves': 143, 'min_child_samples': 100, 'subsample': 0.8237943147963162, 'colsample_bytree': 0.6167571826244366, 'reg_alpha': 0.013679988917289054, 'reg_lambda': 1.7016810829275064, 'min_split_gain': 0.0890783733897744, 'tweedie_variance_power': 1.2067968200127233}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004768 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.1278\tvalid_0's poisson: -27.6953\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's rmse: 22.8819\tvalid_0's poisson: -27.6068\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005174 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6143\tvalid_0's poisson: -13.7707\n",
            "[400]\tvalid_0's rmse: 12.362\tvalid_0's poisson: -13.9915\n",
            "Early stopping, best iteration is:\n",
            "[335]\tvalid_0's rmse: 12.3553\tvalid_0's poisson: -13.9678\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005732 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3912\tvalid_0's poisson: -17.3551\n",
            "[400]\tvalid_0's rmse: 13.4824\tvalid_0's poisson: -17.5627\n",
            "Early stopping, best iteration is:\n",
            "[252]\tvalid_0's rmse: 13.218\tvalid_0's poisson: -17.4953\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005118 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6433\tvalid_0's poisson: -15.9743\n",
            "[400]\tvalid_0's rmse: 15.4688\tvalid_0's poisson: -16.0735\n",
            "Early stopping, best iteration is:\n",
            "[313]\tvalid_0's rmse: 15.4503\tvalid_0's poisson: -16.1204\n",
            "[I 2025-08-18 16:23:04,228] Trial 38 finished with value: 15.976373569403238 and parameters: {'objective': 'poisson', 'learning_rate': 0.03578341287524359, 'n_estimators': 3000, 'num_leaves': 143, 'min_child_samples': 80, 'subsample': 0.8306203570720593, 'colsample_bytree': 0.6728480565287784, 'reg_alpha': 0.03387562226961796, 'reg_lambda': 0.9599536894781066, 'min_split_gain': 0.08686673540199859}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005023 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.6328\tvalid_0's tweedie: 30.8325\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid_0's rmse: 21.8102\tvalid_0's tweedie: 30.3664\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004698 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.0989\tvalid_0's tweedie: 22.2797\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 12.0868\tvalid_0's tweedie: 22.2718\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.011082 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2671\tvalid_0's tweedie: 24.0534\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 12.7823\tvalid_0's tweedie: 24.0649\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.011761 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7653\tvalid_0's tweedie: 23.6373\n",
            "Early stopping, best iteration is:\n",
            "[145]\tvalid_0's rmse: 15.5337\tvalid_0's tweedie: 23.5284\n",
            "[I 2025-08-18 16:23:43,623] Trial 39 finished with value: 15.553248790114292 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04255236992528109, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 160, 'subsample': 0.9324136274820501, 'colsample_bytree': 0.6190244748014355, 'reg_alpha': 0.06549120129327345, 'reg_lambda': 2.5002470877996155, 'min_split_gain': 0.044566099328695555, 'tweedie_variance_power': 1.207168556169362}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004919 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\tvalid_0's rmse: 22.481\tvalid_0's l2: 505.396\n",
            "[400]\tvalid_0's rmse: 22.3064\tvalid_0's l2: 497.576\n",
            "[600]\tvalid_0's rmse: 22.3266\tvalid_0's l2: 498.479\n",
            "Early stopping, best iteration is:\n",
            "[438]\tvalid_0's rmse: 22.2456\tvalid_0's l2: 494.866\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005149 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.08\tvalid_0's l2: 171.085\n",
            "[400]\tvalid_0's rmse: 13.1178\tvalid_0's l2: 172.076\n",
            "Early stopping, best iteration is:\n",
            "[260]\tvalid_0's rmse: 13.0166\tvalid_0's l2: 169.432\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004893 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3444\tvalid_0's l2: 178.072\n",
            "[400]\tvalid_0's rmse: 13.2804\tvalid_0's l2: 176.37\n",
            "Early stopping, best iteration is:\n",
            "[281]\tvalid_0's rmse: 13.2535\tvalid_0's l2: 175.655\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005150 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9557\tvalid_0's l2: 254.585\n",
            "[400]\tvalid_0's rmse: 15.9735\tvalid_0's l2: 255.152\n",
            "Early stopping, best iteration is:\n",
            "[317]\tvalid_0's rmse: 15.9129\tvalid_0's l2: 253.221\n",
            "[I 2025-08-18 16:24:47,921] Trial 40 finished with value: 16.10715235508522 and parameters: {'objective': 'regression', 'learning_rate': 0.042413791350369524, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 160, 'subsample': 0.7520972398144102, 'colsample_bytree': 0.663375498860472, 'reg_alpha': 0.0017854540866834023, 'reg_lambda': 2.5797954438457538, 'min_split_gain': 0.010589540278482142}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004703 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 25.3816\tvalid_0's tweedie: 28.5419\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's rmse: 21.8706\tvalid_0's tweedie: 27.8556\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004806 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.1469\tvalid_0's tweedie: 20.59\n",
            "[400]\tvalid_0's rmse: 12.0559\tvalid_0's tweedie: 20.6056\n",
            "Early stopping, best iteration is:\n",
            "[234]\tvalid_0's rmse: 12.0874\tvalid_0's tweedie: 20.5744\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.041185 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4652\tvalid_0's tweedie: 22.1619\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's rmse: 12.7687\tvalid_0's tweedie: 22.1166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005167 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5852\tvalid_0's tweedie: 21.7946\n",
            "Early stopping, best iteration is:\n",
            "[115]\tvalid_0's rmse: 15.458\tvalid_0's tweedie: 21.6974\n",
            "[I 2025-08-18 16:25:24,456] Trial 41 finished with value: 15.546176920852144 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04976620386694348, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 180, 'subsample': 0.9313450208116298, 'colsample_bytree': 0.6171383068310977, 'reg_alpha': 0.05648908062486628, 'reg_lambda': 3.975820000901154, 'min_split_gain': 0.03482374567475849, 'tweedie_variance_power': 1.2204489437337391}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005542 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 26.0714\tvalid_0's tweedie: 29.677\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 21.7375\tvalid_0's tweedie: 28.7976\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004718 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5191\tvalid_0's tweedie: 21.2201\n",
            "Early stopping, best iteration is:\n",
            "[136]\tvalid_0's rmse: 12.4658\tvalid_0's tweedie: 21.2184\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005065 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5024\tvalid_0's tweedie: 22.872\n",
            "Early stopping, best iteration is:\n",
            "[100]\tvalid_0's rmse: 12.9414\tvalid_0's tweedie: 22.8821\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004844 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6508\tvalid_0's tweedie: 22.4683\n",
            "Early stopping, best iteration is:\n",
            "[116]\tvalid_0's rmse: 15.4097\tvalid_0's tweedie: 22.3408\n",
            "[I 2025-08-18 16:25:56,339] Trial 42 finished with value: 15.638598494195405 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04818942225013079, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 180, 'subsample': 0.936882118025234, 'colsample_bytree': 0.646117124195759, 'reg_alpha': 0.06335263971567895, 'reg_lambda': 1.9782671807626349, 'min_split_gain': 0.039670770405212136, 'tweedie_variance_power': 1.2154432499594399}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004883 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 25.7055\tvalid_0's tweedie: 21.2818\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 21.8625\tvalid_0's tweedie: 20.6909\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004989 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2111\tvalid_0's tweedie: 15.7619\n",
            "Early stopping, best iteration is:\n",
            "[185]\tvalid_0's rmse: 12.2113\tvalid_0's tweedie: 15.7501\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004802 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0566\tvalid_0's tweedie: 16.769\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's rmse: 12.8693\tvalid_0's tweedie: 16.8088\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.039471 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6363\tvalid_0's tweedie: 16.5584\n",
            "Early stopping, best iteration is:\n",
            "[138]\tvalid_0's rmse: 15.4548\tvalid_0's tweedie: 16.4678\n",
            "[I 2025-08-18 16:26:35,563] Trial 43 finished with value: 15.5994859521955 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04162763445340171, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 180, 'subsample': 0.9220123384773427, 'colsample_bytree': 0.6171956240827371, 'reg_alpha': 0.08529635829513922, 'reg_lambda': 1.3765433194712642, 'min_split_gain': 0.022751039427260172, 'tweedie_variance_power': 1.270024869278364}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004967 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.233\tvalid_0's poisson: -27.5781\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid_0's rmse: 22.2543\tvalid_0's poisson: -27.6135\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004845 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4906\tvalid_0's poisson: -13.909\n",
            "[400]\tvalid_0's rmse: 12.1316\tvalid_0's poisson: -14.0683\n",
            "[600]\tvalid_0's rmse: 12.0997\tvalid_0's poisson: -14.0779\n",
            "Early stopping, best iteration is:\n",
            "[527]\tvalid_0's rmse: 12.0632\tvalid_0's poisson: -14.0855\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005328 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2689\tvalid_0's poisson: -17.5136\n",
            "[400]\tvalid_0's rmse: 13.5009\tvalid_0's poisson: -17.574\n",
            "Early stopping, best iteration is:\n",
            "[255]\tvalid_0's rmse: 13.043\tvalid_0's poisson: -17.6222\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005009 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6385\tvalid_0's poisson: -16.0898\n",
            "[400]\tvalid_0's rmse: 15.6363\tvalid_0's poisson: -16.0124\n",
            "Early stopping, best iteration is:\n",
            "[228]\tvalid_0's rmse: 15.5675\tvalid_0's poisson: -16.114\n",
            "[I 2025-08-18 16:27:35,426] Trial 44 finished with value: 15.73202325094905 and parameters: {'objective': 'poisson', 'learning_rate': 0.04919900906357204, 'n_estimators': 3000, 'num_leaves': 143, 'min_child_samples': 200, 'subsample': 0.9161062966830922, 'colsample_bytree': 0.6325539813038196, 'reg_alpha': 0.08747408399052062, 'reg_lambda': 2.379803014173719, 'min_split_gain': 0.05474822491524921}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005232 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 26.1417\tvalid_0's tweedie: 15.9786\n",
            "Early stopping, best iteration is:\n",
            "[100]\tvalid_0's rmse: 22.0677\tvalid_0's tweedie: 15.3978\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005318 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.272\tvalid_0's tweedie: 12.1267\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 12.2256\tvalid_0's tweedie: 12.1195\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004961 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0171\tvalid_0's tweedie: 12.7403\n",
            "Early stopping, best iteration is:\n",
            "[129]\tvalid_0's rmse: 12.8518\tvalid_0's tweedie: 12.7699\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005042 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7911\tvalid_0's tweedie: 12.6221\n",
            "Early stopping, best iteration is:\n",
            "[125]\tvalid_0's rmse: 15.4248\tvalid_0's tweedie: 12.5548\n",
            "[I 2025-08-18 16:28:09,488] Trial 45 finished with value: 15.64244404530369 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04410971891519412, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 160, 'subsample': 0.9323046911348298, 'colsample_bytree': 0.6546988734368071, 'reg_alpha': 0.045482772811261896, 'reg_lambda': 3.4002358870523035, 'min_split_gain': 0.028197348990515358, 'tweedie_variance_power': 1.3296314049009004}. Best is trial 37 with value: 15.545001315866337.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004661 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.4309\tvalid_0's tweedie: 20.5895\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 21.7394\tvalid_0's tweedie: 20.2543\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004976 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4032\tvalid_0's tweedie: 15.468\n",
            "Early stopping, best iteration is:\n",
            "[145]\tvalid_0's rmse: 12.2468\tvalid_0's tweedie: 15.4616\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005274 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1807\tvalid_0's tweedie: 16.4551\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 12.7558\tvalid_0's tweedie: 16.479\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4739\tvalid_0's tweedie: 16.151\n",
            "[400]\tvalid_0's rmse: 15.7139\tvalid_0's tweedie: 16.291\n",
            "Early stopping, best iteration is:\n",
            "[210]\tvalid_0's rmse: 15.4237\tvalid_0's tweedie: 16.1391\n",
            "[I 2025-08-18 16:28:53,249] Trial 46 finished with value: 15.54144185143846 and parameters: {'objective': 'tweedie', 'learning_rate': 0.04006170568898008, 'n_estimators': 4000, 'num_leaves': 127, 'min_child_samples': 220, 'subsample': 0.9384454066050854, 'colsample_bytree': 0.6148537296156745, 'reg_alpha': 0.0014984332028471076, 'reg_lambda': 1.1204261307784265, 'min_split_gain': 0.07128656941236652, 'tweedie_variance_power': 1.2738958892801018}. Best is trial 46 with value: 15.54144185143846.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004993 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.0253\tvalid_0's tweedie: 21.3673\n",
            "[400]\tvalid_0's rmse: 21.798\tvalid_0's tweedie: 20.9874\n",
            "Early stopping, best iteration is:\n",
            "[358]\tvalid_0's rmse: 21.7864\tvalid_0's tweedie: 21.0083\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005484 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0575\tvalid_0's tweedie: 16.3365\n",
            "[400]\tvalid_0's rmse: 12.562\tvalid_0's tweedie: 16.0027\n",
            "[600]\tvalid_0's rmse: 12.3834\tvalid_0's tweedie: 15.9408\n",
            "[800]\tvalid_0's rmse: 12.1918\tvalid_0's tweedie: 15.9158\n",
            "[1000]\tvalid_0's rmse: 12.1597\tvalid_0's tweedie: 15.9039\n",
            "[1200]\tvalid_0's rmse: 12.0986\tvalid_0's tweedie: 15.8997\n",
            "[1400]\tvalid_0's rmse: 12.1168\tvalid_0's tweedie: 15.9097\n",
            "Early stopping, best iteration is:\n",
            "[1245]\tvalid_0's rmse: 12.0975\tvalid_0's tweedie: 15.8986\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.015854 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.5038\tvalid_0's tweedie: 17.4854\n",
            "[400]\tvalid_0's rmse: 13.0603\tvalid_0's tweedie: 17.0794\n",
            "[600]\tvalid_0's rmse: 12.9329\tvalid_0's tweedie: 16.9816\n",
            "Early stopping, best iteration is:\n",
            "[533]\tvalid_0's rmse: 12.8372\tvalid_0's tweedie: 16.9977\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004926 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.2865\tvalid_0's tweedie: 17.1367\n",
            "[400]\tvalid_0's rmse: 15.8017\tvalid_0's tweedie: 16.7395\n",
            "[600]\tvalid_0's rmse: 15.5366\tvalid_0's tweedie: 16.6803\n",
            "Early stopping, best iteration is:\n",
            "[597]\tvalid_0's rmse: 15.5328\tvalid_0's tweedie: 16.6785\n",
            "[I 2025-08-18 16:30:19,178] Trial 47 finished with value: 15.563451593846942 and parameters: {'objective': 'tweedie', 'learning_rate': 0.010788350092170766, 'n_estimators': 4000, 'num_leaves': 95, 'min_child_samples': 220, 'subsample': 0.8974436930273155, 'colsample_bytree': 0.8024299234772837, 'reg_alpha': 0.009088226377658184, 'reg_lambda': 1.1423704045982792, 'min_split_gain': 0.06302118656594613, 'tweedie_variance_power': 1.2677158845979113}. Best is trial 46 with value: 15.54144185143846.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005045 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.8844\tvalid_0's tweedie: 14.2905\n",
            "Early stopping, best iteration is:\n",
            "[111]\tvalid_0's rmse: 21.978\tvalid_0's tweedie: 13.9913\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004986 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3784\tvalid_0's tweedie: 11.138\n",
            "[400]\tvalid_0's rmse: 12.0874\tvalid_0's tweedie: 11.195\n",
            "Early stopping, best iteration is:\n",
            "[260]\tvalid_0's rmse: 12.1062\tvalid_0's tweedie: 11.1322\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005106 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6789\tvalid_0's tweedie: 11.6522\n",
            "Early stopping, best iteration is:\n",
            "[139]\tvalid_0's rmse: 12.622\tvalid_0's tweedie: 11.6905\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004804 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.514\tvalid_0's tweedie: 11.4978\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's rmse: 15.5475\tvalid_0's tweedie: 11.477\n",
            "[I 2025-08-18 16:31:06,934] Trial 48 finished with value: 15.563439751888893 and parameters: {'objective': 'tweedie', 'learning_rate': 0.039625337976760024, 'n_estimators': 4000, 'num_leaves': 143, 'min_child_samples': 220, 'subsample': 0.943940214375217, 'colsample_bytree': 0.6888532886293791, 'reg_alpha': 0.11806715902517827, 'reg_lambda': 0.6567340970064561, 'min_split_gain': 0.07534256703677669, 'tweedie_variance_power': 1.351790355093392}. Best is trial 46 with value: 15.54144185143846.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005138 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.2204\tvalid_0's poisson: -27.8301\n",
            "Early stopping, best iteration is:\n",
            "[186]\tvalid_0's rmse: 22.1685\tvalid_0's poisson: -27.8042\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004665 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4717\tvalid_0's poisson: -13.7291\n",
            "[400]\tvalid_0's rmse: 12.1738\tvalid_0's poisson: -13.9893\n",
            "[600]\tvalid_0's rmse: 12.0148\tvalid_0's poisson: -14.0614\n",
            "[800]\tvalid_0's rmse: 12.026\tvalid_0's poisson: -14.0673\n",
            "Early stopping, best iteration is:\n",
            "[649]\tvalid_0's rmse: 11.975\tvalid_0's poisson: -14.0791\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.013444 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3318\tvalid_0's poisson: -17.3213\n",
            "[400]\tvalid_0's rmse: 13.3276\tvalid_0's poisson: -17.5404\n",
            "Early stopping, best iteration is:\n",
            "[253]\tvalid_0's rmse: 13.1374\tvalid_0's poisson: -17.4647\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005236 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9296\tvalid_0's poisson: -15.9139\n",
            "[400]\tvalid_0's rmse: 15.5997\tvalid_0's poisson: -16.0967\n",
            "Early stopping, best iteration is:\n",
            "[346]\tvalid_0's rmse: 15.6109\tvalid_0's poisson: -16.1153\n",
            "[I 2025-08-18 16:32:10,670] Trial 49 finished with value: 15.722941878292238 and parameters: {'objective': 'poisson', 'learning_rate': 0.03546236786794584, 'n_estimators': 4000, 'num_leaves': 127, 'min_child_samples': 200, 'subsample': 0.9108467259795411, 'colsample_bytree': 0.6648395333157844, 'reg_alpha': 0.16457476482493572, 'reg_lambda': 1.5822444776863382, 'min_split_gain': 0.0942148012131576}. Best is trial 46 with value: 15.54144185143846.\n",
            "Best RMSE : 15.54144185143846\n",
            "Best Params: {'objective': 'tweedie', 'learning_rate': 0.04006170568898008, 'n_estimators': 4000, 'num_leaves': 127, 'min_child_samples': 220, 'subsample': 0.9384454066050854, 'colsample_bytree': 0.6148537296156745, 'reg_alpha': 0.0014984332028471076, 'reg_lambda': 1.1204261307784265, 'min_split_gain': 0.07128656941236652, 'tweedie_variance_power': 1.2738958892801018}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "BEST_PARAMS_PATH = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05_best_params.json\"\n",
        "\n",
        "best_params = study.best_params.copy()\n",
        "with open(BEST_PARAMS_PATH, \"w\") as f:\n",
        "    json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ saved best params ->\", BEST_PARAMS_PATH)\n",
        "\n",
        "import joblib\n",
        "META_PATH = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05_meta.pkl\"\n",
        "joblib.dump({\"use_cols\": use_cols, \"cat_idx\": cat_features}, META_PATH)\n",
        "print(\"✅ saved meta ->\", META_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W82m-Sf-2A_M",
        "outputId": "1a8d362c-0bb2-483a-c808-ff7797632526"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ saved best params -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_05_best_params.json\n",
            "✅ saved meta -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_05_meta.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, joblib\n",
        "from lightgbm import LGBMRegressor, log_evaluation, early_stopping\n",
        "\n",
        "BEST_PARAMS_PATH = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05_best_params.json\"\n",
        "META_PATH        = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05_meta.pkl\"\n",
        "MODEL_OUT        = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05.pkl\"\n",
        "\n",
        "# 1) 파라미터 로드\n",
        "with open(BEST_PARAMS_PATH, \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Tweedie가 아니라면 안전하게 제거\n",
        "if best_params.get(\"objective\") != \"tweedie\":\n",
        "    best_params.pop(\"tweedie_variance_power\", None)\n",
        "\n",
        "# GPU/공통 설정 부착\n",
        "USE_GPU = True\n",
        "best_params.update({\"device\": \"gpu\" if USE_GPU else \"cpu\", \"random_state\": 42, \"n_jobs\": -1})\n",
        "\n",
        "# 2) 메타 로드(use_cols, cat_idx) — 없으면 스크립트에서 재생성해도 OK\n",
        "meta = joblib.load(META_PATH)\n",
        "use_cols = meta[\"use_cols\"]\n",
        "cat_features = meta[\"cat_idx\"]  # [index]\n",
        "key_col = use_cols[cat_features[0]]\n",
        "\n",
        "# 3) 홀드아웃 + early_stopping으로 best_iteration_\n",
        "VALID_START = pd.Timestamp(\"2024-06-01\")\n",
        "VALID_END   = pd.Timestamp(\"2024-06-15\")\n",
        "MAX_LAG     = 28\n",
        "\n",
        "train_end = VALID_START - pd.Timedelta(days=MAX_LAG)\n",
        "df_tr = df[df[\"영업일자\"] <= train_end].copy()\n",
        "df_va = df[(df[\"영업일자\"] >= VALID_START) & (df[\"영업일자\"] <= VALID_END)].copy()\n",
        "\n",
        "df_tr = df_tr.dropna(subset=[\"lag_1\"])\n",
        "df_va = df_va.dropna(subset=[\"lag_1\"])\n",
        "fill_cols = list(set(use_cols) - {key_col})\n",
        "df_tr[fill_cols] = df_tr[fill_cols].fillna(0)\n",
        "df_va[fill_cols] = df_va[fill_cols].fillna(0)\n",
        "\n",
        "X_tr, y_tr = df_tr[use_cols], df_tr[\"매출수량\"].astype(float).values\n",
        "X_va, y_va = df_va[use_cols], df_va[\"매출수량\"].astype(float).values\n",
        "\n",
        "if best_params[\"objective\"] in {\"poisson\",\"tweedie\"}:\n",
        "    y_tr = np.clip(y_tr, 0, None)\n",
        "    y_va = np.clip(y_va, 0, None)\n",
        "\n",
        "finder_params = best_params.copy()\n",
        "finder_params[\"n_estimators\"] = 20000  # 크게 주고 early_stopping으로 최적 라운드 탐색\n",
        "finder = LGBMRegressor(**finder_params)\n",
        "finder.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_va, y_va)],\n",
        "    eval_metric=\"rmse\",\n",
        "    categorical_feature=cat_features,\n",
        "    callbacks=[log_evaluation(200), early_stopping(800)],\n",
        ")\n",
        "best_iter = int(finder.best_iteration_)\n",
        "print(\"best_iteration_:\", best_iter)\n",
        "\n",
        "# 4) 전체 데이터로 refit\n",
        "df_final = df.dropna(subset=[\"lag_1\"]).copy()\n",
        "df_final[fill_cols] = df_final[fill_cols].fillna(0)\n",
        "X_final = df_final[use_cols]\n",
        "y_final = df_final[\"매출수량\"].astype(float).values\n",
        "if best_params[\"objective\"] in {\"poisson\",\"tweedie\"}:\n",
        "    y_final = np.clip(y_final, 0, None)\n",
        "\n",
        "refit_params = best_params.copy()\n",
        "refit_params[\"n_estimators\"] = best_iter\n",
        "final_model = LGBMRegressor(**refit_params)\n",
        "final_model.fit(\n",
        "    X_final, y_final,\n",
        "    categorical_feature=cat_features,\n",
        ")\n",
        "\n",
        "joblib.dump({\"model\": final_model, \"features\": list(X_final.columns), \"cat_idx\": cat_features}, MODEL_OUT)\n",
        "print(\"✅ Saved ->\", MODEL_OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LSYdsXF3C-p",
        "outputId": "72fe0fce-0777-4baf-db2e-b6dbf8878dd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94377, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.008248 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391786\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[200]\tvalid_0's rmse: 15.244\tvalid_0's tweedie: 16.7256\n",
            "[400]\tvalid_0's rmse: 15.6147\tvalid_0's tweedie: 16.9915\n",
            "[600]\tvalid_0's rmse: 15.659\tvalid_0's tweedie: 17.0967\n",
            "[800]\tvalid_0's rmse: 16.0053\tvalid_0's tweedie: 17.323\n",
            "Early stopping, best iteration is:\n",
            "[167]\tvalid_0's rmse: 14.9939\tvalid_0's tweedie: 16.6204\n",
            "best_iteration_: 167\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2927\n",
            "[LightGBM] [Info] Number of data points in the train set: 102483, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (2.35 MB) transferred to GPU in 0.005843 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.364153\n",
            "✅ Saved -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_05.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lightgbm -q\n",
        "import os, glob, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# ===== 경로 설정 =====\n",
        "MODEL_PATH   = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_05.pkl\"\n",
        "TEST_DIR     = \"/content/drive/MyDrive/lg_aimers_2/data/test\"   # TEST_00.csv ~ TEST_09.csv\n",
        "SAMPLE_PATH  = \"/content/drive/MyDrive/lg_aimers_2/data/sample_submission.csv\"  # 필요 시 수정\n",
        "OUT_PATH     = \"/content/drive/MyDrive/lg_aimers_2/submission_lightgbm_05.csv\"\n",
        "\n",
        "# ===== 모델 로드 =====\n",
        "bundle = joblib.load(MODEL_PATH)\n",
        "model: LGBMRegressor = bundle[\"model\"]\n",
        "use_cols = bundle[\"features\"]\n",
        "cat_idx  = bundle[\"cat_idx\"]\n",
        "cat_col  = use_cols[cat_idx[0]]\n",
        "\n",
        "# ===== 공휴일 계산: holidays 라이브러리 사용 =====\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    # Colab 등에서 미설치 시\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"holidays\", \"-q\"], check=True)\n",
        "    import holidays\n",
        "\n",
        "# 연도별 캐시(테스트 파일마다 연도가 다를 수 있으므로 지연 생성)\n",
        "_HOL_CACHE = {}\n",
        "def is_holiday(ts: pd.Timestamp) -> bool:\n",
        "    y = int(ts.year)\n",
        "    if y not in _HOL_CACHE:\n",
        "        try:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y], language=\"ko\")\n",
        "        except Exception:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y])\n",
        "    return ts.date() in _HOL_CACHE[y]\n",
        "\n",
        "# ===== 출시 정보 (전처리와 동일 규칙 사용: dict 기반) =====\n",
        "launch_dates = {\n",
        "    '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "    '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "    '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "    '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "    '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "    '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "    '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "    '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "    '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "    '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "    '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "    '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "    '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "    '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "    '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "    '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "    '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "    '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "    '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "    '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "    '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "    '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "    '라그로타_해산물 토마토 스튜 파스타': '2023-12-08',\n",
        "    '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "    '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "    '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "    '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "    '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "    '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "    '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "    '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "    '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "    '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "    '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "    '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "    '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "    '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "    '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "    '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "    '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "    '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "    '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "    '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "    '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "    '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "    '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "    '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "    '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "    '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "    '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "    '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "    '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "}\n",
        "launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "def add_future_meta_row(date, key):\n",
        "    \"\"\"모델 입력 피처(학습 시 사용한 use_cols 기준)와 일치하도록 미래 1행 메타 피처 생성\"\"\"\n",
        "    row = pd.DataFrame({\"영업일자\":[pd.Timestamp(date)], \"영업장명_메뉴명\":[key]})\n",
        "    d = row.loc[0, \"영업일자\"]\n",
        "\n",
        "    # 기본 달력\n",
        "    row[\"년\"] = d.year\n",
        "    row[\"월\"] = d.month\n",
        "    row[\"일\"] = d.day\n",
        "    row[\"요일\"] = d.dayofweek\n",
        "    row[\"주말여부\"] = int(row.loc[0,\"요일\"] in [5,6])\n",
        "\n",
        "    # 공휴일 & 휴무일\n",
        "    hol = is_holiday(d)\n",
        "    row[\"공휴일여부\"] = int(hol)\n",
        "    row[\"휴무일여부\"] = int(hol or bool(row.loc[0,\"주말여부\"]))\n",
        "\n",
        "    # 사이클릭\n",
        "    row[\"요일_sin\"] = np.sin(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"요일_cos\"] = np.cos(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"월_sin\"]   = np.sin(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "    row[\"월_cos\"]   = np.cos(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "\n",
        "    # 계절\n",
        "    m = row.loc[0,\"월\"]\n",
        "    if m in [12,1,2]:\n",
        "        season = 0\n",
        "    elif m in [3,4,5]:\n",
        "        season = 1\n",
        "    elif m in [6,7,8]:\n",
        "        season = 2\n",
        "    else:\n",
        "        season = 3\n",
        "    row[\"계절(겨울0봄1여름2가을3)\"] = np.int8(season)\n",
        "\n",
        "    # 출시 파생\n",
        "    row[\"신규메뉴여부\"] = int(key in launch_dates)\n",
        "    if key in launch_dates and d >= launch_dates[key]:\n",
        "        row[\"출시일로부터경과일\"] = int((d - launch_dates[key]).days)\n",
        "    else:\n",
        "        row[\"출시일로부터경과일\"] = 0\n",
        "\n",
        "    # 출시 후 주차 & 더미 (학습 시 사용했다면 동일하게)\n",
        "    row[\"출시후_주차\"] = (row[\"출시일로부터경과일\"] // 7)\n",
        "    row[\"출시_0주\"]   = (row[\"출시후_주차\"] == 0).astype(int)\n",
        "    row[\"출시_1_2주\"] = row[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    row[\"출시_3_4주\"] = row[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    row[\"출시_5주이상\"] = (row[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "    return row\n",
        "\n",
        "def predict_group_autoreg(g: pd.DataFrame):\n",
        "    key = g[cat_col].iloc[0]\n",
        "    g = g.sort_values(\"영업일자\").copy()\n",
        "    g = g.dropna(subset=[\"매출수량\"])\n",
        "    assert len(g) >= 28, f\"{key}: 28일 히스토리 부족\"\n",
        "\n",
        "    hist_vals = g[\"매출수량\"].values.tolist()[-28:]\n",
        "    hist_dates = g[\"영업일자\"].tolist()[-28:]\n",
        "    last_date = g[\"영업일자\"].max()\n",
        "    preds = []\n",
        "\n",
        "    for h in range(1, 8):\n",
        "        cur_date = last_date + pd.Timedelta(days=h)\n",
        "        row = add_future_meta_row(cur_date, key)\n",
        "\n",
        "        # lags\n",
        "        def lag(n): return hist_vals[-n] if len(hist_vals) >= n else np.nan\n",
        "        row[\"lag_1\"], row[\"lag_7\"], row[\"lag_14\"], row[\"lag_28\"] = lag(1), lag(7), lag(14), lag(28)\n",
        "\n",
        "        # rolling (과거값만)\n",
        "        def rmean(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.mean(arr)) if arr else 0.0\n",
        "        def rsum(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.sum(arr)) if arr else 0.0\n",
        "        row[\"roll7_mean\"], row[\"roll7_sum\"], row[\"roll14_mean\"], row[\"roll28_mean\"] = \\\n",
        "            rmean(7), rsum(7), rmean(14), rmean(28)\n",
        "\n",
        "        # 같은 요일 평균(최근 28일 범위)\n",
        "        cur_dow = cur_date.dayofweek\n",
        "        hist_dows = [pd.Timestamp(d).dayofweek for d in hist_dates]\n",
        "        same_idx = [i for i in range(len(hist_vals)) if hist_dows[i] == cur_dow]\n",
        "        row[\"same_dow_mean_28\"] = float(np.mean([hist_vals[i] for i in same_idx])) if same_idx else 0.0\n",
        "\n",
        "        # 모델 입력 정렬\n",
        "        X = row.reindex(columns=use_cols).copy()\n",
        "        X[cat_col] = X[cat_col].astype(\"category\")\n",
        "        for c in X.columns:\n",
        "            if c != cat_col:\n",
        "                X[c] = X[c].fillna(0)\n",
        "\n",
        "        yhat = float(model.predict(X, num_iteration=getattr(model, \"best_iteration_\", None))[0])\n",
        "        yhat = max(0.0, yhat)  # 음수 방지\n",
        "        preds.append(yhat)\n",
        "\n",
        "        # autoreg 업데이트\n",
        "        hist_vals.append(yhat); hist_dates.append(cur_date)\n",
        "        if len(hist_vals) > 28:\n",
        "            hist_vals, hist_dates = hist_vals[-28:], hist_dates[-28:]\n",
        "\n",
        "    return preds  # 길이 7\n",
        "\n",
        "# ===== 샘플 제출 파일 불러오기 =====\n",
        "sub = pd.read_csv(SAMPLE_PATH)\n",
        "menu_cols = [c for c in sub.columns if c != \"영업일자\"]\n",
        "sub[menu_cols] = sub[menu_cols].astype(float)\n",
        "\n",
        "# ===== TEST 파일 순회 =====\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "print(\"Found:\", test_files)\n",
        "\n",
        "for f in test_files:\n",
        "    test_name = os.path.splitext(os.path.basename(f))[0]  # TEST_00\n",
        "    test_id = test_name.split(\"_\")[1]                     # 00\n",
        "    df = pd.read_csv(f, parse_dates=[\"영업일자\"])\n",
        "\n",
        "    # 타입/정렬\n",
        "    if cat_col in df.columns:\n",
        "        df[cat_col] = df[cat_col].astype(\"category\")\n",
        "    df = df.sort_values([cat_col, \"영업일자\"]).copy()\n",
        "\n",
        "    # 메뉴 단위 예측\n",
        "    for key, g in df.groupby(cat_col, observed=True):\n",
        "        preds7 = predict_group_autoreg(g)\n",
        "\n",
        "        # 제출 파일에 해당 메뉴 열이 없으면 skip\n",
        "        if key not in menu_cols:\n",
        "            continue\n",
        "\n",
        "        # +1~+7일 채우기\n",
        "        for k in range(1, 8):\n",
        "            ridx = sub.index[sub[\"영업일자\"] == f\"{test_name}+{k}일\"]\n",
        "            if len(ridx) == 1:\n",
        "                sub.loc[ridx[0], key] = preds7[k-1]\n",
        "\n",
        "# (선택) 후처리\n",
        "# sub[menu_cols] = sub[menu_cols].clip(lower=0).round(4)\n",
        "\n",
        "# ===== 저장 =====\n",
        "sub.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved submission ->\", OUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfnVI1zpFFOd",
        "outputId": "33f50cdf-f866-4924-bfba-9b3ceb0ede9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: ['/content/drive/MyDrive/lg_aimers_2/data/test/TEST_00.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_01.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_02.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_03.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_04.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_05.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_06.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_07.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_08.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_09.csv']\n",
            "Saved submission -> /content/drive/MyDrive/lg_aimers_2/submission_lightgbm_05.csv\n"
          ]
        }
      ]
    }
  ]
}