{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f2c774a2fc84f9bb28264f072b3f63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b4a84a258d149c5b3f5c3f636976e77",
              "IPY_MODEL_c4896b7b260b4268acb1d8d0d85bd629",
              "IPY_MODEL_8e7c45c2df1a47068a2fc12dd3dc8fab"
            ],
            "layout": "IPY_MODEL_7ac3008c9d7848548717629108d0f631"
          }
        },
        "0b4a84a258d149c5b3f5c3f636976e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3390860053f4f858f9b70bff333bf5c",
            "placeholder": "​",
            "style": "IPY_MODEL_35d352df4b724e50aed0d16be3557565",
            "value": "Best trial: 45. Best value: 15.4939: 100%"
          }
        },
        "c4896b7b260b4268acb1d8d0d85bd629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe263f2ae6ba4c8880a7d50c7628a675",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c91d4ca24e742a58872bf3d28e40d71",
            "value": 50
          }
        },
        "8e7c45c2df1a47068a2fc12dd3dc8fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd1720c1b24048ebb7cb679bd96c6c3c",
            "placeholder": "​",
            "style": "IPY_MODEL_232e21eeead1446e9d99c910aa0e3b82",
            "value": " 50/50 [42:14&lt;00:00, 35.40s/it]"
          }
        },
        "7ac3008c9d7848548717629108d0f631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3390860053f4f858f9b70bff333bf5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d352df4b724e50aed0d16be3557565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe263f2ae6ba4c8880a7d50c7628a675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c91d4ca24e742a58872bf3d28e40d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd1720c1b24048ebb7cb679bd96c6c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "232e21eeead1446e9d99c910aa0e3b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 가능 여부\n",
        "print(torch.cuda.is_available())  # True면 GPU 사용 가능\n",
        "print(torch.cuda.get_device_name(0))  # GPU 이름 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVriMxe9_Mvc",
        "outputId": "42ab48b8-0f99-4fcd-d79a-04d8fe6a0390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab 환경을 위한 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltSlwie6Fvg",
        "outputId": "155c04ab-f12f-47df-f6b2-84a04c375325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-Cell Clean Preprocessing (공휴일여부 + 휴무일여부) ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/lg_aimers_2/data/train/train.csv'\n",
        "OUT_PATH   = '/content/drive/MyDrive/lg_aimers_2/train_preprocessed_04.csv'\n",
        "\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    !pip install holidays -q\n",
        "    import holidays\n",
        "\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"train.csv 전처리 + (공휴일/휴무일) + 계절/사이클릭 + 출시일 파생\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "     # === 음수 매출수량 보정 ===\n",
        "    if \"매출수량\" in df.columns:\n",
        "        df[\"매출수량\"] = df[\"매출수량\"].clip(lower=0)\n",
        "\n",
        "    df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    df['년'] = df['영업일자'].dt.year\n",
        "    df['월'] = df['영업일자'].dt.month\n",
        "    df['일'] = df['영업일자'].dt.day\n",
        "    df['요일'] = df['영업일자'].dt.dayofweek\n",
        "    df['주말여부'] = df['요일'].isin([5, 6])\n",
        "\n",
        "    # 메뉴 분리\n",
        "    df[['영업장명', '메뉴명']] = df['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "\n",
        "    # === 캘린더 피처 ===\n",
        "    def add_calendar_features(frame: pd.DataFrame, date_col=\"영업일자\") -> pd.DataFrame:\n",
        "        out = frame.copy()\n",
        "        d = pd.to_datetime(out[date_col])\n",
        "        years = sorted(d.dt.year.unique())\n",
        "        try:\n",
        "            KR_HOL = holidays.KR(years=years, language=\"ko\")\n",
        "        except Exception:\n",
        "            KR_HOL = holidays.KR(years=years)\n",
        "\n",
        "        # 공휴일 여부만\n",
        "        out[\"공휴일여부\"] = d.dt.date.map(lambda x: x in KR_HOL)\n",
        "\n",
        "        # 계절\n",
        "        m = d.dt.month\n",
        "        out[\"계절(겨울0봄1여름2가을3)\"] = (\n",
        "            (m.isin([12,1,2]))*0 +\n",
        "            (m.isin([3,4,5]))*1 +\n",
        "            (m.isin([6,7,8]))*2 +\n",
        "            (m.isin([9,10,11]))*3\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "        # 사이클릭\n",
        "        out[\"요일_sin\"] = np.sin(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"요일_cos\"] = np.cos(2*np.pi*out[\"요일\"]/7)\n",
        "        out[\"월_sin\"]   = np.sin(2*np.pi*(out[\"월\"]-1)/12)\n",
        "        out[\"월_cos\"]   = np.cos(2*np.pi*(out[\"월\"]-1)/12)\n",
        "\n",
        "        return out\n",
        "\n",
        "    df = add_calendar_features(df, date_col='영업일자')\n",
        "\n",
        "    # === 추가: 휴무일여부 (주말 OR 공휴일) ===\n",
        "    df[\"휴무일여부\"] = df[\"주말여부\"] | df[\"공휴일여부\"]\n",
        "\n",
        "    # === 출시일 파생 ===\n",
        "    launch_dates = {\n",
        "        '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "        '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "        '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "        '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "        '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "        '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "        '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "        '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "        '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "        '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "        '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "        '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "        '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "        '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "        '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "        '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "        '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "        '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "        '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "        '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "        '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "        '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "        '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "        '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "        '라그로타_해산물 토마토 스튜 파스타': '2023-12-08', '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "        '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "        '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "        '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "        '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "        '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "        '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "        '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "        '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "        '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "        '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "        '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "        '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "        '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "        '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "        '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "        '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "        '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "        '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "        '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "        '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "        '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "        '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "        '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "        '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "        '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "        '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "        '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "        '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "    }\n",
        "    launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "    def calculate_days_since_launch(row):\n",
        "        menu = row['영업장명_메뉴명']\n",
        "        if menu in launch_dates:\n",
        "            launch_date = launch_dates[menu]\n",
        "            if row['영업일자'] >= launch_date:\n",
        "                return (row['영업일자'] - launch_date).days\n",
        "        return 0\n",
        "\n",
        "    df['출시일로부터경과일'] = df.apply(calculate_days_since_launch, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 실행\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "train_preprocessed = preprocess_data(train_df)\n",
        "train_preprocessed.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(\"전처리 완료 →\", OUT_PATH)\n",
        "\n",
        "# 점검\n",
        "print(\"공휴일여부 분포:\", train_preprocessed[\"공휴일여부\"].value_counts(dropna=False).to_dict())\n",
        "print(\"휴무일여부 분포:\", train_preprocessed[\"휴무일여부\"].value_counts(dropna=False).to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5qtDmEV0vXk",
        "outputId": "59a86184-c165-41c7-c508-0f9621a3da90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 완료 → /content/drive/MyDrive/lg_aimers_2/train_preprocessed_04.csv\n",
            "공휴일여부 분포: {False: 97079, True: 5597}\n",
            "휴무일여부 분포: {False: 69287, True: 33389}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Colab 셋업 =====\n",
        "!pip -q install optuna lightgbm\n",
        "\n",
        "import os, joblib, optuna, numpy as np, pandas as pd\n",
        "from lightgbm import LGBMRegressor, log_evaluation, early_stopping\n",
        "\n",
        "# -----------------------------\n",
        "# 0) 경로/상수\n",
        "# -----------------------------\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/lg_aimers_2/train_preprocessed_04.csv\"\n",
        "MODEL_OUT  = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_04_optuna.pkl\"\n",
        "USE_GPU    = True\n",
        "DEVICE     = {\"device\": \"gpu\"} if USE_GPU else {\"device\": \"cpu\"}\n",
        "MAX_LAG    = 28  # 너의 lag 최댓값과 동일해야 함\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 데이터 로드 & 기본 세팅\n",
        "# -----------------------------\n",
        "df = pd.read_csv(TRAIN_PATH, parse_dates=[\"영업일자\"])\n",
        "\n",
        "# 음수 매출 안전 처리(대회 데이터 이슈 대응)\n",
        "df[\"매출수량\"] = pd.to_numeric(df[\"매출수량\"], errors=\"coerce\").fillna(0)\n",
        "df[\"매출수량\"] = df[\"매출수량\"].clip(lower=0)\n",
        "\n",
        "# bool -> int\n",
        "for c in [c for c in [\"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\"] if c in df.columns]:\n",
        "    df[c] = df[c].astype(int)\n",
        "\n",
        "# key 컬럼\n",
        "key_col = \"영업장명_메뉴명\"\n",
        "df[key_col] = df[key_col].astype(\"category\")\n",
        "\n",
        "# 시간 정렬\n",
        "df = df.sort_values([key_col, \"영업일자\"]).copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) lag/rolling/same_dow 생성 (네 기존 규칙 유지)\n",
        "# -----------------------------\n",
        "lag_list = [1, 7, 14, 28]\n",
        "for lag in lag_list:\n",
        "    df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
        "\n",
        "def add_rolling(g, window, how=\"mean\"):\n",
        "    s = g[\"매출수량\"].shift(1)  # 과거만\n",
        "    if how == \"mean\":\n",
        "        return s.rolling(window, min_periods=1).mean()\n",
        "    elif how == \"sum\":\n",
        "        return s.rolling(window, min_periods=1).sum()\n",
        "\n",
        "df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
        "df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
        "df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
        "df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
        "\n",
        "def same_dow_mean_28(g):\n",
        "    out = np.full(len(g), np.nan, dtype=float)\n",
        "    vals = g[\"매출수량\"].shift(1)\n",
        "    dows = g[\"요일\"]\n",
        "    for i in range(len(g)):\n",
        "        lo = max(0, i-28)\n",
        "        same_idx = [j for j in range(lo, i) if dows.iloc[j] == dows.iloc[i]]\n",
        "        if same_idx:\n",
        "            out[i] = vals.iloc[same_idx].mean()\n",
        "    return pd.Series(out, index=g.index)\n",
        "\n",
        "if \"요일\" in df.columns:\n",
        "    df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
        "\n",
        "# 신규메뉴 파생(있을 때만)\n",
        "if \"출시일로부터경과일\" in df.columns:\n",
        "    df[\"출시후_주차\"]   = (df[\"출시일로부터경과일\"] // 7).clip(lower=0).astype(int)\n",
        "    df[\"출시_0주\"]     = (df[\"출시후_주차\"] == 0).astype(int)\n",
        "    df[\"출시_1_2주\"]   = df[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    df[\"출시_3_4주\"]   = df[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    df[\"출시_5주이상\"] = (df[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 학습에 쓸 피처 목록 (네 기준 유지 + 휴무일여부 포함)\n",
        "# -----------------------------\n",
        "base_feats = [c for c in [\n",
        "    \"년\",\"월\",\"일\",\"요일\",\"주말여부\",\"공휴일여부\",\"휴무일여부\",\"신규메뉴여부\",\n",
        "    \"출시일로부터경과일\",\"출시후_주차\",\"출시_0주\",\"출시_1_2주\",\"출시_3_4주\",\"출시_5주이상\",\n",
        "    \"요일_sin\",\"요일_cos\",\"월_sin\",\"월_cos\",\"계절(겨울0봄1여름2가을3)\"\n",
        "] if c in df.columns]\n",
        "\n",
        "lag_feats  = [f\"lag_{l}\" for l in lag_list]\n",
        "roll_feats = [c for c in [\"roll7_mean\",\"roll7_sum\",\"roll14_mean\",\"roll28_mean\",\"same_dow_mean_28\"] if c in df.columns]\n",
        "\n",
        "use_cols = base_feats + lag_feats + roll_feats + [key_col]\n",
        "cat_features = [use_cols.index(key_col)]  # LightGBM용 범주형 인덱스\n",
        "\n",
        "# -----------------------------\n",
        "# 4) 폴드 정의(롤링) + gap purge\n",
        "#    - 아래는 예시: 14일 검증창을 여러 번 슬라이딩\n",
        "#    - 네 데이터의 마지막: 2024-06-15 기준으로 몇 개 컷 생성\n",
        "# -----------------------------\n",
        "def build_folds(df, val_window_days=14, n_folds=4, last_val_end=\"2024-06-15\"):\n",
        "    last_val_end = pd.Timestamp(last_val_end)\n",
        "    folds = []\n",
        "    for k in range(n_folds):\n",
        "        val_end   = last_val_end - pd.Timedelta(days=(n_folds-1-k)*val_window_days)\n",
        "        val_start = val_end - pd.Timedelta(days=val_window_days-1)\n",
        "        # train은 val_start - MAX_LAG 까지만 사용 (누수 방지)\n",
        "        train_end = val_start - pd.Timedelta(days=MAX_LAG)\n",
        "        folds.append((train_end, val_start, val_end))\n",
        "    return folds\n",
        "\n",
        "folds = build_folds(df, val_window_days=14, n_folds=4, last_val_end=\"2024-06-15\")\n",
        "print(\"Folds:\")\n",
        "for i,(te,vs,ve) in enumerate(folds,1):\n",
        "    print(f\"Fold{i}: train_end={te.date()} | val={vs.date()}~{ve.date()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) 폴드 데이터 구성 함수\n",
        "# -----------------------------\n",
        "def make_fold_data(df, use_cols, key_col, tr_end, va_st, va_en):\n",
        "    tr = df[df[\"영업일자\"] <= tr_end].copy()\n",
        "    va = df[(df[\"영업일자\"] >= va_st) & (df[\"영업일자\"] <= va_en)].copy()\n",
        "\n",
        "    # 최소 lag 확보(가장 작은 lag만 체크)\n",
        "    tr = tr.dropna(subset=[\"lag_1\"])\n",
        "    va = va.dropna(subset=[\"lag_1\"])\n",
        "\n",
        "    # NA 대체\n",
        "    fill_cols = list(set(use_cols) - {key_col})\n",
        "    tr[fill_cols] = tr[fill_cols].fillna(0)\n",
        "    va[fill_cols] = va[fill_cols].fillna(0)\n",
        "\n",
        "    X_tr, y_tr = tr[use_cols], tr[\"매출수량\"].astype(float).values\n",
        "    X_va, y_va = va[use_cols], va[\"매출수량\"].astype(float).values\n",
        "    return X_tr, y_tr, X_va, y_va\n",
        "\n",
        "# -----------------------------\n",
        "# 6) 폴드 평균 RMSE 계산\n",
        "# -----------------------------\n",
        "def cv_rmse(params, df, use_cols, key_col, cat_features, nonneg_target=False):\n",
        "    rmses = []\n",
        "    for (tr_end, va_st, va_en) in folds:\n",
        "        # gap purge: train_end를 val_start - MAX_LAG 로 강제\n",
        "        te = va_st - pd.Timedelta(days=MAX_LAG)\n",
        "        X_tr, y_tr, X_va, y_va = make_fold_data(df, use_cols, key_col, te, va_st, va_en)\n",
        "\n",
        "        # poisson/tweedie 평가를 원하면 비음수 타깃 사용\n",
        "        if nonneg_target:\n",
        "            y_tr = np.clip(y_tr, 0, None)\n",
        "            y_va = np.clip(y_va, 0, None)\n",
        "\n",
        "        model = LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"rmse\",\n",
        "            categorical_feature=cat_features,\n",
        "            callbacks=[log_evaluation(200), early_stopping(200)],\n",
        "        )\n",
        "        pred = model.predict(X_va, num_iteration=getattr(model, \"best_iteration_\", None))\n",
        "        rmse = float(np.sqrt(((y_va - pred)**2).mean()))\n",
        "        rmses.append(rmse)\n",
        "    return float(np.mean(rmses))\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Optuna 목적함수 (시계열 CV + gap)\n",
        "# -----------------------------\n",
        "def objective(trial):\n",
        "    obj = trial.suggest_categorical(\"objective\", [\"regression\",\"poisson\",\"tweedie\"])\n",
        "    params = dict(\n",
        "        objective=obj,\n",
        "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True),\n",
        "        n_estimators=trial.suggest_int(\"n_estimators\", 3000, 9000, step=1000),\n",
        "        num_leaves=trial.suggest_int(\"num_leaves\", 63, 191, step=16),\n",
        "        min_child_samples=trial.suggest_int(\"min_child_samples\", 80, 220, step=20),\n",
        "        subsample=trial.suggest_float(\"subsample\", 0.75, 0.95),\n",
        "        subsample_freq=1,\n",
        "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
        "        reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 0.6),\n",
        "        reg_lambda=trial.suggest_float(\"reg_lambda\", 0.5, 4.0),\n",
        "        min_split_gain=trial.suggest_float(\"min_split_gain\", 0.0, 0.2),\n",
        "        random_state=42, n_jobs=-1, **DEVICE\n",
        "    )\n",
        "    if obj == \"tweedie\":\n",
        "        params[\"tweedie_variance_power\"] = trial.suggest_float(\"tweedie_variance_power\", 1.1, 1.6)\n",
        "\n",
        "    # poisson/tweedie는 비음수 타깃으로 CV\n",
        "    nonneg = (obj in {\"poisson\",\"tweedie\"})\n",
        "    score = cv_rmse(params, df, use_cols, key_col, cat_features, nonneg_target=nonneg)\n",
        "    return score\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best RMSE :\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) 최적 파라미터로 최종 학습(전체 train 기간 사용)\n",
        "#    - 최종 홀드아웃을 따로 둘 수도 있지만, 여기선 전체 사용 예시\n",
        "# -----------------------------\n",
        "best_params = study.best_params.copy()\n",
        "if best_params.get(\"objective\") != \"tweedie\":\n",
        "    best_params.pop(\"tweedie_variance_power\", None)\n",
        "best_params.update(DEVICE)\n",
        "best_params.update({\"random_state\":42, \"n_jobs\":-1})\n",
        "\n",
        "# 최종 학습용 데이터(가장 최근 28일 lag가 존재하도록 dropna)\n",
        "df_final = df.dropna(subset=[\"lag_1\"]).copy()\n",
        "fill_cols = list(set(use_cols) - {key_col})\n",
        "df_final[fill_cols] = df_final[fill_cols].fillna(0)\n",
        "\n",
        "X_final = df_final[use_cols]\n",
        "y_final = df_final[\"매출수량\"].astype(float).values\n",
        "if best_params[\"objective\"] in {\"poisson\",\"tweedie\"}:\n",
        "    y_final = np.clip(y_final, 0, None)\n",
        "\n",
        "final_model = LGBMRegressor(**best_params)\n",
        "final_model.fit(\n",
        "    X_final, y_final,\n",
        "    eval_set=[(X_final, y_final)],  # 모니터링만; 조기종료 필요 없으면 콜백 제거 가능\n",
        "    eval_metric=\"rmse\",\n",
        "    categorical_feature=cat_features,\n",
        "    callbacks=[log_evaluation(500)],\n",
        ")\n",
        "\n",
        "os.makedirs(os.path.dirname(MODEL_OUT), exist_ok=True)\n",
        "joblib.dump({\"model\": final_model, \"features\": list(X_final.columns), \"cat_idx\": cat_features}, MODEL_OUT)\n",
        "print(\"✅ Saved ->\", MODEL_OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6f2c774a2fc84f9bb28264f072b3f63d",
            "0b4a84a258d149c5b3f5c3f636976e77",
            "c4896b7b260b4268acb1d8d0d85bd629",
            "8e7c45c2df1a47068a2fc12dd3dc8fab",
            "7ac3008c9d7848548717629108d0f631",
            "a3390860053f4f858f9b70bff333bf5c",
            "35d352df4b724e50aed0d16be3557565",
            "fe263f2ae6ba4c8880a7d50c7628a675",
            "4c91d4ca24e742a58872bf3d28e40d71",
            "bd1720c1b24048ebb7cb679bd96c6c3c",
            "232e21eeead1446e9d99c910aa0e3b82"
          ]
        },
        "id": "cerX1PYIjcGQ",
        "outputId": "f811988e-2a94-4be1-e103-d7a990cb0ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/247.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1400879358.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-1400879358.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-1400879358.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-1400879358.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[f\"lag_{lag}\"] = df.groupby(key_col)[\"매출수량\"].shift(lag)\n",
            "/tmp/ipython-input-1400879358.py:50: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_mean\"]  = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:51: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
            "/tmp/ipython-input-1400879358.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll7_sum\"]   = df.groupby(key_col, group_keys=False).apply(add_rolling, window=7,  how=\"sum\")\n",
            "/tmp/ipython-input-1400879358.py:52: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll14_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=14, how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:53: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"roll28_mean\"] = df.groupby(key_col, group_keys=False).apply(add_rolling, window=28, how=\"mean\")\n",
            "/tmp/ipython-input-1400879358.py:67: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
            "/tmp/ipython-input-1400879358.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df[\"same_dow_mean_28\"] = df.groupby(key_col, group_keys=False).apply(same_dow_mean_28)\n",
            "[I 2025-08-18 14:24:12,105] A new study created in memory with name: no-name-85ce0530-2cb9-4641-84f2-810b265720d8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds:\n",
            "Fold1: train_end=2024-03-24 | val=2024-04-21~2024-05-04\n",
            "Fold2: train_end=2024-04-07 | val=2024-05-05~2024-05-18\n",
            "Fold3: train_end=2024-04-21 | val=2024-05-19~2024-06-01\n",
            "Fold4: train_end=2024-05-05 | val=2024-06-02~2024-06-15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f2c774a2fc84f9bb28264f072b3f63d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004669 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5426\tvalid_0's poisson: -27.568\n",
            "[400]\tvalid_0's rmse: 23.1596\tvalid_0's poisson: -27.6732\n",
            "Early stopping, best iteration is:\n",
            "[273]\tvalid_0's rmse: 22.3114\tvalid_0's poisson: -27.8289\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004477 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7478\tvalid_0's poisson: -13.4241\n",
            "[400]\tvalid_0's rmse: 12.5719\tvalid_0's poisson: -13.8114\n",
            "[600]\tvalid_0's rmse: 12.533\tvalid_0's poisson: -13.8806\n",
            "[800]\tvalid_0's rmse: 12.4808\tvalid_0's poisson: -13.912\n",
            "[1000]\tvalid_0's rmse: 12.4433\tvalid_0's poisson: -13.9293\n",
            "[1200]\tvalid_0's rmse: 12.4202\tvalid_0's poisson: -13.9414\n",
            "[1400]\tvalid_0's rmse: 12.4517\tvalid_0's poisson: -13.9285\n",
            "Early stopping, best iteration is:\n",
            "[1239]\tvalid_0's rmse: 12.4067\tvalid_0's poisson: -13.947\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004872 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.6127\tvalid_0's poisson: -17.0366\n",
            "[400]\tvalid_0's rmse: 13.2765\tvalid_0's poisson: -17.4825\n",
            "[600]\tvalid_0's rmse: 13.478\tvalid_0's poisson: -17.5334\n",
            "Early stopping, best iteration is:\n",
            "[453]\tvalid_0's rmse: 13.2266\tvalid_0's poisson: -17.5273\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004659 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9199\tvalid_0's poisson: -15.6371\n",
            "[400]\tvalid_0's rmse: 15.6311\tvalid_0's poisson: -16.072\n",
            "[600]\tvalid_0's rmse: 15.5019\tvalid_0's poisson: -16.0954\n",
            "Early stopping, best iteration is:\n",
            "[577]\tvalid_0's rmse: 15.4812\tvalid_0's poisson: -16.1134\n",
            "[I 2025-08-18 14:25:22,080] Trial 0 finished with value: 15.856484999945911 and parameters: {'objective': 'poisson', 'learning_rate': 0.026208630215377525, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 80, 'subsample': 0.923235229154987, 'colsample_bytree': 0.7803345035229626, 'reg_alpha': 0.4248435466776273, 'reg_lambda': 0.5720457300353086, 'min_split_gain': 0.19398197043239887}. Best is trial 0 with value: 15.856484999945911.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004567 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3029\tvalid_0's l2: 497.418\n",
            "[400]\tvalid_0's rmse: 22.0308\tvalid_0's l2: 485.354\n",
            "[600]\tvalid_0's rmse: 22.0018\tvalid_0's l2: 484.078\n",
            "[800]\tvalid_0's rmse: 22.0142\tvalid_0's l2: 484.624\n",
            "Early stopping, best iteration is:\n",
            "[684]\tvalid_0's rmse: 21.961\tvalid_0's l2: 482.286\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.011782 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.04\tvalid_0's l2: 170.042\n",
            "[400]\tvalid_0's rmse: 12.8383\tvalid_0's l2: 164.823\n",
            "Early stopping, best iteration is:\n",
            "[388]\tvalid_0's rmse: 12.8331\tvalid_0's l2: 164.688\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004655 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.8935\tvalid_0's l2: 193.028\n",
            "[400]\tvalid_0's rmse: 13.2879\tvalid_0's l2: 176.569\n",
            "[600]\tvalid_0's rmse: 13.1993\tvalid_0's l2: 174.222\n",
            "[800]\tvalid_0's rmse: 13.1717\tvalid_0's l2: 173.493\n",
            "Early stopping, best iteration is:\n",
            "[678]\tvalid_0's rmse: 13.1577\tvalid_0's l2: 173.125\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.008284 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.0327\tvalid_0's l2: 257.049\n",
            "[400]\tvalid_0's rmse: 15.9595\tvalid_0's l2: 254.707\n",
            "[600]\tvalid_0's rmse: 15.9595\tvalid_0's l2: 254.705\n",
            "Early stopping, best iteration is:\n",
            "[493]\tvalid_0's rmse: 15.9397\tvalid_0's l2: 254.075\n",
            "[I 2025-08-18 14:26:49,921] Trial 1 finished with value: 15.972883618339855 and parameters: {'objective': 'regression', 'learning_rate': 0.013433656868034296, 'n_estimators': 5000, 'num_leaves': 127, 'min_child_samples': 140, 'subsample': 0.8082458280396083, 'colsample_bytree': 0.7835558684167139, 'reg_alpha': 0.0836963163912251, 'reg_lambda': 1.5225062698732637, 'min_split_gain': 0.07327236865873835}. Best is trial 0 with value: 15.856484999945911.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.014646 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3523\tvalid_0's poisson: -27.3557\n",
            "[400]\tvalid_0's rmse: 22.3562\tvalid_0's poisson: -27.8293\n",
            "Early stopping, best iteration is:\n",
            "[303]\tvalid_0's rmse: 21.9653\tvalid_0's poisson: -27.8177\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.035247 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9214\tvalid_0's poisson: -13.3144\n",
            "[400]\tvalid_0's rmse: 12.5909\tvalid_0's poisson: -13.8262\n",
            "[600]\tvalid_0's rmse: 12.426\tvalid_0's poisson: -13.9522\n",
            "[800]\tvalid_0's rmse: 12.3448\tvalid_0's poisson: -14.0018\n",
            "[1000]\tvalid_0's rmse: 12.2273\tvalid_0's poisson: -14.0428\n",
            "[1200]\tvalid_0's rmse: 12.1869\tvalid_0's poisson: -14.0756\n",
            "[1400]\tvalid_0's rmse: 12.1587\tvalid_0's poisson: -14.0944\n",
            "Early stopping, best iteration is:\n",
            "[1367]\tvalid_0's rmse: 12.1434\tvalid_0's poisson: -14.0967\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.010592 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.9681\tvalid_0's poisson: -16.8071\n",
            "[400]\tvalid_0's rmse: 13.2565\tvalid_0's poisson: -17.3991\n",
            "[600]\tvalid_0's rmse: 13.0862\tvalid_0's poisson: -17.5579\n",
            "Early stopping, best iteration is:\n",
            "[530]\tvalid_0's rmse: 13.0577\tvalid_0's poisson: -17.5276\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004743 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9641\tvalid_0's poisson: -15.4591\n",
            "[400]\tvalid_0's rmse: 15.5646\tvalid_0's poisson: -16.0868\n",
            "[600]\tvalid_0's rmse: 15.394\tvalid_0's poisson: -16.1996\n",
            "[800]\tvalid_0's rmse: 15.4183\tvalid_0's poisson: -16.1839\n",
            "Early stopping, best iteration is:\n",
            "[687]\tvalid_0's rmse: 15.3568\tvalid_0's poisson: -16.2142\n",
            "[I 2025-08-18 14:27:50,556] Trial 2 finished with value: 15.630776991726515 and parameters: {'objective': 'poisson', 'learning_rate': 0.022878863522445895, 'n_estimators': 7000, 'num_leaves': 63, 'min_child_samples': 160, 'subsample': 0.7841048247374584, 'colsample_bytree': 0.6195154778955838, 'reg_alpha': 0.5693313223519999, 'reg_lambda': 3.8797121157609578, 'min_split_gain': 0.16167946962329224}. Best is trial 2 with value: 15.630776991726515.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004567 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.8558\tvalid_0's tweedie: 12.774\n",
            "[400]\tvalid_0's rmse: 24.8278\tvalid_0's tweedie: 13.1615\n",
            "Early stopping, best iteration is:\n",
            "[200]\tvalid_0's rmse: 21.8558\tvalid_0's tweedie: 12.774\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004587 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2479\tvalid_0's tweedie: 10.336\n",
            "[400]\tvalid_0's rmse: 11.8622\tvalid_0's tweedie: 10.3285\n",
            "Early stopping, best iteration is:\n",
            "[252]\tvalid_0's rmse: 12.1148\tvalid_0's tweedie: 10.3064\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004748 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1154\tvalid_0's tweedie: 10.8466\n",
            "[400]\tvalid_0's rmse: 13.1243\tvalid_0's tweedie: 10.7688\n",
            "Early stopping, best iteration is:\n",
            "[303]\tvalid_0's rmse: 12.8855\tvalid_0's tweedie: 10.7683\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004628 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4242\tvalid_0's tweedie: 10.6706\n",
            "[400]\tvalid_0's rmse: 15.7869\tvalid_0's tweedie: 10.7939\n",
            "Early stopping, best iteration is:\n",
            "[235]\tvalid_0's rmse: 15.3911\tvalid_0's tweedie: 10.6608\n",
            "[I 2025-08-18 14:28:36,991] Trial 3 finished with value: 15.561814604172316 and parameters: {'objective': 'tweedie', 'learning_rate': 0.020307356380344244, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 80, 'subsample': 0.9318640804157564, 'colsample_bytree': 0.677633994480005, 'reg_alpha': 0.39751337061238917, 'reg_lambda': 1.5909887663129383, 'min_split_gain': 0.10401360423556216, 'tweedie_variance_power': 1.37335513967164}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004512 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7573\tvalid_0's poisson: -27.763\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 21.8716\tvalid_0's poisson: -27.8441\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004622 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2594\tvalid_0's poisson: -13.8651\n",
            "[400]\tvalid_0's rmse: 12.2627\tvalid_0's poisson: -14.0361\n",
            "Early stopping, best iteration is:\n",
            "[225]\tvalid_0's rmse: 12.178\tvalid_0's poisson: -13.9165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004623 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3885\tvalid_0's poisson: -17.4217\n",
            "[400]\tvalid_0's rmse: 13.4927\tvalid_0's poisson: -17.567\n",
            "Early stopping, best iteration is:\n",
            "[277]\tvalid_0's rmse: 13.1491\tvalid_0's poisson: -17.5577\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004649 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8289\tvalid_0's poisson: -16.0009\n",
            "[400]\tvalid_0's rmse: 15.6729\tvalid_0's poisson: -16.0772\n",
            "Early stopping, best iteration is:\n",
            "[292]\tvalid_0's rmse: 15.648\tvalid_0's poisson: -16.1011\n",
            "[I 2025-08-18 14:29:26,606] Trial 4 finished with value: 15.71165439695664 and parameters: {'objective': 'poisson', 'learning_rate': 0.04536089127921624, 'n_estimators': 9000, 'num_leaves': 143, 'min_child_samples': 220, 'subsample': 0.7676985004103839, 'colsample_bytree': 0.6587948587257435, 'reg_alpha': 0.02713637334632284, 'reg_lambda': 1.6386561576714251, 'min_split_gain': 0.07773545793789641}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004678 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.3596\tvalid_0's poisson: -26.5176\n",
            "[400]\tvalid_0's rmse: 22.495\tvalid_0's poisson: -27.5836\n",
            "Early stopping, best iteration is:\n",
            "[396]\tvalid_0's rmse: 22.4697\tvalid_0's poisson: -27.5857\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004602 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4672\tvalid_0's poisson: -12.6716\n",
            "[400]\tvalid_0's rmse: 13.0106\tvalid_0's poisson: -13.5357\n",
            "[600]\tvalid_0's rmse: 12.7553\tvalid_0's poisson: -13.7766\n",
            "[800]\tvalid_0's rmse: 12.5576\tvalid_0's poisson: -13.8946\n",
            "[1000]\tvalid_0's rmse: 12.3951\tvalid_0's poisson: -13.9674\n",
            "[1200]\tvalid_0's rmse: 12.2786\tvalid_0's poisson: -14.0204\n",
            "[1400]\tvalid_0's rmse: 12.2099\tvalid_0's poisson: -14.0536\n",
            "[1600]\tvalid_0's rmse: 12.2023\tvalid_0's poisson: -14.0712\n",
            "Early stopping, best iteration is:\n",
            "[1468]\tvalid_0's rmse: 12.1803\tvalid_0's poisson: -14.0672\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004647 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.7053\tvalid_0's poisson: -16.1448\n",
            "[400]\tvalid_0's rmse: 13.325\tvalid_0's poisson: -17.2179\n",
            "[600]\tvalid_0's rmse: 13.0391\tvalid_0's poisson: -17.466\n",
            "[800]\tvalid_0's rmse: 13.0225\tvalid_0's poisson: -17.5732\n",
            "Early stopping, best iteration is:\n",
            "[688]\tvalid_0's rmse: 12.9822\tvalid_0's poisson: -17.532\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004931 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.5222\tvalid_0's poisson: -14.7141\n",
            "[400]\tvalid_0's rmse: 16.0399\tvalid_0's poisson: -15.7372\n",
            "[600]\tvalid_0's rmse: 15.7839\tvalid_0's poisson: -16.0167\n",
            "[800]\tvalid_0's rmse: 15.5613\tvalid_0's poisson: -16.1408\n",
            "[1000]\tvalid_0's rmse: 15.4751\tvalid_0's poisson: -16.1793\n",
            "Early stopping, best iteration is:\n",
            "[980]\tvalid_0's rmse: 15.4618\tvalid_0's poisson: -16.1856\n",
            "[I 2025-08-18 14:30:50,225] Trial 5 finished with value: 15.77351485257228 and parameters: {'objective': 'poisson', 'learning_rate': 0.015716824201651516, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 200, 'subsample': 0.7649101287359542, 'colsample_bytree': 0.8960660809801553, 'reg_alpha': 0.46334686157799443, 'reg_lambda': 1.1955048853696035, 'min_split_gain': 0.00110442342472048}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.039278 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.7197\tvalid_0's l2: 471.747\n",
            "Early stopping, best iteration is:\n",
            "[167]\tvalid_0's rmse: 21.6677\tvalid_0's l2: 469.487\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004865 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5431\tvalid_0's l2: 157.329\n",
            "[400]\tvalid_0's rmse: 12.6764\tvalid_0's l2: 160.692\n",
            "Early stopping, best iteration is:\n",
            "[210]\tvalid_0's rmse: 12.4933\tvalid_0's l2: 156.084\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004615 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1831\tvalid_0's l2: 173.794\n",
            "[400]\tvalid_0's rmse: 13.1779\tvalid_0's l2: 173.658\n",
            "Early stopping, best iteration is:\n",
            "[229]\tvalid_0's rmse: 13.1494\tvalid_0's l2: 172.907\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004740 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.0125\tvalid_0's l2: 256.402\n",
            "[400]\tvalid_0's rmse: 15.9567\tvalid_0's l2: 254.618\n",
            "Early stopping, best iteration is:\n",
            "[257]\tvalid_0's rmse: 15.9285\tvalid_0's l2: 253.717\n",
            "[I 2025-08-18 14:31:28,474] Trial 6 finished with value: 15.809726382576695 and parameters: {'objective': 'regression', 'learning_rate': 0.03460149293996123, 'n_estimators': 3000, 'num_leaves': 111, 'min_child_samples': 80, 'subsample': 0.9226206851751186, 'colsample_bytree': 0.7869894380482674, 'reg_alpha': 0.1985388149115895, 'reg_lambda': 0.7224542260010827, 'min_split_gain': 0.062196464343132446}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004976 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.6021\tvalid_0's poisson: -27.7368\n",
            "Early stopping, best iteration is:\n",
            "[146]\tvalid_0's rmse: 22.3152\tvalid_0's poisson: -27.6688\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004638 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.289\tvalid_0's poisson: -13.8082\n",
            "[400]\tvalid_0's rmse: 12.1084\tvalid_0's poisson: -14.0091\n",
            "[600]\tvalid_0's rmse: 11.9986\tvalid_0's poisson: -14.0687\n",
            "[800]\tvalid_0's rmse: 12.0492\tvalid_0's poisson: -14.0619\n",
            "Early stopping, best iteration is:\n",
            "[605]\tvalid_0's rmse: 11.9874\tvalid_0's poisson: -14.0723\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004614 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2434\tvalid_0's poisson: -17.3813\n",
            "[400]\tvalid_0's rmse: 13.4111\tvalid_0's poisson: -17.5433\n",
            "Early stopping, best iteration is:\n",
            "[271]\tvalid_0's rmse: 13.0583\tvalid_0's poisson: -17.5348\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004648 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9443\tvalid_0's poisson: -15.9079\n",
            "[400]\tvalid_0's rmse: 15.7401\tvalid_0's poisson: -16.0516\n",
            "Early stopping, best iteration is:\n",
            "[398]\tvalid_0's rmse: 15.7375\tvalid_0's poisson: -16.0531\n",
            "[I 2025-08-18 14:32:17,271] Trial 7 finished with value: 15.774602914500635 and parameters: {'objective': 'poisson', 'learning_rate': 0.04169990777997927, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.9021570097233794, 'colsample_bytree': 0.7683831592708489, 'reg_alpha': 0.4625803079727366, 'reg_lambda': 2.2282845872753674, 'min_split_gain': 0.10454656587639882}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004629 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7287\tvalid_0's l2: 516.592\n",
            "[400]\tvalid_0's rmse: 22.1308\tvalid_0's l2: 489.771\n",
            "[600]\tvalid_0's rmse: 21.9799\tvalid_0's l2: 483.116\n",
            "[800]\tvalid_0's rmse: 21.9296\tvalid_0's l2: 480.907\n",
            "[1000]\tvalid_0's rmse: 21.9287\tvalid_0's l2: 480.869\n",
            "Early stopping, best iteration is:\n",
            "[838]\tvalid_0's rmse: 21.9064\tvalid_0's l2: 479.892\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004556 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1317\tvalid_0's l2: 172.442\n",
            "[400]\tvalid_0's rmse: 12.8869\tvalid_0's l2: 166.071\n",
            "[600]\tvalid_0's rmse: 12.8661\tvalid_0's l2: 165.536\n",
            "Early stopping, best iteration is:\n",
            "[479]\tvalid_0's rmse: 12.8499\tvalid_0's l2: 165.119\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004708 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.4773\tvalid_0's l2: 209.591\n",
            "[400]\tvalid_0's rmse: 13.5585\tvalid_0's l2: 183.833\n",
            "[600]\tvalid_0's rmse: 13.3743\tvalid_0's l2: 178.871\n",
            "[800]\tvalid_0's rmse: 13.3131\tvalid_0's l2: 177.238\n",
            "[1000]\tvalid_0's rmse: 13.3065\tvalid_0's l2: 177.062\n",
            "Early stopping, best iteration is:\n",
            "[933]\tvalid_0's rmse: 13.2912\tvalid_0's l2: 176.657\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004684 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.106\tvalid_0's l2: 259.403\n",
            "[400]\tvalid_0's rmse: 15.891\tvalid_0's l2: 252.524\n",
            "[600]\tvalid_0's rmse: 15.8373\tvalid_0's l2: 250.821\n",
            "[800]\tvalid_0's rmse: 15.8391\tvalid_0's l2: 250.878\n",
            "Early stopping, best iteration is:\n",
            "[690]\tvalid_0's rmse: 15.8186\tvalid_0's l2: 250.228\n",
            "[I 2025-08-18 14:33:34,359] Trial 8 finished with value: 15.966533973845298 and parameters: {'objective': 'regression', 'learning_rate': 0.01051884505877539, 'n_estimators': 7000, 'num_leaves': 95, 'min_child_samples': 160, 'subsample': 0.9315132947852186, 'colsample_bytree': 0.6747876687446624, 'reg_alpha': 0.24622975382137782, 'reg_lambda': 3.1444289849006704, 'min_split_gain': 0.045759633098324495}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004609 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7893\tvalid_0's poisson: -27.6927\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 22.1193\tvalid_0's poisson: -27.7037\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004829 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5718\tvalid_0's poisson: -13.8407\n",
            "[400]\tvalid_0's rmse: 12.3359\tvalid_0's poisson: -14.0108\n",
            "Early stopping, best iteration is:\n",
            "[395]\tvalid_0's rmse: 12.3324\tvalid_0's poisson: -14.0098\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004662 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1648\tvalid_0's poisson: -17.4744\n",
            "[400]\tvalid_0's rmse: 13.3529\tvalid_0's poisson: -17.5929\n",
            "Early stopping, best iteration is:\n",
            "[274]\tvalid_0's rmse: 13.0083\tvalid_0's poisson: -17.5981\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004619 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6182\tvalid_0's poisson: -16.0936\n",
            "[400]\tvalid_0's rmse: 15.5873\tvalid_0's poisson: -16.0551\n",
            "Early stopping, best iteration is:\n",
            "[261]\tvalid_0's rmse: 15.4381\tvalid_0's poisson: -16.1578\n",
            "[I 2025-08-18 14:34:28,077] Trial 9 finished with value: 15.72453754313688 and parameters: {'objective': 'poisson', 'learning_rate': 0.044650957058567906, 'n_estimators': 8000, 'num_leaves': 143, 'min_child_samples': 200, 'subsample': 0.9107344153798229, 'colsample_bytree': 0.6559710176658108, 'reg_alpha': 0.5355353990939866, 'reg_lambda': 2.3876978467047776, 'min_split_gain': 0.1614880310328125}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005013 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5827\tvalid_0's tweedie: 11.8218\n",
            "[400]\tvalid_0's rmse: 24.7017\tvalid_0's tweedie: 12.0109\n",
            "Early stopping, best iteration is:\n",
            "[245]\tvalid_0's rmse: 22.303\tvalid_0's tweedie: 11.7874\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.041060 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6499\tvalid_0's tweedie: 9.68123\n",
            "[400]\tvalid_0's rmse: 12.1189\tvalid_0's tweedie: 9.63165\n",
            "Early stopping, best iteration is:\n",
            "[303]\tvalid_0's rmse: 12.3401\tvalid_0's tweedie: 9.61981\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004677 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3454\tvalid_0's tweedie: 10.1088\n",
            "[400]\tvalid_0's rmse: 13.1744\tvalid_0's tweedie: 9.99939\n",
            "Early stopping, best iteration is:\n",
            "[311]\tvalid_0's rmse: 12.9687\tvalid_0's tweedie: 10.0127\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004669 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6957\tvalid_0's tweedie: 9.92613\n",
            "[400]\tvalid_0's rmse: 15.6498\tvalid_0's tweedie: 9.90674\n",
            "Early stopping, best iteration is:\n",
            "[297]\tvalid_0's rmse: 15.5525\tvalid_0's tweedie: 9.86722\n",
            "[I 2025-08-18 14:35:42,108] Trial 10 finished with value: 15.791077153060188 and parameters: {'objective': 'tweedie', 'learning_rate': 0.01718814765928189, 'n_estimators': 3000, 'num_leaves': 191, 'min_child_samples': 120, 'subsample': 0.8650416133630545, 'colsample_bytree': 0.7083935989325427, 'reg_alpha': 0.34664294031762954, 'reg_lambda': 2.416453843746414, 'min_split_gain': 0.11481705215099816, 'tweedie_variance_power': 1.395237344796865}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004621 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.445\tvalid_0's tweedie: 38.5603\n",
            "Early stopping, best iteration is:\n",
            "[174]\tvalid_0's rmse: 22.0346\tvalid_0's tweedie: 38.5207\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004657 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.1727\tvalid_0's tweedie: 27.7263\n",
            "[400]\tvalid_0's rmse: 12.2011\tvalid_0's tweedie: 27.7017\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 12.136\tvalid_0's tweedie: 27.7031\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004744 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0733\tvalid_0's tweedie: 30.1668\n",
            "[400]\tvalid_0's rmse: 13.619\tvalid_0's tweedie: 30.157\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's rmse: 12.9065\tvalid_0's tweedie: 30.1129\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004671 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5988\tvalid_0's tweedie: 29.4732\n",
            "[400]\tvalid_0's rmse: 15.8276\tvalid_0's tweedie: 29.6297\n",
            "Early stopping, best iteration is:\n",
            "[225]\tvalid_0's rmse: 15.5192\tvalid_0's tweedie: 29.4472\n",
            "[I 2025-08-18 14:36:40,045] Trial 11 finished with value: 15.649064079495005 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02247060760847469, 'n_estimators': 7000, 'num_leaves': 191, 'min_child_samples': 120, 'subsample': 0.8354224611266663, 'colsample_bytree': 0.603483925739636, 'reg_alpha': 0.5643890617661809, 'reg_lambda': 3.821435471654726, 'min_split_gain': 0.14752469291369416, 'tweedie_variance_power': 1.1748892059080516}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004608 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3941\tvalid_0's tweedie: 8.07716\n",
            "Early stopping, best iteration is:\n",
            "[155]\tvalid_0's rmse: 21.7233\tvalid_0's tweedie: 8.01328\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004610 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5984\tvalid_0's tweedie: 6.96564\n",
            "[400]\tvalid_0's rmse: 12.0542\tvalid_0's tweedie: 6.99365\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 12.3233\tvalid_0's tweedie: 6.95235\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004675 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1891\tvalid_0's tweedie: 7.14318\n",
            "[400]\tvalid_0's rmse: 13.4807\tvalid_0's tweedie: 7.10649\n",
            "Early stopping, best iteration is:\n",
            "[249]\tvalid_0's rmse: 12.8934\tvalid_0's tweedie: 7.10843\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004727 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4469\tvalid_0's tweedie: 7.01998\n",
            "[400]\tvalid_0's rmse: 16.2345\tvalid_0's tweedie: 7.12593\n",
            "Early stopping, best iteration is:\n",
            "[201]\tvalid_0's rmse: 15.4435\tvalid_0's tweedie: 7.02003\n",
            "[I 2025-08-18 14:37:11,019] Trial 12 finished with value: 15.595893083115566 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028114508506570077, 'n_estimators': 5000, 'num_leaves': 63, 'min_child_samples': 120, 'subsample': 0.8696820387147985, 'colsample_bytree': 0.6235387154900341, 'reg_alpha': 0.5966685117386447, 'reg_lambda': 3.997768535245429, 'min_split_gain': 0.14690472616130312, 'tweedie_variance_power': 1.5277921423948664}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004615 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.882\tvalid_0's tweedie: 7.25619\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 22.4804\tvalid_0's tweedie: 7.10258\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004532 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2392\tvalid_0's tweedie: 6.42924\n",
            "Early stopping, best iteration is:\n",
            "[136]\tvalid_0's rmse: 12.4685\tvalid_0's tweedie: 6.3959\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004637 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1454\tvalid_0's tweedie: 6.44788\n",
            "[400]\tvalid_0's rmse: 13.8848\tvalid_0's tweedie: 6.47697\n",
            "Early stopping, best iteration is:\n",
            "[243]\tvalid_0's rmse: 13.084\tvalid_0's tweedie: 6.44338\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004675 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6552\tvalid_0's tweedie: 6.41065\n",
            "Early stopping, best iteration is:\n",
            "[166]\tvalid_0's rmse: 15.6256\tvalid_0's tweedie: 6.40519\n",
            "[I 2025-08-18 14:37:56,045] Trial 13 finished with value: 15.914634350320167 and parameters: {'objective': 'tweedie', 'learning_rate': 0.030272743765332996, 'n_estimators': 4000, 'num_leaves': 159, 'min_child_samples': 100, 'subsample': 0.8742266313601691, 'colsample_bytree': 0.7119775029677049, 'reg_alpha': 0.35538400311332774, 'reg_lambda': 3.106852184061829, 'min_split_gain': 0.12935838012227258, 'tweedie_variance_power': 1.5952155422560634}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004739 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.1339\tvalid_0's tweedie: 9.09015\n",
            "[400]\tvalid_0's rmse: 24.1912\tvalid_0's tweedie: 9.29688\n",
            "Early stopping, best iteration is:\n",
            "[226]\tvalid_0's rmse: 21.8017\tvalid_0's tweedie: 9.08439\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.019479 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5348\tvalid_0's tweedie: 7.76071\n",
            "[400]\tvalid_0's rmse: 12.0355\tvalid_0's tweedie: 7.7245\n",
            "Early stopping, best iteration is:\n",
            "[351]\tvalid_0's rmse: 12.1923\tvalid_0's tweedie: 7.70997\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004607 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.6307\tvalid_0's tweedie: 8.02632\n",
            "[400]\tvalid_0's rmse: 13.4109\tvalid_0's tweedie: 7.92587\n",
            "Early stopping, best iteration is:\n",
            "[368]\tvalid_0's rmse: 13.2273\tvalid_0's tweedie: 7.92352\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004678 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6416\tvalid_0's tweedie: 7.89292\n",
            "[400]\tvalid_0's rmse: 15.6153\tvalid_0's tweedie: 7.88748\n",
            "Early stopping, best iteration is:\n",
            "[293]\tvalid_0's rmse: 15.5072\tvalid_0's tweedie: 7.84473\n",
            "[I 2025-08-18 14:38:46,326] Trial 14 finished with value: 15.682142762928905 and parameters: {'objective': 'tweedie', 'learning_rate': 0.018580948318753128, 'n_estimators': 4000, 'num_leaves': 111, 'min_child_samples': 100, 'subsample': 0.8861921123644749, 'colsample_bytree': 0.7180692335554328, 'reg_alpha': 0.2227462510441177, 'reg_lambda': 3.163713216492078, 'min_split_gain': 0.1954532931190156, 'tweedie_variance_power': 1.4766190354316429}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004524 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7652\tvalid_0's tweedie: 22.0974\n",
            "Early stopping, best iteration is:\n",
            "[147]\tvalid_0's rmse: 21.9943\tvalid_0's tweedie: 22.0164\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004558 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3319\tvalid_0's tweedie: 16.6748\n",
            "[400]\tvalid_0's rmse: 12.3035\tvalid_0's tweedie: 16.6982\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's rmse: 12.2614\tvalid_0's tweedie: 16.6709\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004709 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9681\tvalid_0's tweedie: 17.7988\n",
            "Early stopping, best iteration is:\n",
            "[190]\tvalid_0's rmse: 12.9049\tvalid_0's tweedie: 17.8021\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004660 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4614\tvalid_0's tweedie: 17.4849\n",
            "Early stopping, best iteration is:\n",
            "[183]\tvalid_0's rmse: 15.4387\tvalid_0's tweedie: 17.4673\n",
            "[I 2025-08-18 14:39:35,102] Trial 15 finished with value: 15.649813877371269 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02777667028828531, 'n_estimators': 5000, 'num_leaves': 159, 'min_child_samples': 120, 'subsample': 0.9473318728590885, 'colsample_bytree': 0.6409886150481998, 'reg_alpha': 0.3999213835846313, 'reg_lambda': 1.9043340794356796, 'min_split_gain': 0.13504259776746236, 'tweedie_variance_power': 1.2589179195234093}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004784 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.8594\tvalid_0's tweedie: 8.54596\n",
            "Early stopping, best iteration is:\n",
            "[119]\tvalid_0's rmse: 21.685\tvalid_0's tweedie: 8.3533\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004550 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2412\tvalid_0's tweedie: 7.23661\n",
            "Early stopping, best iteration is:\n",
            "[187]\tvalid_0's rmse: 12.2284\tvalid_0's tweedie: 7.23011\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004596 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3154\tvalid_0's tweedie: 7.38182\n",
            "Early stopping, best iteration is:\n",
            "[178]\tvalid_0's rmse: 13.1314\tvalid_0's tweedie: 7.39166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004851 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4319\tvalid_0's tweedie: 7.30789\n",
            "Early stopping, best iteration is:\n",
            "[197]\tvalid_0's rmse: 15.4363\tvalid_0's tweedie: 7.30505\n",
            "[I 2025-08-18 14:40:01,883] Trial 16 finished with value: 15.620245464515557 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03315868754198881, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 100, 'subsample': 0.8463047083041412, 'colsample_bytree': 0.8414322847396881, 'reg_alpha': 0.5220100788638861, 'reg_lambda': 1.1415687453492653, 'min_split_gain': 0.09386925355271923, 'tweedie_variance_power': 1.5095336881172505}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004802 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3885\tvalid_0's tweedie: 14.7492\n",
            "[400]\tvalid_0's rmse: 25.2647\tvalid_0's tweedie: 15.0982\n",
            "Early stopping, best iteration is:\n",
            "[201]\tvalid_0's rmse: 22.3839\tvalid_0's tweedie: 14.7486\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004584 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.386\tvalid_0's tweedie: 11.6943\n",
            "[400]\tvalid_0's rmse: 12.0193\tvalid_0's tweedie: 11.6253\n",
            "Early stopping, best iteration is:\n",
            "[392]\tvalid_0's rmse: 12.0368\tvalid_0's tweedie: 11.6235\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.048997 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.054\tvalid_0's tweedie: 12.3353\n",
            "[400]\tvalid_0's rmse: 13.3301\tvalid_0's tweedie: 12.2587\n",
            "Early stopping, best iteration is:\n",
            "[258]\tvalid_0's rmse: 12.8976\tvalid_0's tweedie: 12.2753\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004743 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5459\tvalid_0's tweedie: 12.1266\n",
            "[400]\tvalid_0's rmse: 15.7706\tvalid_0's tweedie: 12.1837\n",
            "Early stopping, best iteration is:\n",
            "[274]\tvalid_0's rmse: 15.4066\tvalid_0's tweedie: 12.0895\n",
            "[I 2025-08-18 14:40:47,297] Trial 17 finished with value: 15.68123575505887 and parameters: {'objective': 'tweedie', 'learning_rate': 0.019884347306329243, 'n_estimators': 5000, 'num_leaves': 111, 'min_child_samples': 80, 'subsample': 0.8258921124362707, 'colsample_bytree': 0.6856480965580098, 'reg_alpha': 0.2951486838853746, 'reg_lambda': 2.7545320222120893, 'min_split_gain': 0.03990107435966163, 'tweedie_variance_power': 1.3396765886951214}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005532 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.9672\tvalid_0's tweedie: 11.3184\n",
            "[400]\tvalid_0's rmse: 22.4546\tvalid_0's tweedie: 11.2229\n",
            "Early stopping, best iteration is:\n",
            "[303]\tvalid_0's rmse: 21.975\tvalid_0's tweedie: 11.1854\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.035923 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8312\tvalid_0's tweedie: 9.32999\n",
            "[400]\tvalid_0's rmse: 12.5041\tvalid_0's tweedie: 9.19947\n",
            "Early stopping, best iteration is:\n",
            "[379]\tvalid_0's rmse: 12.4604\tvalid_0's tweedie: 9.19564\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004644 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.1417\tvalid_0's tweedie: 9.75846\n",
            "[400]\tvalid_0's rmse: 13.027\tvalid_0's tweedie: 9.56107\n",
            "[600]\tvalid_0's rmse: 13.4833\tvalid_0's tweedie: 9.55515\n",
            "Early stopping, best iteration is:\n",
            "[453]\tvalid_0's rmse: 12.9862\tvalid_0's tweedie: 9.54591\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.026028 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7964\tvalid_0's tweedie: 9.57626\n",
            "[400]\tvalid_0's rmse: 15.3877\tvalid_0's tweedie: 9.42857\n",
            "Early stopping, best iteration is:\n",
            "[398]\tvalid_0's rmse: 15.3823\tvalid_0's tweedie: 9.42759\n",
            "[I 2025-08-18 14:42:02,752] Trial 18 finished with value: 15.700976512160322 and parameters: {'objective': 'tweedie', 'learning_rate': 0.013946899292001554, 'n_estimators': 4000, 'num_leaves': 175, 'min_child_samples': 140, 'subsample': 0.8955074525356831, 'colsample_bytree': 0.6205072231322499, 'reg_alpha': 0.1440470957242423, 'reg_lambda': 3.5312819572167715, 'min_split_gain': 0.17336677435234124, 'tweedie_variance_power': 1.4091614684959153}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004537 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.9774\tvalid_0's tweedie: 7.27378\n",
            "Early stopping, best iteration is:\n",
            "[158]\tvalid_0's rmse: 21.869\tvalid_0's tweedie: 7.21851\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004559 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5315\tvalid_0's tweedie: 6.43995\n",
            "[400]\tvalid_0's rmse: 12.1718\tvalid_0's tweedie: 6.5194\n",
            "Early stopping, best iteration is:\n",
            "[224]\tvalid_0's rmse: 12.4611\tvalid_0's tweedie: 6.43641\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005416 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1059\tvalid_0's tweedie: 6.54706\n",
            "[400]\tvalid_0's rmse: 13.3586\tvalid_0's tweedie: 6.53069\n",
            "Early stopping, best iteration is:\n",
            "[248]\tvalid_0's rmse: 12.978\tvalid_0's tweedie: 6.52993\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004667 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6996\tvalid_0's tweedie: 6.46058\n",
            "[400]\tvalid_0's rmse: 15.9432\tvalid_0's tweedie: 6.54587\n",
            "Early stopping, best iteration is:\n",
            "[235]\tvalid_0's rmse: 15.6609\tvalid_0's tweedie: 6.45395\n",
            "[I 2025-08-18 14:42:40,279] Trial 19 finished with value: 15.742231303264374 and parameters: {'objective': 'tweedie', 'learning_rate': 0.024993847539188246, 'n_estimators': 5000, 'num_leaves': 95, 'min_child_samples': 100, 'subsample': 0.86790131985384, 'colsample_bytree': 0.7471759463607004, 'reg_alpha': 0.49505707922142606, 'reg_lambda': 1.9666561828849067, 'min_split_gain': 0.12081084068629021, 'tweedie_variance_power': 1.5848024319125553}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005314 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.9419\tvalid_0's tweedie: 18.8238\n",
            "Early stopping, best iteration is:\n",
            "[109]\tvalid_0's rmse: 21.9484\tvalid_0's tweedie: 18.5259\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.029467 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4799\tvalid_0's tweedie: 14.3165\n",
            "Early stopping, best iteration is:\n",
            "[142]\tvalid_0's rmse: 12.2945\tvalid_0's tweedie: 14.3047\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004887 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8704\tvalid_0's tweedie: 15.132\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's rmse: 12.7466\tvalid_0's tweedie: 15.1436\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004591 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.717\tvalid_0's tweedie: 14.97\n",
            "Early stopping, best iteration is:\n",
            "[144]\tvalid_0's rmse: 15.5064\tvalid_0's tweedie: 14.8908\n",
            "[I 2025-08-18 14:43:16,494] Trial 20 finished with value: 15.623984134330964 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03543522381674099, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 120, 'subsample': 0.9494939656961947, 'colsample_bytree': 0.6881252165285292, 'reg_alpha': 0.30036477556366215, 'reg_lambda': 2.7418554667797057, 'min_split_gain': 0.09814360081167603, 'tweedie_variance_power': 1.291068354390487}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004505 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.9848\tvalid_0's tweedie: 8.90383\n",
            "Early stopping, best iteration is:\n",
            "[132]\tvalid_0's rmse: 21.6554\tvalid_0's tweedie: 8.72989\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004981 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.2421\tvalid_0's tweedie: 7.47073\n",
            "Early stopping, best iteration is:\n",
            "[197]\tvalid_0's rmse: 12.2359\tvalid_0's tweedie: 7.4683\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004569 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9886\tvalid_0's tweedie: 7.66962\n",
            "Early stopping, best iteration is:\n",
            "[199]\tvalid_0's rmse: 12.9809\tvalid_0's tweedie: 7.67038\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004756 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5553\tvalid_0's tweedie: 7.58196\n",
            "Early stopping, best iteration is:\n",
            "[188]\tvalid_0's rmse: 15.5656\tvalid_0's tweedie: 7.57864\n",
            "[I 2025-08-18 14:43:43,659] Trial 21 finished with value: 15.609434112078969 and parameters: {'objective': 'tweedie', 'learning_rate': 0.0328433080045458, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 100, 'subsample': 0.843862736901848, 'colsample_bytree': 0.8604409042662362, 'reg_alpha': 0.5019264333826692, 'reg_lambda': 1.0632982647446036, 'min_split_gain': 0.08919148221569424, 'tweedie_variance_power': 1.4911325442624894}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004535 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.91\tvalid_0's tweedie: 8.79356\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's rmse: 21.8363\tvalid_0's tweedie: 8.68375\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004561 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.3761\tvalid_0's tweedie: 7.44302\n",
            "Early stopping, best iteration is:\n",
            "[162]\tvalid_0's rmse: 12.4883\tvalid_0's tweedie: 7.42775\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004699 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9789\tvalid_0's tweedie: 7.57878\n",
            "Early stopping, best iteration is:\n",
            "[176]\tvalid_0's rmse: 12.8397\tvalid_0's tweedie: 7.58582\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004690 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5848\tvalid_0's tweedie: 7.51739\n",
            "Early stopping, best iteration is:\n",
            "[168]\tvalid_0's rmse: 15.4855\tvalid_0's tweedie: 7.49239\n",
            "[I 2025-08-18 14:44:09,071] Trial 22 finished with value: 15.66245710269051 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03798758660189606, 'n_estimators': 4000, 'num_leaves': 63, 'min_child_samples': 80, 'subsample': 0.8177404377935434, 'colsample_bytree': 0.8247088302080987, 'reg_alpha': 0.4136448166793312, 'reg_lambda': 1.025864242263196, 'min_split_gain': 0.08879705469125412, 'tweedie_variance_power': 1.4960250285429917}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.011900 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 24.425\tvalid_0's tweedie: 10.3242\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 22.1403\tvalid_0's tweedie: 10.1421\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004615 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.243\tvalid_0's tweedie: 8.48332\n",
            "[400]\tvalid_0's rmse: 12.2405\tvalid_0's tweedie: 8.54171\n",
            "Early stopping, best iteration is:\n",
            "[225]\tvalid_0's rmse: 12.0843\tvalid_0's tweedie: 8.47417\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004674 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8185\tvalid_0's tweedie: 8.72943\n",
            "[400]\tvalid_0's rmse: 13.7495\tvalid_0's tweedie: 8.73763\n",
            "Early stopping, best iteration is:\n",
            "[209]\tvalid_0's rmse: 12.7978\tvalid_0's tweedie: 8.72347\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004698 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6719\tvalid_0's tweedie: 8.65946\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 15.652\tvalid_0's tweedie: 8.65265\n",
            "[I 2025-08-18 14:44:43,343] Trial 23 finished with value: 15.668609969095716 and parameters: {'objective': 'tweedie', 'learning_rate': 0.030372608777143345, 'n_estimators': 3000, 'num_leaves': 95, 'min_child_samples': 100, 'subsample': 0.7960121561957906, 'colsample_bytree': 0.8972932991360462, 'reg_alpha': 0.5838468019854246, 'reg_lambda': 1.4617972924410005, 'min_split_gain': 0.14419703627164468, 'tweedie_variance_power': 1.439807340059604}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004602 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.7534\tvalid_0's tweedie: 8.46865\n",
            "[400]\tvalid_0's rmse: 25.7829\tvalid_0's tweedie: 8.79891\n",
            "Early stopping, best iteration is:\n",
            "[203]\tvalid_0's rmse: 21.7337\tvalid_0's tweedie: 8.4642\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004838 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7114\tvalid_0's tweedie: 7.32117\n",
            "[400]\tvalid_0's rmse: 12.0747\tvalid_0's tweedie: 7.27781\n",
            "Early stopping, best iteration is:\n",
            "[389]\tvalid_0's rmse: 12.0485\tvalid_0's tweedie: 7.27485\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004664 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.388\tvalid_0's tweedie: 7.52007\n",
            "[400]\tvalid_0's rmse: 13.1798\tvalid_0's tweedie: 7.42981\n",
            "Early stopping, best iteration is:\n",
            "[285]\tvalid_0's rmse: 12.9053\tvalid_0's tweedie: 7.45017\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004705 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8981\tvalid_0's tweedie: 7.41571\n",
            "[400]\tvalid_0's rmse: 15.8376\tvalid_0's tweedie: 7.41242\n",
            "Early stopping, best iteration is:\n",
            "[292]\tvalid_0's rmse: 15.6933\tvalid_0's tweedie: 7.37659\n",
            "[I 2025-08-18 14:45:25,616] Trial 24 finished with value: 15.595202423632234 and parameters: {'objective': 'tweedie', 'learning_rate': 0.020593805943850938, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 140, 'subsample': 0.8468918996778647, 'colsample_bytree': 0.8499494725108373, 'reg_alpha': 0.4927283407424088, 'reg_lambda': 0.8714431036854411, 'min_split_gain': 0.05664293419689653, 'tweedie_variance_power': 1.5053258942280607}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004571 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.1376\tvalid_0's l2: 490.071\n",
            "[400]\tvalid_0's rmse: 21.9867\tvalid_0's l2: 483.414\n",
            "[600]\tvalid_0's rmse: 21.9916\tvalid_0's l2: 483.63\n",
            "Early stopping, best iteration is:\n",
            "[422]\tvalid_0's rmse: 21.9508\tvalid_0's l2: 481.839\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.010502 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.844\tvalid_0's l2: 164.968\n",
            "[400]\tvalid_0's rmse: 12.8446\tvalid_0's l2: 164.985\n",
            "Early stopping, best iteration is:\n",
            "[210]\tvalid_0's rmse: 12.8139\tvalid_0's l2: 164.197\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004664 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5918\tvalid_0's l2: 184.738\n",
            "[400]\tvalid_0's rmse: 13.2161\tvalid_0's l2: 174.666\n",
            "[600]\tvalid_0's rmse: 13.202\tvalid_0's l2: 174.294\n",
            "Early stopping, best iteration is:\n",
            "[584]\tvalid_0's rmse: 13.1984\tvalid_0's l2: 174.198\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004694 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9306\tvalid_0's l2: 253.785\n",
            "[400]\tvalid_0's rmse: 15.817\tvalid_0's l2: 250.178\n",
            "[600]\tvalid_0's rmse: 15.8405\tvalid_0's l2: 250.92\n",
            "Early stopping, best iteration is:\n",
            "[445]\tvalid_0's rmse: 15.7971\tvalid_0's l2: 249.548\n",
            "[I 2025-08-18 14:46:10,146] Trial 25 finished with value: 15.940069459156193 and parameters: {'objective': 'regression', 'learning_rate': 0.020869189056618873, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 140, 'subsample': 0.8584844197747412, 'colsample_bytree': 0.6451917382007734, 'reg_alpha': 0.5892637958030723, 'reg_lambda': 0.9042740638744728, 'min_split_gain': 0.01906585319391002}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004558 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.0025\tvalid_0's tweedie: 7.89846\n",
            "Early stopping, best iteration is:\n",
            "[163]\tvalid_0's rmse: 21.9805\tvalid_0's tweedie: 7.87899\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004670 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5246\tvalid_0's tweedie: 6.88015\n",
            "[400]\tvalid_0's rmse: 12.0569\tvalid_0's tweedie: 6.89333\n",
            "Early stopping, best iteration is:\n",
            "[275]\tvalid_0's rmse: 12.2197\tvalid_0's tweedie: 6.86831\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.025926 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1753\tvalid_0's tweedie: 7.03713\n",
            "[400]\tvalid_0's rmse: 13.6757\tvalid_0's tweedie: 7.00898\n",
            "Early stopping, best iteration is:\n",
            "[235]\tvalid_0's rmse: 12.9701\tvalid_0's tweedie: 7.01078\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004637 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6113\tvalid_0's tweedie: 6.9324\n",
            "[400]\tvalid_0's rmse: 15.8491\tvalid_0's tweedie: 6.95929\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid_0's rmse: 15.4381\tvalid_0's tweedie: 6.9129\n",
            "[I 2025-08-18 14:46:48,704] Trial 26 finished with value: 15.652096009265168 and parameters: {'objective': 'tweedie', 'learning_rate': 0.024203185783730298, 'n_estimators': 5000, 'num_leaves': 95, 'min_child_samples': 160, 'subsample': 0.8784609830134092, 'colsample_bytree': 0.7358958070163119, 'reg_alpha': 0.45899215198779775, 'reg_lambda': 1.79709837272499, 'min_split_gain': 0.04919618421913942, 'tweedie_variance_power': 1.5365050083200185}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004720 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.4286\tvalid_0's tweedie: 13.7242\n",
            "[400]\tvalid_0's rmse: 24.1122\tvalid_0's tweedie: 13.8697\n",
            "Early stopping, best iteration is:\n",
            "[243]\tvalid_0's rmse: 22.2472\tvalid_0's tweedie: 13.6762\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004590 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7255\tvalid_0's tweedie: 11.0289\n",
            "[400]\tvalid_0's rmse: 12.3206\tvalid_0's tweedie: 10.9405\n",
            "[600]\tvalid_0's rmse: 12.1307\tvalid_0's tweedie: 10.9612\n",
            "Early stopping, best iteration is:\n",
            "[467]\tvalid_0's rmse: 12.2099\tvalid_0's tweedie: 10.9345\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.035667 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.4856\tvalid_0's tweedie: 11.5757\n",
            "[400]\tvalid_0's rmse: 13.1348\tvalid_0's tweedie: 11.4341\n",
            "Early stopping, best iteration is:\n",
            "[309]\tvalid_0's rmse: 12.9312\tvalid_0's tweedie: 11.4493\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004875 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.903\tvalid_0's tweedie: 11.3773\n",
            "[400]\tvalid_0's rmse: 15.6104\tvalid_0's tweedie: 11.3036\n",
            "Early stopping, best iteration is:\n",
            "[324]\tvalid_0's rmse: 15.5806\tvalid_0's tweedie: 11.291\n",
            "[I 2025-08-18 14:47:51,844] Trial 27 finished with value: 15.742209946959175 and parameters: {'objective': 'tweedie', 'learning_rate': 0.016530213522963407, 'n_estimators': 4000, 'num_leaves': 143, 'min_child_samples': 140, 'subsample': 0.8887771731772734, 'colsample_bytree': 0.8091482776896969, 'reg_alpha': 0.3921802660905733, 'reg_lambda': 1.3430821813613951, 'min_split_gain': 0.023195757112949383, 'tweedie_variance_power': 1.3576512567537393}. Best is trial 3 with value: 15.561814604172316.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004521 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.7897\tvalid_0's tweedie: 7.64552\n",
            "[400]\tvalid_0's rmse: 24.1572\tvalid_0's tweedie: 7.78217\n",
            "Early stopping, best iteration is:\n",
            "[214]\tvalid_0's rmse: 21.6925\tvalid_0's tweedie: 7.64424\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.007942 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7621\tvalid_0's tweedie: 6.76417\n",
            "[400]\tvalid_0's rmse: 11.8825\tvalid_0's tweedie: 6.7249\n",
            "Early stopping, best iteration is:\n",
            "[391]\tvalid_0's rmse: 11.901\tvalid_0's tweedie: 6.72396\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004720 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.7516\tvalid_0's tweedie: 6.93286\n",
            "[400]\tvalid_0's rmse: 13.017\tvalid_0's tweedie: 6.83647\n",
            "[600]\tvalid_0's rmse: 13.633\tvalid_0's tweedie: 6.8426\n",
            "Early stopping, best iteration is:\n",
            "[407]\tvalid_0's rmse: 13.004\tvalid_0's tweedie: 6.83689\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004658 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7509\tvalid_0's tweedie: 6.81764\n",
            "[400]\tvalid_0's rmse: 15.7882\tvalid_0's tweedie: 6.78916\n",
            "Early stopping, best iteration is:\n",
            "[313]\tvalid_0's rmse: 15.6133\tvalid_0's tweedie: 6.76442\n",
            "[I 2025-08-18 14:48:33,644] Trial 28 finished with value: 15.552718292125014 and parameters: {'objective': 'tweedie', 'learning_rate': 0.01972121386434425, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.837042082382094, 'colsample_bytree': 0.6284658777951322, 'reg_alpha': 0.5319018403972384, 'reg_lambda': 0.5191050138205948, 'min_split_gain': 0.11591547940073672, 'tweedie_variance_power': 1.550886914200931}. Best is trial 28 with value: 15.552718292125014.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004570 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7486\tvalid_0's l2: 517.498\n",
            "[400]\tvalid_0's rmse: 22.6133\tvalid_0's l2: 511.361\n",
            "[600]\tvalid_0's rmse: 22.502\tvalid_0's l2: 506.34\n",
            "[800]\tvalid_0's rmse: 22.4261\tvalid_0's l2: 502.931\n",
            "[1000]\tvalid_0's rmse: 22.3993\tvalid_0's l2: 501.729\n",
            "[1200]\tvalid_0's rmse: 22.3758\tvalid_0's l2: 500.677\n",
            "Early stopping, best iteration is:\n",
            "[1057]\tvalid_0's rmse: 22.3467\tvalid_0's l2: 499.377\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004531 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3011\tvalid_0's l2: 176.92\n",
            "[400]\tvalid_0's rmse: 13.2969\tvalid_0's l2: 176.808\n",
            "Early stopping, best iteration is:\n",
            "[261]\tvalid_0's rmse: 13.2463\tvalid_0's l2: 175.465\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004626 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.9013\tvalid_0's l2: 193.246\n",
            "[400]\tvalid_0's rmse: 13.3465\tvalid_0's l2: 178.128\n",
            "[600]\tvalid_0's rmse: 13.24\tvalid_0's l2: 175.298\n",
            "[800]\tvalid_0's rmse: 13.2032\tvalid_0's l2: 174.324\n",
            "[1000]\tvalid_0's rmse: 13.1853\tvalid_0's l2: 173.853\n",
            "Early stopping, best iteration is:\n",
            "[953]\tvalid_0's rmse: 13.1784\tvalid_0's l2: 173.67\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.011448 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.1722\tvalid_0's l2: 261.539\n",
            "[400]\tvalid_0's rmse: 16.0745\tvalid_0's l2: 258.388\n",
            "[600]\tvalid_0's rmse: 15.9867\tvalid_0's l2: 255.576\n",
            "[800]\tvalid_0's rmse: 15.9195\tvalid_0's l2: 253.43\n",
            "Early stopping, best iteration is:\n",
            "[795]\tvalid_0's rmse: 15.9163\tvalid_0's l2: 253.327\n",
            "[I 2025-08-18 14:49:50,908] Trial 29 finished with value: 16.17192777422241 and parameters: {'objective': 'regression', 'learning_rate': 0.014034832816196182, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.8275016426981819, 'colsample_bytree': 0.8706723480236775, 'reg_alpha': 0.4462995289022038, 'reg_lambda': 0.5718050162111463, 'min_split_gain': 0.06421285851522536}. Best is trial 28 with value: 15.552718292125014.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005145 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.4627\tvalid_0's tweedie: 10.1439\n",
            "[400]\tvalid_0's rmse: 24.2927\tvalid_0's tweedie: 10.3008\n",
            "Early stopping, best iteration is:\n",
            "[262]\tvalid_0's rmse: 22.1342\tvalid_0's tweedie: 10.1138\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004547 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6107\tvalid_0's tweedie: 8.47983\n",
            "[400]\tvalid_0's rmse: 12.1272\tvalid_0's tweedie: 8.4005\n",
            "[600]\tvalid_0's rmse: 12.2534\tvalid_0's tweedie: 8.44205\n",
            "Early stopping, best iteration is:\n",
            "[400]\tvalid_0's rmse: 12.1272\tvalid_0's tweedie: 8.4005\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005167 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5338\tvalid_0's tweedie: 8.82653\n",
            "[400]\tvalid_0's rmse: 12.8754\tvalid_0's tweedie: 8.69131\n",
            "[600]\tvalid_0's rmse: 13.7274\tvalid_0's tweedie: 8.71611\n",
            "Early stopping, best iteration is:\n",
            "[408]\tvalid_0's rmse: 12.8585\tvalid_0's tweedie: 8.69061\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004682 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5301\tvalid_0's tweedie: 8.66213\n",
            "[400]\tvalid_0's rmse: 15.5425\tvalid_0's tweedie: 8.62659\n",
            "Early stopping, best iteration is:\n",
            "[312]\tvalid_0's rmse: 15.287\tvalid_0's tweedie: 8.60134\n",
            "[I 2025-08-18 14:50:43,648] Trial 30 finished with value: 15.601714284343657 and parameters: {'objective': 'tweedie', 'learning_rate': 0.018860144833151928, 'n_estimators': 4000, 'num_leaves': 111, 'min_child_samples': 180, 'subsample': 0.8511566563583459, 'colsample_bytree': 0.6023944870861697, 'reg_alpha': 0.3504025064969778, 'reg_lambda': 0.7980724804326633, 'min_split_gain': 0.11331317036082889, 'tweedie_variance_power': 1.4401029275294794}. Best is trial 28 with value: 15.552718292125014.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004770 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.4616\tvalid_0's tweedie: 7.62179\n",
            "Early stopping, best iteration is:\n",
            "[154]\tvalid_0's rmse: 21.5276\tvalid_0's tweedie: 7.56489\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.014939 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5805\tvalid_0's tweedie: 6.66633\n",
            "[400]\tvalid_0's rmse: 12.3061\tvalid_0's tweedie: 6.70108\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid_0's rmse: 12.3171\tvalid_0's tweedie: 6.65791\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004760 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.181\tvalid_0's tweedie: 6.79687\n",
            "[400]\tvalid_0's rmse: 13.2786\tvalid_0's tweedie: 6.76384\n",
            "Early stopping, best iteration is:\n",
            "[253]\tvalid_0's rmse: 12.9456\tvalid_0's tweedie: 6.76965\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005073 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4874\tvalid_0's tweedie: 6.69679\n",
            "[400]\tvalid_0's rmse: 15.9213\tvalid_0's tweedie: 6.74593\n",
            "Early stopping, best iteration is:\n",
            "[253]\tvalid_0's rmse: 15.367\tvalid_0's tweedie: 6.68746\n",
            "[I 2025-08-18 14:51:18,606] Trial 31 finished with value: 15.539324203356646 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028137361886279903, 'n_estimators': 5000, 'num_leaves': 79, 'min_child_samples': 160, 'subsample': 0.8077722468415358, 'colsample_bytree': 0.6265117558338704, 'reg_alpha': 0.5499197217397466, 'reg_lambda': 0.6108654749358401, 'min_split_gain': 0.17983525496291547, 'tweedie_variance_power': 1.5570894221280926}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004709 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.8255\tvalid_0's tweedie: 7.51743\n",
            "Early stopping, best iteration is:\n",
            "[198]\tvalid_0's rmse: 21.7456\tvalid_0's tweedie: 7.5147\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004630 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7885\tvalid_0's tweedie: 6.65635\n",
            "[400]\tvalid_0's rmse: 12.1612\tvalid_0's tweedie: 6.61809\n",
            "Early stopping, best iteration is:\n",
            "[291]\tvalid_0's rmse: 12.2458\tvalid_0's tweedie: 6.60853\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004622 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5392\tvalid_0's tweedie: 6.81076\n",
            "[400]\tvalid_0's rmse: 12.9502\tvalid_0's tweedie: 6.71912\n",
            "Early stopping, best iteration is:\n",
            "[359]\tvalid_0's rmse: 12.8678\tvalid_0's tweedie: 6.72444\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004656 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.6806\tvalid_0's tweedie: 6.70387\n",
            "[400]\tvalid_0's rmse: 15.5653\tvalid_0's tweedie: 6.66898\n",
            "Early stopping, best iteration is:\n",
            "[293]\tvalid_0's rmse: 15.4107\tvalid_0's tweedie: 6.64892\n",
            "[I 2025-08-18 14:51:57,422] Trial 32 finished with value: 15.567478495327947 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02165477225753054, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 200, 'subsample': 0.7976648939696054, 'colsample_bytree': 0.6380317637801378, 'reg_alpha': 0.5426937816546932, 'reg_lambda': 0.5581625406133538, 'min_split_gain': 0.18482069967998072, 'tweedie_variance_power': 1.5606654669125544}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004542 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.2137\tvalid_0's tweedie: 7.46483\n",
            "Early stopping, best iteration is:\n",
            "[167]\tvalid_0's rmse: 21.647\tvalid_0's tweedie: 7.43192\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.005799 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5785\tvalid_0's tweedie: 6.58384\n",
            "[400]\tvalid_0's rmse: 12.304\tvalid_0's tweedie: 6.62672\n",
            "Early stopping, best iteration is:\n",
            "[225]\tvalid_0's rmse: 12.459\tvalid_0's tweedie: 6.57744\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004688 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3025\tvalid_0's tweedie: 6.72398\n",
            "[400]\tvalid_0's rmse: 13.4034\tvalid_0's tweedie: 6.679\n",
            "Early stopping, best iteration is:\n",
            "[248]\tvalid_0's rmse: 13.0124\tvalid_0's tweedie: 6.69321\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004660 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5288\tvalid_0's tweedie: 6.61761\n",
            "[400]\tvalid_0's rmse: 15.6175\tvalid_0's tweedie: 6.64157\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 15.3933\tvalid_0's tweedie: 6.59713\n",
            "[I 2025-08-18 14:52:35,548] Trial 33 finished with value: 15.627906566575716 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02642735741889656, 'n_estimators': 5000, 'num_leaves': 95, 'min_child_samples': 200, 'subsample': 0.8040296690049737, 'colsample_bytree': 0.6379648590105444, 'reg_alpha': 0.5393505866997049, 'reg_lambda': 0.6028389337719668, 'min_split_gain': 0.17961004007306078, 'tweedie_variance_power': 1.5660470129317123}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004581 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3351\tvalid_0's tweedie: 61.383\n",
            "Early stopping, best iteration is:\n",
            "[151]\tvalid_0's rmse: 21.9376\tvalid_0's tweedie: 61.4053\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004653 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4193\tvalid_0's tweedie: 42.7997\n",
            "[400]\tvalid_0's rmse: 12.1428\tvalid_0's tweedie: 42.6444\n",
            "Early stopping, best iteration is:\n",
            "[381]\tvalid_0's rmse: 12.0619\tvalid_0's tweedie: 42.645\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004971 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1668\tvalid_0's tweedie: 47.0922\n",
            "[400]\tvalid_0's rmse: 13.2349\tvalid_0's tweedie: 46.9513\n",
            "Early stopping, best iteration is:\n",
            "[277]\tvalid_0's rmse: 13.0314\tvalid_0's tweedie: 46.9899\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005761 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7157\tvalid_0's tweedie: 45.8241\n",
            "[400]\tvalid_0's rmse: 15.4878\tvalid_0's tweedie: 45.7728\n",
            "Early stopping, best iteration is:\n",
            "[313]\tvalid_0's rmse: 15.4638\tvalid_0's tweedie: 45.7473\n",
            "[I 2025-08-18 14:53:14,970] Trial 34 finished with value: 15.623677143403192 and parameters: {'objective': 'tweedie', 'learning_rate': 0.022612009364102827, 'n_estimators': 6000, 'num_leaves': 79, 'min_child_samples': 220, 'subsample': 0.7786548920460605, 'colsample_bytree': 0.6632521700087458, 'reg_alpha': 0.5556226644474986, 'reg_lambda': 0.5185004251001024, 'min_split_gain': 0.18107835622379645, 'tweedie_variance_power': 1.1234014563460408}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004475 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.9218\tvalid_0's poisson: -25.9475\n",
            "[400]\tvalid_0's rmse: 22.3001\tvalid_0's poisson: -27.4941\n",
            "[600]\tvalid_0's rmse: 22.1501\tvalid_0's poisson: -27.8392\n",
            "Early stopping, best iteration is:\n",
            "[579]\tvalid_0's rmse: 22.1106\tvalid_0's poisson: -27.8348\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004610 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.6582\tvalid_0's poisson: -12.2345\n",
            "[400]\tvalid_0's rmse: 12.933\tvalid_0's poisson: -13.4299\n",
            "[600]\tvalid_0's rmse: 12.741\tvalid_0's poisson: -13.7408\n",
            "[800]\tvalid_0's rmse: 12.5773\tvalid_0's poisson: -13.8752\n",
            "[1000]\tvalid_0's rmse: 12.4857\tvalid_0's poisson: -13.9412\n",
            "[1200]\tvalid_0's rmse: 12.3752\tvalid_0's poisson: -13.9975\n",
            "[1400]\tvalid_0's rmse: 12.3019\tvalid_0's poisson: -14.0353\n",
            "[1600]\tvalid_0's rmse: 12.2639\tvalid_0's poisson: -14.0544\n",
            "Early stopping, best iteration is:\n",
            "[1568]\tvalid_0's rmse: 12.2508\tvalid_0's poisson: -14.0576\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004703 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4827\tvalid_0's poisson: -15.554\n",
            "[400]\tvalid_0's rmse: 13.8288\tvalid_0's poisson: -16.9503\n",
            "[600]\tvalid_0's rmse: 13.3588\tvalid_0's poisson: -17.3216\n",
            "[800]\tvalid_0's rmse: 13.1758\tvalid_0's poisson: -17.4795\n",
            "[1000]\tvalid_0's rmse: 13.1422\tvalid_0's poisson: -17.5576\n",
            "[1200]\tvalid_0's rmse: 13.1354\tvalid_0's poisson: -17.6079\n",
            "Early stopping, best iteration is:\n",
            "[1140]\tvalid_0's rmse: 13.1122\tvalid_0's poisson: -17.6008\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004860 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 16.6653\tvalid_0's poisson: -14.2539\n",
            "[400]\tvalid_0's rmse: 15.9611\tvalid_0's poisson: -15.58\n",
            "[600]\tvalid_0's rmse: 15.7817\tvalid_0's poisson: -15.9438\n",
            "[800]\tvalid_0's rmse: 15.6428\tvalid_0's poisson: -16.0751\n",
            "[1000]\tvalid_0's rmse: 15.5568\tvalid_0's poisson: -16.1206\n",
            "[1200]\tvalid_0's rmse: 15.5695\tvalid_0's poisson: -16.1145\n",
            "Early stopping, best iteration is:\n",
            "[1071]\tvalid_0's rmse: 15.5439\tvalid_0's poisson: -16.1319\n",
            "[I 2025-08-18 14:55:28,790] Trial 35 finished with value: 15.754386442811253 and parameters: {'objective': 'poisson', 'learning_rate': 0.012093682500885723, 'n_estimators': 3000, 'num_leaves': 127, 'min_child_samples': 180, 'subsample': 0.7554850395503827, 'colsample_bytree': 0.6309302518284194, 'reg_alpha': 0.5145737989471706, 'reg_lambda': 1.2798135734854434, 'min_split_gain': 0.16136184754596575}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.009135 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.0547\tvalid_0's l2: 531.52\n",
            "[400]\tvalid_0's rmse: 22.8056\tvalid_0's l2: 520.096\n",
            "[600]\tvalid_0's rmse: 22.6986\tvalid_0's l2: 515.226\n",
            "[800]\tvalid_0's rmse: 22.6297\tvalid_0's l2: 512.103\n",
            "[1000]\tvalid_0's rmse: 22.5832\tvalid_0's l2: 510.001\n",
            "[1200]\tvalid_0's rmse: 22.5904\tvalid_0's l2: 510.324\n",
            "Early stopping, best iteration is:\n",
            "[1070]\tvalid_0's rmse: 22.5594\tvalid_0's l2: 508.925\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004638 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3884\tvalid_0's l2: 179.25\n",
            "[400]\tvalid_0's rmse: 13.3691\tvalid_0's l2: 178.733\n",
            "[600]\tvalid_0's rmse: 13.3602\tvalid_0's l2: 178.495\n",
            "[800]\tvalid_0's rmse: 13.2901\tvalid_0's l2: 176.626\n",
            "[1000]\tvalid_0's rmse: 13.28\tvalid_0's l2: 176.358\n",
            "[1200]\tvalid_0's rmse: 13.2271\tvalid_0's l2: 174.955\n",
            "[1400]\tvalid_0's rmse: 13.2584\tvalid_0's l2: 175.785\n",
            "Early stopping, best iteration is:\n",
            "[1281]\tvalid_0's rmse: 13.2044\tvalid_0's l2: 174.355\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004831 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.8674\tvalid_0's l2: 192.305\n",
            "[400]\tvalid_0's rmse: 13.4211\tvalid_0's l2: 180.125\n",
            "[600]\tvalid_0's rmse: 13.3416\tvalid_0's l2: 177.997\n",
            "Early stopping, best iteration is:\n",
            "[589]\tvalid_0's rmse: 13.3297\tvalid_0's l2: 177.68\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9119\tvalid_0's l2: 253.188\n",
            "[400]\tvalid_0's rmse: 15.7946\tvalid_0's l2: 249.469\n",
            "[600]\tvalid_0's rmse: 15.7102\tvalid_0's l2: 246.812\n",
            "[800]\tvalid_0's rmse: 15.7345\tvalid_0's l2: 247.573\n",
            "Early stopping, best iteration is:\n",
            "[650]\tvalid_0's rmse: 15.698\tvalid_0's l2: 246.427\n",
            "[I 2025-08-18 14:57:10,280] Trial 36 finished with value: 16.197847155164563 and parameters: {'objective': 'regression', 'learning_rate': 0.01733145217665259, 'n_estimators': 5000, 'num_leaves': 95, 'min_child_samples': 200, 'subsample': 0.7935336009702659, 'colsample_bytree': 0.6716044271741759, 'reg_alpha': 0.4753879184663926, 'reg_lambda': 1.5440667313431695, 'min_split_gain': 0.19904718761440543}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004584 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.9813\tvalid_0's tweedie: 7.52842\n",
            "[400]\tvalid_0's rmse: 22.998\tvalid_0's tweedie: 7.50104\n",
            "Early stopping, best iteration is:\n",
            "[302]\tvalid_0's rmse: 21.7898\tvalid_0's tweedie: 7.44679\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004638 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1636\tvalid_0's tweedie: 6.68175\n",
            "[400]\tvalid_0's rmse: 12.6637\tvalid_0's tweedie: 6.55774\n",
            "[600]\tvalid_0's rmse: 12.2992\tvalid_0's tweedie: 6.54785\n",
            "[800]\tvalid_0's rmse: 12.1867\tvalid_0's tweedie: 6.56997\n",
            "Early stopping, best iteration is:\n",
            "[601]\tvalid_0's rmse: 12.2999\tvalid_0's tweedie: 6.54784\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.009444 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 14.5782\tvalid_0's tweedie: 6.88401\n",
            "[400]\tvalid_0's rmse: 13.105\tvalid_0's tweedie: 6.72086\n",
            "[600]\tvalid_0's rmse: 13.264\tvalid_0's tweedie: 6.69161\n",
            "Early stopping, best iteration is:\n",
            "[460]\tvalid_0's rmse: 12.9824\tvalid_0's tweedie: 6.70573\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004613 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.986\tvalid_0's tweedie: 6.7453\n",
            "[400]\tvalid_0's rmse: 15.4969\tvalid_0's tweedie: 6.61543\n",
            "[600]\tvalid_0's rmse: 15.4915\tvalid_0's tweedie: 6.60722\n",
            "Early stopping, best iteration is:\n",
            "[425]\tvalid_0's rmse: 15.4301\tvalid_0's tweedie: 6.60738\n",
            "[I 2025-08-18 14:57:55,723] Trial 37 finished with value: 15.625556581692956 and parameters: {'objective': 'tweedie', 'learning_rate': 0.01533527361866902, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 220, 'subsample': 0.8139100372738405, 'colsample_bytree': 0.6945374385254158, 'reg_alpha': 0.013679988917289054, 'reg_lambda': 0.7523716700898821, 'min_split_gain': 0.1873929702589432, 'tweedie_variance_power': 1.564907485256111}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004829 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.3269\tvalid_0's poisson: -27.4915\n",
            "[400]\tvalid_0's rmse: 22.6922\tvalid_0's poisson: -27.7791\n",
            "Early stopping, best iteration is:\n",
            "[300]\tvalid_0's rmse: 22.0906\tvalid_0's poisson: -27.8735\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004617 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8377\tvalid_0's poisson: -13.4424\n",
            "[400]\tvalid_0's rmse: 12.4889\tvalid_0's poisson: -13.9065\n",
            "[600]\tvalid_0's rmse: 12.2742\tvalid_0's poisson: -14.0048\n",
            "[800]\tvalid_0's rmse: 12.3594\tvalid_0's poisson: -14.0112\n",
            "Early stopping, best iteration is:\n",
            "[626]\tvalid_0's rmse: 12.2413\tvalid_0's poisson: -14.0184\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004640 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.7374\tvalid_0's poisson: -16.9702\n",
            "[400]\tvalid_0's rmse: 13.0007\tvalid_0's poisson: -17.5323\n",
            "[600]\tvalid_0's rmse: 13.0248\tvalid_0's poisson: -17.6345\n",
            "Early stopping, best iteration is:\n",
            "[499]\tvalid_0's rmse: 12.9359\tvalid_0's poisson: -17.611\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004872 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.868\tvalid_0's poisson: -15.6214\n",
            "[400]\tvalid_0's rmse: 15.4971\tvalid_0's poisson: -16.1628\n",
            "[600]\tvalid_0's rmse: 15.3735\tvalid_0's poisson: -16.2154\n",
            "Early stopping, best iteration is:\n",
            "[542]\tvalid_0's rmse: 15.3387\tvalid_0's poisson: -16.2258\n",
            "[I 2025-08-18 14:59:08,690] Trial 38 finished with value: 15.651621446532145 and parameters: {'objective': 'poisson', 'learning_rate': 0.0239924392375826, 'n_estimators': 9000, 'num_leaves': 127, 'min_child_samples': 160, 'subsample': 0.7812672359192636, 'colsample_bytree': 0.6113743589855611, 'reg_alpha': 0.43284538485031004, 'reg_lambda': 0.9637111634714178, 'min_split_gain': 0.07747011933781972}. Best is trial 31 with value: 15.539324203356646.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004736 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.6502\tvalid_0's tweedie: 7.681\n",
            "Early stopping, best iteration is:\n",
            "[196]\tvalid_0's rmse: 21.5234\tvalid_0's tweedie: 7.67672\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004649 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7647\tvalid_0's tweedie: 6.76756\n",
            "[400]\tvalid_0's rmse: 12.2591\tvalid_0's tweedie: 6.73692\n",
            "Early stopping, best iteration is:\n",
            "[377]\tvalid_0's rmse: 12.2473\tvalid_0's tweedie: 6.73238\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004661 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5093\tvalid_0's tweedie: 6.93755\n",
            "[400]\tvalid_0's rmse: 12.903\tvalid_0's tweedie: 6.8568\n",
            "Early stopping, best iteration is:\n",
            "[335]\tvalid_0's rmse: 12.8429\tvalid_0's tweedie: 6.86572\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004656 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7122\tvalid_0's tweedie: 6.82597\n",
            "[400]\tvalid_0's rmse: 15.6171\tvalid_0's tweedie: 6.78841\n",
            "Early stopping, best iteration is:\n",
            "[295]\tvalid_0's rmse: 15.3957\tvalid_0's tweedie: 6.77039\n",
            "[I 2025-08-18 14:59:49,319] Trial 39 finished with value: 15.502324071033339 and parameters: {'objective': 'tweedie', 'learning_rate': 0.021763926218126194, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.8013288172026912, 'colsample_bytree': 0.6512079143368947, 'reg_alpha': 0.5432353580838567, 'reg_lambda': 0.6509611800271689, 'min_split_gain': 0.1703228123043471, 'tweedie_variance_power': 1.5482132085418658}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004730 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.998381\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.5088\tvalid_0's l2: 506.648\n",
            "[400]\tvalid_0's rmse: 22.305\tvalid_0's l2: 497.515\n",
            "Early stopping, best iteration is:\n",
            "[376]\tvalid_0's rmse: 22.2628\tvalid_0's l2: 495.633\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004585 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.922044\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8834\tvalid_0's l2: 165.983\n",
            "[400]\tvalid_0's rmse: 12.9352\tvalid_0's l2: 167.319\n",
            "Early stopping, best iteration is:\n",
            "[242]\tvalid_0's rmse: 12.8681\tvalid_0's l2: 165.589\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004640 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.953923\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.7579\tvalid_0's l2: 189.279\n",
            "[400]\tvalid_0's rmse: 13.2909\tvalid_0's l2: 176.648\n",
            "[600]\tvalid_0's rmse: 13.2751\tvalid_0's l2: 176.227\n",
            "[800]\tvalid_0's rmse: 13.2999\tvalid_0's l2: 176.887\n",
            "Early stopping, best iteration is:\n",
            "[653]\tvalid_0's rmse: 13.2493\tvalid_0's l2: 175.545\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005426 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 10.928096\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.8703\tvalid_0's l2: 251.868\n",
            "[400]\tvalid_0's rmse: 15.8211\tvalid_0's l2: 250.307\n",
            "Early stopping, best iteration is:\n",
            "[302]\tvalid_0's rmse: 15.7871\tvalid_0's l2: 249.233\n",
            "[I 2025-08-18 15:00:45,648] Trial 40 finished with value: 16.041854342798107 and parameters: {'objective': 'regression', 'learning_rate': 0.01870752948014187, 'n_estimators': 6000, 'num_leaves': 111, 'min_child_samples': 160, 'subsample': 0.8312831447082484, 'colsample_bytree': 0.6530437338065294, 'reg_alpha': 0.3827319650562997, 'reg_lambda': 1.7412686750685165, 'min_split_gain': 0.15411054370065966}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004581 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 21.9523\tvalid_0's tweedie: 7.67099\n",
            "Early stopping, best iteration is:\n",
            "[192]\tvalid_0's rmse: 21.8418\tvalid_0's tweedie: 7.66762\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004829 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6879\tvalid_0's tweedie: 6.74242\n",
            "[400]\tvalid_0's rmse: 12.1058\tvalid_0's tweedie: 6.6998\n",
            "Early stopping, best iteration is:\n",
            "[380]\tvalid_0's rmse: 12.0823\tvalid_0's tweedie: 6.69447\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004733 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.5426\tvalid_0's tweedie: 6.92716\n",
            "[400]\tvalid_0's rmse: 13.0738\tvalid_0's tweedie: 6.84668\n",
            "Early stopping, best iteration is:\n",
            "[286]\tvalid_0's rmse: 12.9914\tvalid_0's tweedie: 6.86796\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004639 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.7671\tvalid_0's tweedie: 6.8187\n",
            "[400]\tvalid_0's rmse: 15.6191\tvalid_0's tweedie: 6.80014\n",
            "Early stopping, best iteration is:\n",
            "[293]\tvalid_0's rmse: 15.4648\tvalid_0's tweedie: 6.76748\n",
            "[I 2025-08-18 15:01:24,936] Trial 41 finished with value: 15.595058039864238 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02155450506421104, 'n_estimators': 4000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.8033775699521416, 'colsample_bytree': 0.6250951080308461, 'reg_alpha': 0.5497024161174298, 'reg_lambda': 0.677166818978517, 'min_split_gain': 0.17341732123537668, 'tweedie_variance_power': 1.5494162738464876}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005132 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.2092\tvalid_0's tweedie: 7.13399\n",
            "Early stopping, best iteration is:\n",
            "[166]\tvalid_0's rmse: 21.8384\tvalid_0's tweedie: 7.09077\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004639 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.7033\tvalid_0's tweedie: 6.35112\n",
            "[400]\tvalid_0's rmse: 12.2724\tvalid_0's tweedie: 6.38145\n",
            "Early stopping, best iteration is:\n",
            "[249]\tvalid_0's rmse: 12.4638\tvalid_0's tweedie: 6.3367\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004602 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1627\tvalid_0's tweedie: 6.46662\n",
            "[400]\tvalid_0's rmse: 13.519\tvalid_0's tweedie: 6.43022\n",
            "Early stopping, best iteration is:\n",
            "[248]\tvalid_0's rmse: 13.0299\tvalid_0's tweedie: 6.43885\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.015844 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4568\tvalid_0's tweedie: 6.36387\n",
            "[400]\tvalid_0's rmse: 15.5875\tvalid_0's tweedie: 6.39203\n",
            "Early stopping, best iteration is:\n",
            "[243]\tvalid_0's rmse: 15.3971\tvalid_0's tweedie: 6.35225\n",
            "[I 2025-08-18 15:02:03,604] Trial 42 finished with value: 15.682310597511794 and parameters: {'objective': 'tweedie', 'learning_rate': 0.025914247788004472, 'n_estimators': 4000, 'num_leaves': 95, 'min_child_samples': 200, 'subsample': 0.7900140373348935, 'colsample_bytree': 0.6628630944711797, 'reg_alpha': 0.5373399425833484, 'reg_lambda': 0.7217055143676814, 'min_split_gain': 0.16865514502739185, 'tweedie_variance_power': 1.5967189551241456}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004758 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.0066\tvalid_0's tweedie: 9.63657\n",
            "Early stopping, best iteration is:\n",
            "[191]\tvalid_0's rmse: 21.7425\tvalid_0's tweedie: 9.62509\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004574 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.6365\tvalid_0's tweedie: 8.12102\n",
            "[400]\tvalid_0's rmse: 12.0354\tvalid_0's tweedie: 8.07528\n",
            "[600]\tvalid_0's rmse: 11.9844\tvalid_0's tweedie: 8.08861\n",
            "Early stopping, best iteration is:\n",
            "[449]\tvalid_0's rmse: 12.0549\tvalid_0's tweedie: 8.07402\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004618 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.3646\tvalid_0's tweedie: 8.43859\n",
            "[400]\tvalid_0's rmse: 13.0833\tvalid_0's tweedie: 8.34911\n",
            "Early stopping, best iteration is:\n",
            "[349]\tvalid_0's rmse: 13.0153\tvalid_0's tweedie: 8.35499\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.031362 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5604\tvalid_0's tweedie: 8.26147\n",
            "[400]\tvalid_0's rmse: 15.4428\tvalid_0's tweedie: 8.24431\n",
            "Early stopping, best iteration is:\n",
            "[313]\tvalid_0's rmse: 15.3018\tvalid_0's tweedie: 8.2125\n",
            "[I 2025-08-18 15:02:40,990] Trial 43 finished with value: 15.528627439885279 and parameters: {'objective': 'tweedie', 'learning_rate': 0.02302049491154098, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 180, 'subsample': 0.8155291330556915, 'colsample_bytree': 0.6450511878916859, 'reg_alpha': 0.5630292507458894, 'reg_lambda': 1.1936307829956565, 'min_split_gain': 0.18985145967360373, 'tweedie_variance_power': 1.4562762917677652}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004605 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.1849\tvalid_0's poisson: -27.4471\n",
            "[400]\tvalid_0's rmse: 22.4865\tvalid_0's poisson: -27.7991\n",
            "Early stopping, best iteration is:\n",
            "[315]\tvalid_0's rmse: 21.9466\tvalid_0's poisson: -27.8728\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004830 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.9121\tvalid_0's poisson: -13.311\n",
            "[400]\tvalid_0's rmse: 12.6389\tvalid_0's poisson: -13.8035\n",
            "[600]\tvalid_0's rmse: 12.4297\tvalid_0's poisson: -13.9171\n",
            "[800]\tvalid_0's rmse: 12.4288\tvalid_0's poisson: -13.9505\n",
            "[1000]\tvalid_0's rmse: 12.3376\tvalid_0's poisson: -13.9999\n",
            "[1200]\tvalid_0's rmse: 12.2297\tvalid_0's poisson: -14.0373\n",
            "[1400]\tvalid_0's rmse: 12.194\tvalid_0's poisson: -14.0572\n",
            "[1600]\tvalid_0's rmse: 12.1845\tvalid_0's poisson: -14.059\n",
            "[1800]\tvalid_0's rmse: 12.1785\tvalid_0's poisson: -14.0645\n",
            "Early stopping, best iteration is:\n",
            "[1678]\tvalid_0's rmse: 12.1595\tvalid_0's poisson: -14.0706\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004688 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.9434\tvalid_0's poisson: -16.8464\n",
            "[400]\tvalid_0's rmse: 13.3063\tvalid_0's poisson: -17.4231\n",
            "[600]\tvalid_0's rmse: 13.2768\tvalid_0's poisson: -17.5466\n",
            "Early stopping, best iteration is:\n",
            "[489]\tvalid_0's rmse: 13.2375\tvalid_0's poisson: -17.5041\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004713 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.9939\tvalid_0's poisson: -15.4814\n",
            "[400]\tvalid_0's rmse: 15.6562\tvalid_0's poisson: -16.0649\n",
            "[600]\tvalid_0's rmse: 15.4446\tvalid_0's poisson: -16.1843\n",
            "Early stopping, best iteration is:\n",
            "[594]\tvalid_0's rmse: 15.434\tvalid_0's poisson: -16.1903\n",
            "[I 2025-08-18 15:03:45,350] Trial 44 finished with value: 15.694400728567345 and parameters: {'objective': 'poisson', 'learning_rate': 0.023524608228295905, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 160, 'subsample': 0.8189440213996103, 'colsample_bytree': 0.6761403954751349, 'reg_alpha': 0.5735234243654428, 'reg_lambda': 1.3665024231767393, 'min_split_gain': 0.13532870056396548}. Best is trial 39 with value: 15.502324071033339.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004592 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.9263\tvalid_0's tweedie: 9.68617\n",
            "Early stopping, best iteration is:\n",
            "[158]\tvalid_0's rmse: 21.7656\tvalid_0's tweedie: 9.63855\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004653 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5025\tvalid_0's tweedie: 8.10962\n",
            "[400]\tvalid_0's rmse: 12.09\tvalid_0's tweedie: 8.0925\n",
            "Early stopping, best iteration is:\n",
            "[312]\tvalid_0's rmse: 12.1583\tvalid_0's tweedie: 8.07795\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004799 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0298\tvalid_0's tweedie: 8.39139\n",
            "[400]\tvalid_0's rmse: 13.2242\tvalid_0's tweedie: 8.34777\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's rmse: 12.749\tvalid_0's tweedie: 8.35893\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004797 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4912\tvalid_0's tweedie: 8.26681\n",
            "[400]\tvalid_0's rmse: 15.553\tvalid_0's tweedie: 8.28507\n",
            "Early stopping, best iteration is:\n",
            "[253]\tvalid_0's rmse: 15.3026\tvalid_0's tweedie: 8.24146\n",
            "[I 2025-08-18 15:04:18,729] Trial 45 finished with value: 15.493894642896864 and parameters: {'objective': 'tweedie', 'learning_rate': 0.028705110579715018, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 180, 'subsample': 0.7719427879990697, 'colsample_bytree': 0.6514546806013081, 'reg_alpha': 0.4751504347734776, 'reg_lambda': 1.1352173486363917, 'min_split_gain': 0.11118727178963116, 'tweedie_variance_power': 1.4556431079977874}. Best is trial 45 with value: 15.493894642896864.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004584 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.0874\tvalid_0's tweedie: 9.78395\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 21.6813\tvalid_0's tweedie: 9.71285\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004654 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.4719\tvalid_0's tweedie: 8.13373\n",
            "[400]\tvalid_0's rmse: 12.3665\tvalid_0's tweedie: 8.1453\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 12.3734\tvalid_0's tweedie: 8.12208\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004873 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0996\tvalid_0's tweedie: 8.43025\n",
            "[400]\tvalid_0's rmse: 13.6189\tvalid_0's tweedie: 8.39796\n",
            "Early stopping, best iteration is:\n",
            "[237]\tvalid_0's rmse: 12.8976\tvalid_0's tweedie: 8.40838\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.005027 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4525\tvalid_0's tweedie: 8.28459\n",
            "[400]\tvalid_0's rmse: 15.9327\tvalid_0's tweedie: 8.36352\n",
            "Early stopping, best iteration is:\n",
            "[233]\tvalid_0's rmse: 15.4224\tvalid_0's tweedie: 8.27496\n",
            "[I 2025-08-18 15:04:47,941] Trial 46 finished with value: 15.593697454615391 and parameters: {'objective': 'tweedie', 'learning_rate': 0.03029626654969191, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 180, 'subsample': 0.7726754974066526, 'colsample_bytree': 0.6123747388228684, 'reg_alpha': 0.599223394705859, 'reg_lambda': 1.2029895331368916, 'min_split_gain': 0.1905858423481282, 'tweedie_variance_power': 1.4538638875780159}. Best is trial 45 with value: 15.493894642896864.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.005029 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.7218\tvalid_0's tweedie: 9.51032\n",
            "Early stopping, best iteration is:\n",
            "[162]\tvalid_0's rmse: 22.0431\tvalid_0's tweedie: 9.48826\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004704 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.5216\tvalid_0's tweedie: 7.99005\n",
            "[400]\tvalid_0's rmse: 12.1504\tvalid_0's tweedie: 7.96825\n",
            "Early stopping, best iteration is:\n",
            "[333]\tvalid_0's rmse: 12.188\tvalid_0's tweedie: 7.96334\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004612 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.0317\tvalid_0's tweedie: 8.28234\n",
            "[400]\tvalid_0's rmse: 13.3471\tvalid_0's tweedie: 8.24157\n",
            "Early stopping, best iteration is:\n",
            "[249]\tvalid_0's rmse: 12.8939\tvalid_0's tweedie: 8.25427\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004736 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.5075\tvalid_0's tweedie: 8.13193\n",
            "[400]\tvalid_0's rmse: 15.6348\tvalid_0's tweedie: 8.16657\n",
            "Early stopping, best iteration is:\n",
            "[244]\tvalid_0's rmse: 15.3451\tvalid_0's tweedie: 8.1154\n",
            "[I 2025-08-18 15:05:24,911] Trial 47 finished with value: 15.617552962987451 and parameters: {'objective': 'tweedie', 'learning_rate': 0.027987282221295946, 'n_estimators': 8000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.7606994584762871, 'colsample_bytree': 0.6492272084386531, 'reg_alpha': 0.48438169382805074, 'reg_lambda': 0.9037867799339577, 'min_split_gain': 0.15893685960982876, 'tweedie_variance_power': 1.4610152107337113}. Best is trial 45 with value: 15.493894642896864.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004923 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 22.1173\tvalid_0's tweedie: 8.03778\n",
            "Early stopping, best iteration is:\n",
            "[162]\tvalid_0's rmse: 21.6894\tvalid_0's tweedie: 8.02454\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004624 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.8028\tvalid_0's tweedie: 6.98346\n",
            "[400]\tvalid_0's rmse: 12.1229\tvalid_0's tweedie: 6.95014\n",
            "Early stopping, best iteration is:\n",
            "[262]\tvalid_0's rmse: 12.4128\tvalid_0's tweedie: 6.94635\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.004704 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.1884\tvalid_0's tweedie: 7.17227\n",
            "[400]\tvalid_0's rmse: 13.1814\tvalid_0's tweedie: 7.11732\n",
            "Early stopping, best iteration is:\n",
            "[231]\tvalid_0's rmse: 13.0087\tvalid_0's tweedie: 7.14699\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.008055 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.3883\tvalid_0's tweedie: 7.03456\n",
            "[400]\tvalid_0's rmse: 15.7071\tvalid_0's tweedie: 7.06238\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 15.3042\tvalid_0's tweedie: 7.01463\n",
            "[I 2025-08-18 15:05:54,520] Trial 48 finished with value: 15.603783766241534 and parameters: {'objective': 'tweedie', 'learning_rate': 0.026452314963219137, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 160, 'subsample': 0.810460212701745, 'colsample_bytree': 0.6933380673140519, 'reg_alpha': 0.5154078184503448, 'reg_lambda': 1.0844190806275251, 'min_split_gain': 0.12640260829163122, 'tweedie_variance_power': 1.5267782247302666}. Best is trial 45 with value: 15.493894642896864.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 86464, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (1.98 MB) transferred to GPU in 0.004578 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.397748\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 23.9363\tvalid_0's tweedie: 10.9689\n",
            "Early stopping, best iteration is:\n",
            "[109]\tvalid_0's rmse: 21.7001\tvalid_0's tweedie: 10.7907\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2917\n",
            "[LightGBM] [Info] Number of data points in the train set: 89166, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.04 MB) transferred to GPU in 0.004573 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.390783\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 12.1061\tvalid_0's tweedie: 8.87828\n",
            "[400]\tvalid_0's rmse: 12.0853\tvalid_0's tweedie: 8.93616\n",
            "Early stopping, best iteration is:\n",
            "[227]\tvalid_0's rmse: 12.0834\tvalid_0's tweedie: 8.86719\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2919\n",
            "[LightGBM] [Info] Number of data points in the train set: 91868, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.10 MB) transferred to GPU in 0.005352 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.393698\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 13.2604\tvalid_0's tweedie: 9.24167\n",
            "[400]\tvalid_0's rmse: 13.9359\tvalid_0's tweedie: 9.25605\n",
            "Early stopping, best iteration is:\n",
            "[215]\tvalid_0's rmse: 13.1391\tvalid_0's tweedie: 9.23138\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2921\n",
            "[LightGBM] [Info] Number of data points in the train set: 94570, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 21 dense feature groups (2.16 MB) transferred to GPU in 0.004767 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.391337\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\tvalid_0's rmse: 15.4433\tvalid_0's tweedie: 9.09823\n",
            "Early stopping, best iteration is:\n",
            "[191]\tvalid_0's rmse: 15.3212\tvalid_0's tweedie: 9.08563\n",
            "[I 2025-08-18 15:06:26,920] Trial 49 finished with value: 15.560956553362553 and parameters: {'objective': 'tweedie', 'learning_rate': 0.040298478320279295, 'n_estimators': 5000, 'num_leaves': 79, 'min_child_samples': 180, 'subsample': 0.7721308537223036, 'colsample_bytree': 0.6289251699807132, 'reg_alpha': 0.5629069390251383, 'reg_lambda': 0.788638325341419, 'min_split_gain': 0.1065286090569239, 'tweedie_variance_power': 1.4205275174704441}. Best is trial 45 with value: 15.493894642896864.\n",
            "Best RMSE : 15.493894642896864\n",
            "Best Params: {'objective': 'tweedie', 'learning_rate': 0.028705110579715018, 'n_estimators': 3000, 'num_leaves': 63, 'min_child_samples': 180, 'subsample': 0.7719427879990697, 'colsample_bytree': 0.6514546806013081, 'reg_alpha': 0.4751504347734776, 'reg_lambda': 1.1352173486363917, 'min_split_gain': 0.11118727178963116, 'tweedie_variance_power': 1.4556431079977874}\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 2927\n",
            "[LightGBM] [Info] Number of data points in the train set: 102483, number of used features: 28\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (2.35 MB) transferred to GPU in 0.005343 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 2.364153\n",
            "[500]\ttraining's rmse: 13.7259\ttraining's tweedie: 8.45222\n",
            "[1000]\ttraining's rmse: 11.0631\ttraining's tweedie: 8.28716\n",
            "[1500]\ttraining's rmse: 9.47692\ttraining's tweedie: 8.1955\n",
            "[2000]\ttraining's rmse: 8.50897\ttraining's tweedie: 8.13201\n",
            "[2500]\ttraining's rmse: 7.75124\ttraining's tweedie: 8.08265\n",
            "[3000]\ttraining's rmse: 7.17651\ttraining's tweedie: 8.04418\n",
            "✅ Saved -> /content/drive/MyDrive/lg_aimers_2/models/lgbm_04_optuna.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lightgbm -q\n",
        "import os, glob, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# ===== 경로 설정 =====\n",
        "MODEL_PATH   = \"/content/drive/MyDrive/lg_aimers_2/models/lgbm_04_optuna.pkl\"\n",
        "TEST_DIR     = \"/content/drive/MyDrive/lg_aimers_2/data/test\"   # TEST_00.csv ~ TEST_09.csv\n",
        "SAMPLE_PATH  = \"/content/drive/MyDrive/lg_aimers_2/data/sample_submission.csv\"  # 필요 시 수정\n",
        "OUT_PATH     = \"/content/drive/MyDrive/lg_aimers_2/submission_lightgbm_04.csv\"\n",
        "\n",
        "# ===== 모델 로드 =====\n",
        "bundle = joblib.load(MODEL_PATH)\n",
        "model: LGBMRegressor = bundle[\"model\"]\n",
        "use_cols = bundle[\"features\"]\n",
        "cat_idx  = bundle[\"cat_idx\"]\n",
        "cat_col  = use_cols[cat_idx[0]]\n",
        "\n",
        "# ===== 공휴일 계산: holidays 라이브러리 사용 =====\n",
        "try:\n",
        "    import holidays\n",
        "except ModuleNotFoundError:\n",
        "    # Colab 등에서 미설치 시\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"holidays\", \"-q\"], check=True)\n",
        "    import holidays\n",
        "\n",
        "# 연도별 캐시(테스트 파일마다 연도가 다를 수 있으므로 지연 생성)\n",
        "_HOL_CACHE = {}\n",
        "def is_holiday(ts: pd.Timestamp) -> bool:\n",
        "    y = int(ts.year)\n",
        "    if y not in _HOL_CACHE:\n",
        "        try:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y], language=\"ko\")\n",
        "        except Exception:\n",
        "            _HOL_CACHE[y] = holidays.KR(years=[y])\n",
        "    return ts.date() in _HOL_CACHE[y]\n",
        "\n",
        "# ===== 출시 정보 (전처리와 동일 규칙 사용: dict 기반) =====\n",
        "launch_dates = {\n",
        "    '느티나무 셀프BBQ_1인 수저세트': '2023-01-17', '느티나무 셀프BBQ_BBQ55(단체)': '2023-01-05',\n",
        "    '느티나무 셀프BBQ_대여료 90,000원': '2023-01-02', '느티나무 셀프BBQ_본삼겹 (단품,실내)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_스프라이트 (단체)': '2023-01-03', '느티나무 셀프BBQ_신라면': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_쌈야채세트': '2023-01-11', '느티나무 셀프BBQ_쌈장': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_육개장 사발면': '2023-04-14', '느티나무 셀프BBQ_일회용 소주컵': '2023-01-23',\n",
        "    '느티나무 셀프BBQ_일회용 종이컵': '2023-01-22', '느티나무 셀프BBQ_잔디그늘집 대여료 (12인석)': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_잔디그늘집 대여료 (6인석)': '2023-01-05', '느티나무 셀프BBQ_잔디그늘집 의자 추가': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_참이슬 (단체)': '2023-01-03', '느티나무 셀프BBQ_친환경 접시 14cm': '2023-01-22',\n",
        "    '느티나무 셀프BBQ_친환경 접시 23cm': '2023-01-05', '느티나무 셀프BBQ_카스 병(단체)': '2023-01-03',\n",
        "    '느티나무 셀프BBQ_콜라 (단체)': '2023-01-03', '느티나무 셀프BBQ_햇반': '2023-04-14',\n",
        "    '느티나무 셀프BBQ_허브솔트': '2023-04-14', '담하_(단체) 공깃밥': '2023-03-13',\n",
        "    '담하_(단체) 생목살 김치전골 2.0': '2023-09-18', '담하_(단체) 은이버섯 갈비탕': '2023-06-12',\n",
        "    '담하_(단체) 한우 우거지 국밥': '2023-01-06', '담하_(단체) 황태해장국 3/27까지': '2023-01-07',\n",
        "    '담하_(정식) 된장찌개': '2023-06-03', '담하_(정식) 물냉면 ': '2023-06-03',\n",
        "    '담하_(정식) 비빔냉면': '2023-06-03', '담하_(후식) 물냉면': '2023-06-02',\n",
        "    '담하_(후식) 비빔냉면': '2023-06-02', '담하_갑오징어 비빔밥': '2023-03-17',\n",
        "    '담하_갱시기': '2023-12-08', '담하_꼬막 비빔밥': '2023-09-08',\n",
        "    '담하_느린마을 막걸리': '2023-01-02', '담하_담하 한우 불고기 정식': '2023-06-02',\n",
        "    '담하_더덕 한우 지짐': '2023-09-09', '담하_라면사리': '2023-01-04',\n",
        "    '담하_룸 이용료': '2023-01-03', '담하_명인안동소주': '2023-07-01',\n",
        "    '담하_명태회 비빔냉면': '2023-06-02', '담하_문막 복분자 칵테일': '2023-09-12',\n",
        "    '담하_봉평메밀 물냉면': '2023-06-02', '담하_제로콜라': '2023-01-05',\n",
        "    '담하_처음처럼': '2023-01-03', '담하_하동 매실 칵테일': '2023-03-18',\n",
        "    '라그로타_AUS (200g)': '2023-12-08', '라그로타_G-Charge(3)': '2023-01-02',\n",
        "    '라그로타_Open Food': '2023-01-07', '라그로타_그릴드 비프 샐러드': '2023-09-08',\n",
        "    '라그로타_까르보나라': '2023-12-08', '라그로타_모둠 해산물 플래터': '2023-09-09',\n",
        "    '라그로타_미션 서드 카베르네 쉬라': '2023-01-02', '라그로타_버섯 크림 리조또': '2023-12-08',\n",
        "    '라그로타_시저 샐러드 ': '2023-09-08', '라그로타_알리오 에 올리오 ': '2023-09-08',\n",
        "    '라그로타_양갈비 (4ps)': '2023-09-10', '라그로타_한우 (200g)': '2023-12-09',\n",
        "    '라그로타_해산물 토마토 스튜 파스타': '2023-12-08',\n",
        "    '미라시아_(단체)브런치주중 36,000': '2023-01-03',\n",
        "    '미라시아_(오븐) 하와이안 쉬림프 피자': '2023-09-09', '미라시아_BBQ 고기추가': '2023-01-05',\n",
        "    '미라시아_글라스와인 (레드)': '2023-01-02', '미라시아_레인보우칵테일(알코올)': '2023-01-02',\n",
        "    '미라시아_버드와이저(무제한)': '2023-04-21', '미라시아_보일링 랍스타 플래터': '2023-06-05',\n",
        "    '미라시아_보일링 랍스타 플래터(덜매운맛)': '2023-06-03', '미라시아_브런치(대인) 주중': '2023-01-02',\n",
        "    '미라시아_쉬림프 투움바 파스타': '2023-06-03', '미라시아_스텔라(무제한)': '2023-04-21',\n",
        "    '미라시아_스프라이트': '2023-06-02', '미라시아_얼그레이 하이볼': '2023-01-02',\n",
        "    '미라시아_유자 하이볼': '2023-03-17', '미라시아_잭 애플 토닉': '2023-09-09',\n",
        "    '미라시아_칠리 치즈 프라이': '2023-06-03', '미라시아_코카콜라': '2023-06-02',\n",
        "    '미라시아_코카콜라(제로)': '2023-06-12', '미라시아_콥 샐러드': '2023-12-08',\n",
        "    '미라시아_파스타면 추가(150g)': '2023-06-03', '미라시아_핑크레몬에이드': '2023-03-17',\n",
        "    '연회장_Cass Beer': '2023-01-06', '연회장_Conference L1': '2023-01-03',\n",
        "    '연회장_Conference L2': '2023-01-11', '연회장_Conference L3': '2023-01-05',\n",
        "    '연회장_Conference M1': '2023-01-06', '연회장_Conference M8': '2023-01-09',\n",
        "    '연회장_Conference M9': '2023-01-06', '연회장_Convention Hall': '2023-01-03',\n",
        "    '연회장_Cookie Platter': '2023-01-09', '연회장_Grand Ballroom': '2023-01-06',\n",
        "    '연회장_OPUS 2': '2023-01-05', '연회장_Regular Coffee': '2023-02-24',\n",
        "    '연회장_공깃밥': '2023-07-21', '연회장_마라샹궈': '2023-09-08',\n",
        "    '연회장_매콤 무뼈닭발&계란찜': '2023-01-02', '연회장_삼겹살추가 (200g)': '2023-07-21',\n",
        "    '연회장_왕갈비치킨': '2023-07-22', '카페테리아_단체식 13000(신)': '2023-04-18',\n",
        "    '카페테리아_단체식 18000(신)': '2023-04-05', '카페테리아_진사골 설렁탕': '2023-12-06',\n",
        "    '카페테리아_한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '2023-03-17',\n",
        "    '화담숲주막_느린마을 막걸리': '2023-03-31', '화담숲주막_단호박 식혜 ': '2023-03-31',\n",
        "    '화담숲주막_병천순대': '2023-03-31', '화담숲주막_스프라이트': '2023-03-31',\n",
        "    '화담숲주막_참살이 막걸리': '2023-03-31', '화담숲주막_찹쌀식혜': '2023-03-31',\n",
        "    '화담숲주막_콜라': '2023-03-31', '화담숲주막_해물파전': '2023-03-31',\n",
        "    '화담숲카페_메밀미숫가루': '2023-03-31', '화담숲카페_아메리카노 HOT': '2023-03-31',\n",
        "    '화담숲카페_아메리카노 ICE': '2023-03-31', '화담숲카페_카페라떼 ICE': '2023-03-31',\n",
        "    '화담숲카페_현미뻥스크림': '2023-03-31'\n",
        "}\n",
        "launch_dates = {k: pd.to_datetime(v) for k, v in launch_dates.items()}\n",
        "\n",
        "def add_future_meta_row(date, key):\n",
        "    \"\"\"모델 입력 피처(학습 시 사용한 use_cols 기준)와 일치하도록 미래 1행 메타 피처 생성\"\"\"\n",
        "    row = pd.DataFrame({\"영업일자\":[pd.Timestamp(date)], \"영업장명_메뉴명\":[key]})\n",
        "    d = row.loc[0, \"영업일자\"]\n",
        "\n",
        "    # 기본 달력\n",
        "    row[\"년\"] = d.year\n",
        "    row[\"월\"] = d.month\n",
        "    row[\"일\"] = d.day\n",
        "    row[\"요일\"] = d.dayofweek\n",
        "    row[\"주말여부\"] = int(row.loc[0,\"요일\"] in [5,6])\n",
        "\n",
        "    # 공휴일 & 휴무일\n",
        "    hol = is_holiday(d)\n",
        "    row[\"공휴일여부\"] = int(hol)\n",
        "    row[\"휴무일여부\"] = int(hol or bool(row.loc[0,\"주말여부\"]))\n",
        "\n",
        "    # 사이클릭\n",
        "    row[\"요일_sin\"] = np.sin(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"요일_cos\"] = np.cos(2*np.pi*row.loc[0,\"요일\"]/7.0)\n",
        "    row[\"월_sin\"]   = np.sin(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "    row[\"월_cos\"]   = np.cos(2*np.pi*(row.loc[0,\"월\"]-1)/12.0)\n",
        "\n",
        "    # 계절\n",
        "    m = row.loc[0,\"월\"]\n",
        "    if m in [12,1,2]:\n",
        "        season = 0\n",
        "    elif m in [3,4,5]:\n",
        "        season = 1\n",
        "    elif m in [6,7,8]:\n",
        "        season = 2\n",
        "    else:\n",
        "        season = 3\n",
        "    row[\"계절(겨울0봄1여름2가을3)\"] = np.int8(season)\n",
        "\n",
        "    # 출시 파생\n",
        "    row[\"신규메뉴여부\"] = int(key in launch_dates)\n",
        "    if key in launch_dates and d >= launch_dates[key]:\n",
        "        row[\"출시일로부터경과일\"] = int((d - launch_dates[key]).days)\n",
        "    else:\n",
        "        row[\"출시일로부터경과일\"] = 0\n",
        "\n",
        "    # 출시 후 주차 & 더미 (학습 시 사용했다면 동일하게)\n",
        "    row[\"출시후_주차\"] = (row[\"출시일로부터경과일\"] // 7)\n",
        "    row[\"출시_0주\"]   = (row[\"출시후_주차\"] == 0).astype(int)\n",
        "    row[\"출시_1_2주\"] = row[\"출시후_주차\"].between(1,2).astype(int)\n",
        "    row[\"출시_3_4주\"] = row[\"출시후_주차\"].between(3,4).astype(int)\n",
        "    row[\"출시_5주이상\"] = (row[\"출시후_주차\"] >= 5).astype(int)\n",
        "\n",
        "    return row\n",
        "\n",
        "def predict_group_autoreg(g: pd.DataFrame):\n",
        "    key = g[cat_col].iloc[0]\n",
        "    g = g.sort_values(\"영업일자\").copy()\n",
        "    g = g.dropna(subset=[\"매출수량\"])\n",
        "    assert len(g) >= 28, f\"{key}: 28일 히스토리 부족\"\n",
        "\n",
        "    hist_vals = g[\"매출수량\"].values.tolist()[-28:]\n",
        "    hist_dates = g[\"영업일자\"].tolist()[-28:]\n",
        "    last_date = g[\"영업일자\"].max()\n",
        "    preds = []\n",
        "\n",
        "    for h in range(1, 8):\n",
        "        cur_date = last_date + pd.Timedelta(days=h)\n",
        "        row = add_future_meta_row(cur_date, key)\n",
        "\n",
        "        # lags\n",
        "        def lag(n): return hist_vals[-n] if len(hist_vals) >= n else np.nan\n",
        "        row[\"lag_1\"], row[\"lag_7\"], row[\"lag_14\"], row[\"lag_28\"] = lag(1), lag(7), lag(14), lag(28)\n",
        "\n",
        "        # rolling (과거값만)\n",
        "        def rmean(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.mean(arr)) if arr else 0.0\n",
        "        def rsum(n):\n",
        "            arr = hist_vals[-n:] if len(hist_vals) else []\n",
        "            return float(np.sum(arr)) if arr else 0.0\n",
        "        row[\"roll7_mean\"], row[\"roll7_sum\"], row[\"roll14_mean\"], row[\"roll28_mean\"] = \\\n",
        "            rmean(7), rsum(7), rmean(14), rmean(28)\n",
        "\n",
        "        # 같은 요일 평균(최근 28일 범위)\n",
        "        cur_dow = cur_date.dayofweek\n",
        "        hist_dows = [pd.Timestamp(d).dayofweek for d in hist_dates]\n",
        "        same_idx = [i for i in range(len(hist_vals)) if hist_dows[i] == cur_dow]\n",
        "        row[\"same_dow_mean_28\"] = float(np.mean([hist_vals[i] for i in same_idx])) if same_idx else 0.0\n",
        "\n",
        "        # 모델 입력 정렬\n",
        "        X = row.reindex(columns=use_cols).copy()\n",
        "        X[cat_col] = X[cat_col].astype(\"category\")\n",
        "        for c in X.columns:\n",
        "            if c != cat_col:\n",
        "                X[c] = X[c].fillna(0)\n",
        "\n",
        "        yhat = float(model.predict(X, num_iteration=getattr(model, \"best_iteration_\", None))[0])\n",
        "        yhat = max(0.0, yhat)  # 음수 방지\n",
        "        preds.append(yhat)\n",
        "\n",
        "        # autoreg 업데이트\n",
        "        hist_vals.append(yhat); hist_dates.append(cur_date)\n",
        "        if len(hist_vals) > 28:\n",
        "            hist_vals, hist_dates = hist_vals[-28:], hist_dates[-28:]\n",
        "\n",
        "    return preds  # 길이 7\n",
        "\n",
        "# ===== 샘플 제출 파일 불러오기 =====\n",
        "sub = pd.read_csv(SAMPLE_PATH)\n",
        "menu_cols = [c for c in sub.columns if c != \"영업일자\"]\n",
        "sub[menu_cols] = sub[menu_cols].astype(float)\n",
        "\n",
        "# ===== TEST 파일 순회 =====\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "print(\"Found:\", test_files)\n",
        "\n",
        "for f in test_files:\n",
        "    test_name = os.path.splitext(os.path.basename(f))[0]  # TEST_00\n",
        "    test_id = test_name.split(\"_\")[1]                     # 00\n",
        "    df = pd.read_csv(f, parse_dates=[\"영업일자\"])\n",
        "\n",
        "    # 타입/정렬\n",
        "    if cat_col in df.columns:\n",
        "        df[cat_col] = df[cat_col].astype(\"category\")\n",
        "    df = df.sort_values([cat_col, \"영업일자\"]).copy()\n",
        "\n",
        "    # 메뉴 단위 예측\n",
        "    for key, g in df.groupby(cat_col, observed=True):\n",
        "        preds7 = predict_group_autoreg(g)\n",
        "\n",
        "        # 제출 파일에 해당 메뉴 열이 없으면 skip\n",
        "        if key not in menu_cols:\n",
        "            continue\n",
        "\n",
        "        # +1~+7일 채우기\n",
        "        for k in range(1, 8):\n",
        "            ridx = sub.index[sub[\"영업일자\"] == f\"{test_name}+{k}일\"]\n",
        "            if len(ridx) == 1:\n",
        "                sub.loc[ridx[0], key] = preds7[k-1]\n",
        "\n",
        "# (선택) 후처리\n",
        "# sub[menu_cols] = sub[menu_cols].clip(lower=0).round(4)\n",
        "\n",
        "# ===== 저장 =====\n",
        "sub.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved submission ->\", OUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21w6WXEF3i86",
        "outputId": "9d4b316d-1868-4f56-bc9f-5566cf38e73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: ['/content/drive/MyDrive/lg_aimers_2/data/test/TEST_00.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_01.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_02.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_03.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_04.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_05.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_06.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_07.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_08.csv', '/content/drive/MyDrive/lg_aimers_2/data/test/TEST_09.csv']\n",
            "Saved submission -> /content/drive/MyDrive/lg_aimers_2/submission_lightgbm_04.csv\n"
          ]
        }
      ]
    }
  ]
}